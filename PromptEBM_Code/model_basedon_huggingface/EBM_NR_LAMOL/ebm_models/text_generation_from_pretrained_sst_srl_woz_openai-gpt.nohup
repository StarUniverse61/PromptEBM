nohup: ignoring input
Not all gpus support fp16 training! Will use fp32 instead.
2022-12-28 17:09:23,138 - 0:00:06 - 0.0s - INFO - root - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, aggressive=False, anneal=True, anneal_function='logistic', anneal_k=0.0025, anneal_warm_up_step=0, anneal_warm_up_value=0.0, anneal_x0=2500, attn_type='cat', avg_type='seq', backward_size=5, batch_size=30, beam_size=10, beta=0.2, bi_enc_cell=True, ckpt_step=2000, concat_decoder_input=True, data_dir='../data', debug=False, dec_cell_size=512, decay_style='linear', decoder_norm_ff='layer', decoder_norm_self='layer', device_ids=[0], dim_target_kl=1.0, dropout=0.3, dynamic_epochs=False, e_l_step_size=0.5, e_l_steps=40, e_l_with_noise=True, e_prior_sig=1.0, early_stop=True, ebm_data='lamol', ebm_data_dir='/home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/data/ptb', ebm_draw_points=False, ebm_fig_dir='/home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/figs', ebm_hidden=200, ebm_log_dir='/home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae', ebm_model='GMVAE4Lamol', ebm_model_file='/home/xiaodi//NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-05-11T20-43-13-text_generation.py/model_ckpt.pt', embed_size=512, enc_cell_size=512, encoder_norm_ff='layer', encoder_norm_self='layer', fix_batch=False, forward_only=False, fp32=True, gen_lm_sample_percentage=0.05, gen_type='greedy', gmm=True, gpu_idx=0, grad_clip=0.5, improve_threshold=0.996, init_lr=0.001, init_w=0.08, k=5, klw_for_ckl=1.0, klw_for_zkl=1.0, latent_size=40, learning_rate=6.25e-05, lm_lambda=0.25, load_sess='', logging_steps=1000, lr_decay=True, lr_decay_rate=0.8, lr_hold=3, lr_schedule='warmup_linear', max_dec_len=40, max_epoch=60, max_grad_norm=1, max_kl_weight=0.5, max_len=512, max_n_epochs=9, max_utt_len=40, max_vocab_cnt=40484, memory_sizes=[48601.0], min_batch_size=16, min_n_steps=1500, model_dir_root='./ebm_models/openai-gpt/lll/sst_srl_woz.en_0.05', model_name='openai-gpt', momentum=0.0, mult_k=20, mutual_weight=50.0, n_cycle=10, n_gpus=1, n_train_epochs={'sst': 1, 'srl': 1, 'woz.en': 1}, n_warmup_ratio=0.005, n_workers=4, num_cls=20, num_layer_dec=1, num_layer_enc=1, op='adam', patient_increase=2.0, post_sample_num=1, pretrain_ae_step=0, preview_batch_num=1, print_step=100, prior_grad_clip=1, qp_margin=0.5, ratio_increase=0.25, ratio_zero=0.5, real_sample=False, recadam_anneal_fun='sigmoid', recadam_anneal_k=0.5, recadam_anneal_t0=1000, recadam_anneal_w=1.0, recadam_pretrain_cof=5000.0, reg_lambda=1.0, rnn_cell='gru', save_model=True, seed=42, sel_metric='elbo', seq_train_type='lll', skip_tasks=None, step_size=1, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[17010], tie_output_embed=True, tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[17010], unbound=0, use_attn=False, use_gpu=True, use_mutual=False, use_sep=False, utt_type='rnn', warmup_updates=1000, weight_decay=0.01, word_dropout_rate=0.0)
2022-12-28 17:09:23,139 - 0:00:06 - 0.0s - INFO - root - start to train { task: ['sst', 'srl', 'woz.en'], seq train type: lll }
start to train { task: ['sst', 'srl', 'woz.en'], seq train type: lll }
2022-12-28 17:09:23,300 - 0:00:06 - 0.2s - INFO - root - gen token = __gen__ , gen token id = 40482
gen token = __gen__ , gen token id = 40482
2022-12-28 17:09:23,301 - 0:00:06 - 0.0s - INFO - root - task_id=0
task_id=0
Done loading corpus
Done loading corpus
/home/xiaodi/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
2022-12-28 17:09:35,948 - 0:00:19 - 12.6s - INFO - root - GMVAE4Lamol (
  (embedding): Embedding(40484, 512, padding_idx=40479), parameters=20727808
  (dec_embedding): Embedding(40484, 512, padding_idx=40479), parameters=20727808
  (x_encoder): EncoderRNN(
    (input_dropout): Dropout(p=0, inplace=False)
    (rnn): GRU(512, 512, batch_first=True, dropout=0.3, bidirectional=True)
  ), parameters=3151872
  (decoder): DecoderRNN(
    (input_dropout): Dropout(p=0.3, inplace=False)
    (rnn): GRU(552, 512, batch_first=True, dropout=0.3)
    (embedding): Embedding(40484, 512, padding_idx=40479)
    (output_dropout): Dropout(p=0.3, inplace=False)
  ), parameters=22365184
  (q_y_mean): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (q_y_logvar): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (dec_init_connector): LinearConnector(
    (linear): Linear(in_features=40, out_features=512, bias=False)
  ), parameters=20480
  (nll_loss): NLLEntropy(
    (nll_loss): NLLLoss()
  ), parameters=0
  (ebm): Sequential (
    (0): Linear(in_features=40, out_features=200, bias=True), weights=((200, 40), (200,)), parameters=8200
    (1): GELU(), weights=(), parameters=0
    (2): Linear(in_features=200, out_features=200, bias=True), weights=((200, 200), (200,)), parameters=40200
    (3): GELU(), weights=(), parameters=0
    (4): Linear(in_features=200, out_features=20, bias=True), weights=((20, 200), (20,)), parameters=4020
  ) Total Parameters=52420, parameters=52420
) Total Parameters=67127572
Done loading corpus
Max len 40 and min len 11 and avg len 31.746755
Max len 40 and min len 11 and avg len 31.068278
Max len 40 and min len 11 and avg len 31.068278
GMVAE4Lamol (
  (embedding): Embedding(40484, 512, padding_idx=40479), parameters=20727808
  (dec_embedding): Embedding(40484, 512, padding_idx=40479), parameters=20727808
  (x_encoder): EncoderRNN(
    (input_dropout): Dropout(p=0, inplace=False)
    (rnn): GRU(512, 512, batch_first=True, dropout=0.3, bidirectional=True)
  ), parameters=3151872
  (decoder): DecoderRNN(
    (input_dropout): Dropout(p=0.3, inplace=False)
    (rnn): GRU(552, 512, batch_first=True, dropout=0.3)
    (embedding): Embedding(40484, 512, padding_idx=40479)
    (output_dropout): Dropout(p=0.3, inplace=False)
  ), parameters=22365184
  (q_y_mean): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (q_y_logvar): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (dec_init_connector): LinearConnector(
    (linear): Linear(in_features=40, out_features=512, bias=False)
  ), parameters=20480
  (nll_loss): NLLEntropy(
    (nll_loss): NLLLoss()
  ), parameters=0
  (ebm): Sequential (
    (0): Linear(in_features=40, out_features=200, bias=True), weights=((200, 40), (200,)), parameters=8200
    (1): GELU(), weights=(), parameters=0
    (2): Linear(in_features=200, out_features=200, bias=True), weights=((200, 200), (200,)), parameters=40200
    (3): GELU(), weights=(), parameters=0
    (4): Linear(in_features=200, out_features=20, bias=True), weights=((20, 200), (20,)), parameters=4020
  ) Total Parameters=52420, parameters=52420
) Total Parameters=67127572
2022-12-28 17:09:35,948 - 0:00:19 - 0.0s - INFO - root - **** Training Begins ****
**** Training Begins ****
2022-12-28 17:09:35,948 - 0:00:19 - 0.0s - INFO - root - **** Epoch 0/60 ****
**** Epoch 0/60 ****
2022-12-28 17:09:35,958 - 0:00:19 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:09:35,960 - 0:00:19 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:09:46,056 - 0:00:29 - 10.1s - INFO - root - batch/max_batch/ep:   100/   529/     1 rec:  161.652 mi: 0.00062656 zkl:  110.566 cd:    0.010 pos_prob:    3.026 prob_neg:    3.016 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     1 rec:  161.652 mi: 0.00062656 zkl:  110.566 cd:    0.010 pos_prob:    3.026 prob_neg:    3.016 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:09:55,133 - 0:00:38 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/     1 rec:  138.694 mi: 0.00077939 zkl:  136.342 cd:    0.003 pos_prob:    3.022 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     1 rec:  138.694 mi: 0.00077939 zkl:  136.342 cd:    0.003 pos_prob:    3.022 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:10:04,449 - 0:00:47 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/     1 rec:  136.748 mi: 0.00172067 zkl:  143.451 cd:    0.007 pos_prob:    3.026 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     1 rec:  136.748 mi: 0.00172067 zkl:  143.451 cd:    0.007 pos_prob:    3.026 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:10:13,336 - 0:00:56 - 8.9s - INFO - root - batch/max_batch/ep:   400/   529/     1 rec:  158.072 mi: 0.00217152 zkl:  155.106 cd:   -0.002 pos_prob:    3.018 prob_neg:    3.021 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     1 rec:  158.072 mi: 0.00217152 zkl:  155.106 cd:   -0.002 pos_prob:    3.018 prob_neg:    3.021 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:10:22,344 - 0:01:05 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/     1 rec:  117.174 mi: 0.00349355 zkl:  171.823 cd:    0.008 pos_prob:    3.027 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     1 rec:  117.174 mi: 0.00349355 zkl:  171.823 cd:    0.008 pos_prob:    3.027 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:10:22,356 - 0:01:05 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -90.369
Langevin prior   1/ 40: energy= -90.369
2022-12-28 17:10:22,362 - 0:01:05 - 0.0s - INFO - root - Langevin prior   6/ 40: energy= -90.449
Langevin prior   6/ 40: energy= -90.449
2022-12-28 17:10:22,367 - 0:01:05 - 0.0s - INFO - root - Langevin prior  11/ 40: energy= -90.398
Langevin prior  11/ 40: energy= -90.398
2022-12-28 17:10:22,373 - 0:01:05 - 0.0s - INFO - root - Langevin prior  16/ 40: energy= -90.409
Langevin prior  16/ 40: energy= -90.409
2022-12-28 17:10:22,380 - 0:01:05 - 0.0s - INFO - root - Langevin prior  21/ 40: energy= -90.549
Langevin prior  21/ 40: energy= -90.549
2022-12-28 17:10:22,387 - 0:01:05 - 0.0s - INFO - root - Langevin prior  26/ 40: energy= -90.450
Langevin prior  26/ 40: energy= -90.450
2022-12-28 17:10:22,396 - 0:01:05 - 0.0s - INFO - root - Langevin prior  31/ 40: energy= -90.479
Langevin prior  31/ 40: energy= -90.479
2022-12-28 17:10:22,405 - 0:01:05 - 0.0s - INFO - root - Langevin prior  36/ 40: energy= -90.444
Langevin prior  36/ 40: energy= -90.444
2022-12-28 17:10:22,413 - 0:01:05 - 0.0s - INFO - root - Langevin prior  40/ 40: energy= -90.555
Langevin prior  40/ 40: energy= -90.555
2022-12-28 17:10:24,993 - 0:01:08 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-001-test-greedy.txt
Generation: 188 batches
2022-12-28 17:10:30,702 - 0:01:13 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:10:32,087 - 0:01:15 - 1.4s - INFO - root - Generation Done
Generation Done
2022-12-28 17:10:32,087 - 0:01:15 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:10:34,307 - 0:01:17 - 2.2s - INFO - root - --- bleu: BLEU = 25.63, 54.8/31.6/24.3/20.9 (BP=0.837, ratio=0.849, hyp_len=144121, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 25.63, 54.8/31.6/24.3/20.9 (BP=0.837, ratio=0.849, hyp_len=144121, ref_len=169777)
--- bleu: BLEU = 25.63, 54.8/31.6/24.3/20.9 (BP=0.837, ratio=0.849, hyp_len=144121, ref_len=169777)

2022-12-28 17:10:34,307 - 0:01:17 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:10:34,315 - 0:01:17 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:10:43,465 - 0:01:26 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/     2 rec:  118.492 mi: 0.00277352 zkl:  180.536 cd:    0.001 pos_prob:    3.020 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     2 rec:  118.492 mi: 0.00277352 zkl:  180.536 cd:    0.001 pos_prob:    3.020 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:10:52,658 - 0:01:35 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/     2 rec:  101.201 mi: 0.00372362 zkl:  198.243 cd:    0.015 pos_prob:    3.035 prob_neg:    3.020 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     2 rec:  101.201 mi: 0.00372362 zkl:  198.243 cd:    0.015 pos_prob:    3.035 prob_neg:    3.020 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:11:01,899 - 0:01:45 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/     2 rec:  104.115 mi: 0.00393033 zkl:  195.651 cd:    0.016 pos_prob:    3.034 prob_neg:    3.018 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     2 rec:  104.115 mi: 0.00393033 zkl:  195.651 cd:    0.016 pos_prob:    3.034 prob_neg:    3.018 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:11:10,987 - 0:01:54 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/     2 rec:   90.989 mi: 0.00459862 zkl:  202.731 cd:    0.013 pos_prob:    3.032 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     2 rec:   90.989 mi: 0.00459862 zkl:  202.731 cd:    0.013 pos_prob:    3.032 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:11:20,144 - 0:02:03 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/     2 rec:   64.512 mi: 0.00603414 zkl:  218.011 cd:    0.013 pos_prob:    3.036 prob_neg:    3.023 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     2 rec:   64.512 mi: 0.00603414 zkl:  218.011 cd:    0.013 pos_prob:    3.036 prob_neg:    3.023 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:11:20,157 - 0:02:03 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -90.411
Langevin prior   1/ 40: energy= -90.411
2022-12-28 17:11:20,162 - 0:02:03 - 0.0s - INFO - root - Langevin prior   6/ 40: energy= -90.654
Langevin prior   6/ 40: energy= -90.654
2022-12-28 17:11:20,168 - 0:02:03 - 0.0s - INFO - root - Langevin prior  11/ 40: energy= -90.641
Langevin prior  11/ 40: energy= -90.641
2022-12-28 17:11:20,174 - 0:02:03 - 0.0s - INFO - root - Langevin prior  16/ 40: energy= -90.633
Langevin prior  16/ 40: energy= -90.633
2022-12-28 17:11:20,181 - 0:02:03 - 0.0s - INFO - root - Langevin prior  21/ 40: energy= -90.707
Langevin prior  21/ 40: energy= -90.707
2022-12-28 17:11:20,189 - 0:02:03 - 0.0s - INFO - root - Langevin prior  26/ 40: energy= -90.829
Langevin prior  26/ 40: energy= -90.829
2022-12-28 17:11:20,197 - 0:02:03 - 0.0s - INFO - root - Langevin prior  31/ 40: energy= -90.748
Langevin prior  31/ 40: energy= -90.748
2022-12-28 17:11:20,207 - 0:02:03 - 0.0s - INFO - root - Langevin prior  36/ 40: energy= -90.664
Langevin prior  36/ 40: energy= -90.664
2022-12-28 17:11:20,214 - 0:02:03 - 0.0s - INFO - root - Langevin prior  40/ 40: energy= -90.568
Langevin prior  40/ 40: energy= -90.568
2022-12-28 17:11:22,853 - 0:02:06 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-002-test-greedy.txt
Generation: 188 batches
2022-12-28 17:11:28,571 - 0:02:11 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:11:29,988 - 0:02:13 - 1.4s - INFO - root - Generation Done
Generation Done
2022-12-28 17:11:29,988 - 0:02:13 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:11:32,286 - 0:02:15 - 2.3s - INFO - root - --- bleu: BLEU = 28.74, 57.5/33.8/26.4/23.0 (BP=0.872, ratio=0.880, hyp_len=149340, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 28.74, 57.5/33.8/26.4/23.0 (BP=0.872, ratio=0.880, hyp_len=149340, ref_len=169777)
--- bleu: BLEU = 28.74, 57.5/33.8/26.4/23.0 (BP=0.872, ratio=0.880, hyp_len=149340, ref_len=169777)

2022-12-28 17:11:32,286 - 0:02:15 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:11:32,294 - 0:02:15 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:11:41,481 - 0:02:24 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/     3 rec:   78.886 mi: 0.00617981 zkl:  206.129 cd:    0.010 pos_prob:    3.027 prob_neg:    3.017 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     3 rec:   78.886 mi: 0.00617981 zkl:  206.129 cd:    0.010 pos_prob:    3.027 prob_neg:    3.017 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:11:50,468 - 0:02:33 - 9.0s - INFO - root - batch/max_batch/ep:   200/   529/     3 rec:   76.487 mi: 0.00510526 zkl:  214.921 cd:    0.006 pos_prob:    3.025 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     3 rec:   76.487 mi: 0.00510526 zkl:  214.921 cd:    0.006 pos_prob:    3.025 prob_neg:    3.019 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:11:59,542 - 0:02:42 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/     3 rec:   57.755 mi: 0.00797176 zkl:  226.128 cd:    0.006 pos_prob:    3.022 prob_neg:    3.017 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     3 rec:   57.755 mi: 0.00797176 zkl:  226.128 cd:    0.006 pos_prob:    3.022 prob_neg:    3.017 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:12:08,833 - 0:02:52 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/     3 rec:   61.085 mi: 0.00634408 zkl:  216.520 cd:    0.009 pos_prob:    3.028 prob_neg:    3.018 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     3 rec:   61.085 mi: 0.00634408 zkl:  216.520 cd:    0.009 pos_prob:    3.028 prob_neg:    3.018 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:12:17,973 - 0:03:01 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/     3 rec:   60.488 mi: 0.00655246 zkl:  229.586 cd:    0.013 pos_prob:    3.036 prob_neg:    3.023 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     3 rec:   60.488 mi: 0.00655246 zkl:  229.586 cd:    0.013 pos_prob:    3.036 prob_neg:    3.023 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:12:17,988 - 0:03:01 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -90.694
Langevin prior   1/ 40: energy= -90.694
2022-12-28 17:12:17,998 - 0:03:01 - 0.0s - INFO - root - Langevin prior   6/ 40: energy= -90.610
Langevin prior   6/ 40: energy= -90.610
2022-12-28 17:12:18,008 - 0:03:01 - 0.0s - INFO - root - Langevin prior  11/ 40: energy= -90.586
Langevin prior  11/ 40: energy= -90.586
2022-12-28 17:12:18,020 - 0:03:01 - 0.0s - INFO - root - Langevin prior  16/ 40: energy= -90.423
Langevin prior  16/ 40: energy= -90.423
2022-12-28 17:12:18,032 - 0:03:01 - 0.0s - INFO - root - Langevin prior  21/ 40: energy= -90.603
Langevin prior  21/ 40: energy= -90.603
2022-12-28 17:12:18,044 - 0:03:01 - 0.0s - INFO - root - Langevin prior  26/ 40: energy= -90.600
Langevin prior  26/ 40: energy= -90.600
2022-12-28 17:12:18,057 - 0:03:01 - 0.0s - INFO - root - Langevin prior  31/ 40: energy= -90.601
Langevin prior  31/ 40: energy= -90.601
2022-12-28 17:12:18,070 - 0:03:01 - 0.0s - INFO - root - Langevin prior  36/ 40: energy= -90.668
Langevin prior  36/ 40: energy= -90.668
2022-12-28 17:12:18,080 - 0:03:01 - 0.0s - INFO - root - Langevin prior  40/ 40: energy= -90.472
Langevin prior  40/ 40: energy= -90.472
2022-12-28 17:12:20,713 - 0:03:03 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-003-test-greedy.txt
Generation: 188 batches
2022-12-28 17:12:26,433 - 0:03:09 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:12:27,892 - 0:03:11 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 17:12:27,892 - 0:03:11 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:12:30,257 - 0:03:13 - 2.4s - INFO - root - --- bleu: BLEU = 30.21, 56.4/33.6/26.6/23.4 (BP=0.916, ratio=0.920, hyp_len=156139, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.21, 56.4/33.6/26.6/23.4 (BP=0.916, ratio=0.920, hyp_len=156139, ref_len=169777)
--- bleu: BLEU = 30.21, 56.4/33.6/26.6/23.4 (BP=0.916, ratio=0.920, hyp_len=156139, ref_len=169777)

2022-12-28 17:12:30,257 - 0:03:13 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:12:30,265 - 0:03:13 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:12:39,535 - 0:03:22 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/     4 rec:   55.502 mi: 0.53640187 zkl:  159.687 cd:    2.298 pos_prob:    5.322 prob_neg:    3.025 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/     4 rec:   55.502 mi: 0.53640187 zkl:  159.687 cd:    2.298 pos_prob:    5.322 prob_neg:    3.025 kl_weight:    0.062 do_ae_train: False
2022-12-28 17:12:48,780 - 0:03:31 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/     4 rec:   61.709 mi: 0.99023902 zkl:  118.039 cd:    4.657 pos_prob:    7.694 prob_neg:    3.037 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/     4 rec:   61.709 mi: 0.99023902 zkl:  118.039 cd:    4.657 pos_prob:    7.694 prob_neg:    3.037 kl_weight:    0.125 do_ae_train: False
2022-12-28 17:12:58,099 - 0:03:41 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/     4 rec:   42.117 mi: 1.10773373 zkl:   78.315 cd:    5.961 pos_prob:    9.055 prob_neg:    3.094 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/     4 rec:   42.117 mi: 1.10773373 zkl:   78.315 cd:    5.961 pos_prob:    9.055 prob_neg:    3.094 kl_weight:    0.188 do_ae_train: False
2022-12-28 17:13:07,427 - 0:03:50 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/     4 rec:   43.024 mi: 1.49649370 zkl:   73.446 cd:    8.944 pos_prob:   12.129 prob_neg:    3.185 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/     4 rec:   43.024 mi: 1.49649370 zkl:   73.446 cd:    8.944 pos_prob:   12.129 prob_neg:    3.185 kl_weight:    0.251 do_ae_train: False
2022-12-28 17:13:16,668 - 0:03:59 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/     4 rec:   55.907 mi: 1.82562876 zkl:   67.900 cd:    9.950 pos_prob:   13.307 prob_neg:    3.357 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/     4 rec:   55.907 mi: 1.82562876 zkl:   67.900 cd:    9.950 pos_prob:   13.307 prob_neg:    3.357 kl_weight:    0.314 do_ae_train: False
2022-12-28 17:13:16,680 - 0:03:59 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -97.684
Langevin prior   1/ 40: energy= -97.684
2022-12-28 17:13:16,686 - 0:03:59 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-102.376
Langevin prior   6/ 40: energy=-102.376
2022-12-28 17:13:16,692 - 0:03:59 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-112.125
Langevin prior  11/ 40: energy=-112.125
2022-12-28 17:13:16,697 - 0:03:59 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-104.714
Langevin prior  16/ 40: energy=-104.714
2022-12-28 17:13:16,704 - 0:03:59 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-102.244
Langevin prior  21/ 40: energy=-102.244
2022-12-28 17:13:16,712 - 0:03:59 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-103.349
Langevin prior  26/ 40: energy=-103.349
2022-12-28 17:13:16,721 - 0:03:59 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-100.241
Langevin prior  31/ 40: energy=-100.241
2022-12-28 17:13:16,730 - 0:03:59 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-101.593
Langevin prior  36/ 40: energy=-101.593
2022-12-28 17:13:16,738 - 0:03:59 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-106.123
Langevin prior  40/ 40: energy=-106.123
2022-12-28 17:13:19,381 - 0:04:02 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-004-test-greedy.txt
Generation: 188 batches
2022-12-28 17:13:25,038 - 0:04:08 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:13:26,435 - 0:04:09 - 1.4s - INFO - root - Generation Done
Generation Done
2022-12-28 17:13:26,435 - 0:04:09 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:13:28,734 - 0:04:11 - 2.3s - INFO - root - --- bleu: BLEU = 27.43, 58.1/34.3/27.1/23.7 (BP=0.815, ratio=0.830, hyp_len=140882, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 27.43, 58.1/34.3/27.1/23.7 (BP=0.815, ratio=0.830, hyp_len=140882, ref_len=169777)
--- bleu: BLEU = 27.43, 58.1/34.3/27.1/23.7 (BP=0.815, ratio=0.830, hyp_len=140882, ref_len=169777)

2022-12-28 17:13:28,734 - 0:04:11 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:13:28,742 - 0:04:11 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:13:37,910 - 0:04:21 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/     5 rec:   56.356 mi: 1.84022450 zkl:   52.773 cd:   11.244 pos_prob:   15.171 prob_neg:    3.927 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/     5 rec:   56.356 mi: 1.84022450 zkl:   52.773 cd:   11.244 pos_prob:   15.171 prob_neg:    3.927 kl_weight:    0.396 do_ae_train: False
2022-12-28 17:13:47,121 - 0:04:30 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/     5 rec:   70.955 mi: 1.79892588 zkl:   42.832 cd:    9.194 pos_prob:   14.806 prob_neg:    5.612 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/     5 rec:   70.955 mi: 1.79892588 zkl:   42.832 cd:    9.194 pos_prob:   14.806 prob_neg:    5.612 kl_weight:    0.459 do_ae_train: False
2022-12-28 17:13:56,326 - 0:04:39 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/     5 rec:   53.689 mi: 1.88176191 zkl:   39.750 cd:   10.379 pos_prob:   18.702 prob_neg:    8.324 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/     5 rec:   53.689 mi: 1.88176191 zkl:   39.750 cd:   10.379 pos_prob:   18.702 prob_neg:    8.324 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:14:05,618 - 0:04:48 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/     5 rec:   56.087 mi: 1.89548635 zkl:   40.925 cd:    9.761 pos_prob:   20.051 prob_neg:   10.290 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/     5 rec:   56.087 mi: 1.89548635 zkl:   40.925 cd:    9.761 pos_prob:   20.051 prob_neg:   10.290 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:14:14,850 - 0:04:58 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/     5 rec:   51.782 mi: 1.93698525 zkl:   43.497 cd:    5.236 pos_prob:   22.771 prob_neg:   17.535 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/     5 rec:   51.782 mi: 1.93698525 zkl:   43.497 cd:    5.236 pos_prob:   22.771 prob_neg:   17.535 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:14:14,863 - 0:04:58 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-141.204
Langevin prior   1/ 40: energy=-141.204
2022-12-28 17:14:14,868 - 0:04:58 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-237.956
Langevin prior   6/ 40: energy=-237.956
2022-12-28 17:14:14,873 - 0:04:58 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-300.045
Langevin prior  11/ 40: energy=-300.045
2022-12-28 17:14:14,879 - 0:04:58 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-353.976
Langevin prior  16/ 40: energy=-353.976
2022-12-28 17:14:14,885 - 0:04:58 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-430.996
Langevin prior  21/ 40: energy=-430.996
2022-12-28 17:14:14,893 - 0:04:58 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-444.252
Langevin prior  26/ 40: energy=-444.252
2022-12-28 17:14:14,901 - 0:04:58 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-464.087
Langevin prior  31/ 40: energy=-464.087
2022-12-28 17:14:14,910 - 0:04:58 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-481.266
Langevin prior  36/ 40: energy=-481.266
2022-12-28 17:14:14,918 - 0:04:58 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-463.980
Langevin prior  40/ 40: energy=-463.980
2022-12-28 17:24:00,179 - 0:14:43 - 585.3s - INFO - root - Negative Log-likehood -134.873029
200 done. -122.61115742304453
400 done. -118.03991032634497
600 done. -117.64473821608524
800 done. -118.3265197034756
1000 done. -117.50179911786799
1200 done. -118.22621310100175
1400 done. -119.10107143877912
1600 done. -117.82543593273945
1800 done. -117.5476164304553
2000 done. -129.43774110056341
2200 done. -136.78921274715134
2400 done. -144.83730345636107
2600 done. -152.0939269590485
2800 done. -157.0149216697735
3000 done. -163.2119050685398
3200 done. -166.9206678093797
3400 done. -171.6167761683062
3600 done. -174.4608409830024
3800 done. -178.26589939245204
4000 done. -180.88856563024567
4200 done. -173.921160955653
4400 done. -166.8681641273257
4600 done. -160.6322860189067
4800 done. -154.85512854853516
5000 done. -149.70196111186647
5200 done. -144.82420763531147
5400 done. -140.39249488797572
5600 done. -136.24815016488003
Negative Log-likehood -134.873029
2022-12-28 17:24:00,179 - 0:14:43 - 0.0s - INFO - root - log-likelihood:   -134.873
log-likelihood:   -134.873
2022-12-28 17:24:45,902 - 0:15:29 - 45.7s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-005-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 17:24:45,904 - 0:15:29 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-005-test-greedy.txt
Generation: 188 batches
2022-12-28 17:24:51,632 - 0:15:34 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:24:53,136 - 0:15:36 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 17:24:53,136 - 0:15:36 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:24:55,828 - 0:15:39 - 2.7s - INFO - root - --- bleu: BLEU = 28.60, 52.8/30.8/24.2/21.2 (BP=0.946, ratio=0.948, hyp_len=160913, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 28.60, 52.8/30.8/24.2/21.2 (BP=0.946, ratio=0.948, hyp_len=160913, ref_len=169777)
--- bleu: BLEU = 28.60, 52.8/30.8/24.2/21.2 (BP=0.946, ratio=0.948, hyp_len=160913, ref_len=169777)

2022-12-28 17:24:55,829 - 0:15:39 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:24:55,842 - 0:15:39 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:25:05,104 - 0:15:48 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/     6 rec:   54.621 mi: 1.82344615 zkl:   43.555 cd:   10.141 pos_prob:   30.447 prob_neg:   20.306 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/     6 rec:   54.621 mi: 1.82344615 zkl:   43.555 cd:   10.141 pos_prob:   30.447 prob_neg:   20.306 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:25:14,565 - 0:15:57 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/     6 rec:   46.372 mi: 1.99119949 zkl:   41.206 cd:    3.254 pos_prob:   28.582 prob_neg:   25.328 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/     6 rec:   46.372 mi: 1.99119949 zkl:   41.206 cd:    3.254 pos_prob:   28.582 prob_neg:   25.328 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:25:23,919 - 0:16:07 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/     6 rec:   64.950 mi: 1.93094647 zkl:   41.139 cd:    5.045 pos_prob:   32.572 prob_neg:   27.527 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/     6 rec:   64.950 mi: 1.93094647 zkl:   41.139 cd:    5.045 pos_prob:   32.572 prob_neg:   27.527 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:25:33,203 - 0:16:16 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/     6 rec:   51.073 mi: 1.83136487 zkl:   44.319 cd:    2.624 pos_prob:   36.463 prob_neg:   33.839 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/     6 rec:   51.073 mi: 1.83136487 zkl:   44.319 cd:    2.624 pos_prob:   36.463 prob_neg:   33.839 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:25:42,435 - 0:16:25 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/     6 rec:   35.482 mi: 1.68282175 zkl:   47.895 cd:    4.064 pos_prob:   39.285 prob_neg:   35.221 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/     6 rec:   35.482 mi: 1.68282175 zkl:   47.895 cd:    4.064 pos_prob:   39.285 prob_neg:   35.221 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:25:42,447 - 0:16:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-167.481
Langevin prior   1/ 40: energy=-167.481
2022-12-28 17:25:42,453 - 0:16:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-432.127
Langevin prior   6/ 40: energy=-432.127
2022-12-28 17:25:42,459 - 0:16:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-664.219
Langevin prior  11/ 40: energy=-664.219
2022-12-28 17:25:42,464 - 0:16:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-791.861
Langevin prior  16/ 40: energy=-791.861
2022-12-28 17:25:42,470 - 0:16:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-866.439
Langevin prior  21/ 40: energy=-866.439
2022-12-28 17:25:42,476 - 0:16:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-956.055
Langevin prior  26/ 40: energy=-956.055
2022-12-28 17:25:42,485 - 0:16:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-999.545
Langevin prior  31/ 40: energy=-999.545
2022-12-28 17:25:42,494 - 0:16:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-978.405
Langevin prior  36/ 40: energy=-978.405
2022-12-28 17:25:42,502 - 0:16:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1007.379
Langevin prior  40/ 40: energy=-1007.379
2022-12-28 17:35:28,346 - 0:26:11 - 585.8s - INFO - root - Negative Log-likehood -136.034868
200 done. -122.66678171507475
400 done. -118.03518866606845
600 done. -117.67205363714766
800 done. -118.35710556272927
1000 done. -117.55808347681001
1200 done. -118.28064951733562
1400 done. -119.22328379223214
1600 done. -117.92221857012552
1800 done. -117.5391249426275
2000 done. -129.55908148089918
2200 done. -137.19811636241073
2400 done. -145.4545990307425
2600 done. -152.9861070335701
2800 done. -158.08240857806956
3000 done. -164.5536662034018
3200 done. -168.35857457702616
3400 done. -173.16051751016013
3600 done. -176.20083124096004
3800 done. -180.11790606062948
4000 done. -182.76682027337435
4200 done. -175.7116862750971
4400 done. -168.52630653408283
4600 done. -162.17061679955478
4800 done. -156.30249058797594
5000 done. -151.0723993021887
5200 done. -146.1198504374961
5400 done. -141.62831008337358
5600 done. -137.43546805198923
Negative Log-likehood -136.034868
2022-12-28 17:35:28,346 - 0:26:11 - 0.0s - INFO - root - log-likelihood:   -136.035
log-likelihood:   -136.035
2022-12-28 17:36:14,133 - 0:26:57 - 45.8s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-006-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 17:36:14,135 - 0:26:57 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-006-test-greedy.txt
Generation: 188 batches
2022-12-28 17:36:19,865 - 0:27:03 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:36:21,355 - 0:27:04 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 17:36:21,355 - 0:27:04 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:36:23,786 - 0:27:06 - 2.4s - INFO - root - --- bleu: BLEU = 28.73, 53.0/31.2/24.5/21.4 (BP=0.942, ratio=0.944, hyp_len=160250, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 28.73, 53.0/31.2/24.5/21.4 (BP=0.942, ratio=0.944, hyp_len=160250, ref_len=169777)
--- bleu: BLEU = 28.73, 53.0/31.2/24.5/21.4 (BP=0.942, ratio=0.944, hyp_len=160250, ref_len=169777)

2022-12-28 17:36:23,787 - 0:27:06 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:36:23,795 - 0:27:06 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:36:32,922 - 0:27:16 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/     7 rec:   36.674 mi: 1.97898388 zkl:  129.171 cd:   -3.522 pos_prob:   34.977 prob_neg:   38.498 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     7 rec:   36.674 mi: 1.97898388 zkl:  129.171 cd:   -3.522 pos_prob:   34.977 prob_neg:   38.498 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:36:42,024 - 0:27:25 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/     7 rec:   47.714 mi: 1.99673581 zkl:  138.986 cd:   -4.097 pos_prob:   32.623 prob_neg:   36.720 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     7 rec:   47.714 mi: 1.99673581 zkl:  138.986 cd:   -4.097 pos_prob:   32.623 prob_neg:   36.720 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:36:51,134 - 0:27:34 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/     7 rec:   37.122 mi: 1.84278417 zkl:  143.641 cd:   -6.579 pos_prob:   29.542 prob_neg:   36.121 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     7 rec:   37.122 mi: 1.84278417 zkl:  143.641 cd:   -6.579 pos_prob:   29.542 prob_neg:   36.121 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:37:00,232 - 0:27:43 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/     7 rec:   41.856 mi: 1.92929578 zkl:  148.461 cd:   -5.499 pos_prob:   30.145 prob_neg:   35.644 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     7 rec:   41.856 mi: 1.92929578 zkl:  148.461 cd:   -5.499 pos_prob:   30.145 prob_neg:   35.644 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:37:09,184 - 0:27:52 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/     7 rec:   47.127 mi: 1.79687572 zkl:  151.941 cd:   -9.365 pos_prob:   27.874 prob_neg:   37.239 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     7 rec:   47.127 mi: 1.79687572 zkl:  151.941 cd:   -9.365 pos_prob:   27.874 prob_neg:   37.239 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:37:09,197 - 0:27:52 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-178.400
Langevin prior   1/ 40: energy=-178.400
2022-12-28 17:37:09,202 - 0:27:52 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-443.988
Langevin prior   6/ 40: energy=-443.988
2022-12-28 17:37:09,207 - 0:27:52 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-725.969
Langevin prior  11/ 40: energy=-725.969
2022-12-28 17:37:09,212 - 0:27:52 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-940.098
Langevin prior  16/ 40: energy=-940.098
2022-12-28 17:37:09,217 - 0:27:52 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-975.831
Langevin prior  21/ 40: energy=-975.831
2022-12-28 17:37:09,223 - 0:27:52 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1027.407
Langevin prior  26/ 40: energy=-1027.407
2022-12-28 17:37:09,231 - 0:27:52 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1086.729
Langevin prior  31/ 40: energy=-1086.729
2022-12-28 17:37:09,239 - 0:27:52 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1091.447
Langevin prior  36/ 40: energy=-1091.447
2022-12-28 17:37:09,246 - 0:27:52 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1121.507
Langevin prior  40/ 40: energy=-1121.507
2022-12-28 17:37:12,049 - 0:27:55 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-007-test-greedy.txt
Generation: 188 batches
2022-12-28 17:37:17,754 - 0:28:00 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:37:19,231 - 0:28:02 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 17:37:19,231 - 0:28:02 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:37:21,620 - 0:28:04 - 2.4s - INFO - root - --- bleu: BLEU = 30.89, 56.4/34.0/27.3/24.1 (BP=0.921, ratio=0.924, hyp_len=156824, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.89, 56.4/34.0/27.3/24.1 (BP=0.921, ratio=0.924, hyp_len=156824, ref_len=169777)
--- bleu: BLEU = 30.89, 56.4/34.0/27.3/24.1 (BP=0.921, ratio=0.924, hyp_len=156824, ref_len=169777)

2022-12-28 17:37:21,621 - 0:28:04 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:37:21,629 - 0:28:04 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:37:30,807 - 0:28:14 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/     8 rec:   36.145 mi: 1.85849094 zkl:  160.599 cd:   -9.915 pos_prob:   27.145 prob_neg:   37.060 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     8 rec:   36.145 mi: 1.85849094 zkl:  160.599 cd:   -9.915 pos_prob:   27.145 prob_neg:   37.060 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:37:39,980 - 0:28:23 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/     8 rec:   22.839 mi: 1.83841455 zkl:  160.310 cd:   -7.269 pos_prob:   28.074 prob_neg:   35.342 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     8 rec:   22.839 mi: 1.83841455 zkl:  160.310 cd:   -7.269 pos_prob:   28.074 prob_neg:   35.342 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:37:49,021 - 0:28:32 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/     8 rec:   44.315 mi: 1.81001055 zkl:  161.581 cd:  -11.386 pos_prob:   25.366 prob_neg:   36.751 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     8 rec:   44.315 mi: 1.81001055 zkl:  161.581 cd:  -11.386 pos_prob:   25.366 prob_neg:   36.751 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:37:58,125 - 0:28:41 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/     8 rec:   43.493 mi: 1.74339402 zkl:  165.551 cd:  -11.246 pos_prob:   23.990 prob_neg:   35.236 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     8 rec:   43.493 mi: 1.74339402 zkl:  165.551 cd:  -11.246 pos_prob:   23.990 prob_neg:   35.236 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:38:07,163 - 0:28:50 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/     8 rec:   34.205 mi: 1.88272655 zkl:  168.826 cd:  -11.917 pos_prob:   24.790 prob_neg:   36.707 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     8 rec:   34.205 mi: 1.88272655 zkl:  168.826 cd:  -11.917 pos_prob:   24.790 prob_neg:   36.707 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:38:07,175 - 0:28:50 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-176.094
Langevin prior   1/ 40: energy=-176.094
2022-12-28 17:38:07,181 - 0:28:50 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-429.751
Langevin prior   6/ 40: energy=-429.751
2022-12-28 17:38:07,186 - 0:28:50 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-692.631
Langevin prior  11/ 40: energy=-692.631
2022-12-28 17:38:07,191 - 0:28:50 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-859.658
Langevin prior  16/ 40: energy=-859.658
2022-12-28 17:38:07,197 - 0:28:50 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-922.214
Langevin prior  21/ 40: energy=-922.214
2022-12-28 17:38:07,203 - 0:28:50 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1058.306
Langevin prior  26/ 40: energy=-1058.306
2022-12-28 17:38:07,210 - 0:28:50 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1051.653
Langevin prior  31/ 40: energy=-1051.653
2022-12-28 17:38:07,221 - 0:28:50 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1006.121
Langevin prior  36/ 40: energy=-1006.121
2022-12-28 17:38:07,230 - 0:28:50 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1043.452
Langevin prior  40/ 40: energy=-1043.452
2022-12-28 17:38:09,790 - 0:28:52 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-008-test-greedy.txt
Generation: 188 batches
2022-12-28 17:38:15,485 - 0:28:58 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:38:16,962 - 0:29:00 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 17:38:16,963 - 0:29:00 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:38:19,341 - 0:29:02 - 2.4s - INFO - root - --- bleu: BLEU = 31.45, 56.9/34.5/27.9/24.7 (BP=0.922, ratio=0.925, hyp_len=157049, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.45, 56.9/34.5/27.9/24.7 (BP=0.922, ratio=0.925, hyp_len=157049, ref_len=169777)
--- bleu: BLEU = 31.45, 56.9/34.5/27.9/24.7 (BP=0.922, ratio=0.925, hyp_len=157049, ref_len=169777)

2022-12-28 17:38:19,341 - 0:29:02 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:38:19,349 - 0:29:02 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:38:28,509 - 0:29:11 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/     9 rec:   28.094 mi: 1.87154627 zkl:  173.569 cd:  -10.449 pos_prob:   24.669 prob_neg:   35.118 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     9 rec:   28.094 mi: 1.87154627 zkl:  173.569 cd:  -10.449 pos_prob:   24.669 prob_neg:   35.118 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:38:37,718 - 0:29:20 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/     9 rec:   26.656 mi: 1.84430611 zkl:  171.186 cd:  -10.783 pos_prob:   25.221 prob_neg:   36.005 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     9 rec:   26.656 mi: 1.84430611 zkl:  171.186 cd:  -10.783 pos_prob:   25.221 prob_neg:   36.005 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:38:46,987 - 0:29:30 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/     9 rec:   25.399 mi: 1.69312274 zkl:  176.645 cd:   -9.203 pos_prob:   24.542 prob_neg:   33.746 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     9 rec:   25.399 mi: 1.69312274 zkl:  176.645 cd:   -9.203 pos_prob:   24.542 prob_neg:   33.746 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:38:56,180 - 0:29:39 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/     9 rec:   28.668 mi: 1.90158367 zkl:  170.840 cd:  -13.836 pos_prob:   23.626 prob_neg:   37.462 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     9 rec:   28.668 mi: 1.90158367 zkl:  170.840 cd:  -13.836 pos_prob:   23.626 prob_neg:   37.462 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:39:05,396 - 0:29:48 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/     9 rec:   34.061 mi: 1.84532571 zkl:  179.308 cd:   -8.924 pos_prob:   24.405 prob_neg:   33.329 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     9 rec:   34.061 mi: 1.84532571 zkl:  179.308 cd:   -8.924 pos_prob:   24.405 prob_neg:   33.329 kl_weight:    0.000 do_ae_train: True
2022-12-28 17:39:05,409 - 0:29:48 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-183.315
Langevin prior   1/ 40: energy=-183.315
2022-12-28 17:39:05,415 - 0:29:48 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-480.113
Langevin prior   6/ 40: energy=-480.113
2022-12-28 17:39:05,420 - 0:29:48 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-702.575
Langevin prior  11/ 40: energy=-702.575
2022-12-28 17:39:05,425 - 0:29:48 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-879.496
Langevin prior  16/ 40: energy=-879.496
2022-12-28 17:39:05,431 - 0:29:48 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1007.008
Langevin prior  21/ 40: energy=-1007.008
2022-12-28 17:39:05,436 - 0:29:48 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1085.934
Langevin prior  26/ 40: energy=-1085.934
2022-12-28 17:39:05,443 - 0:29:48 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1033.686
Langevin prior  31/ 40: energy=-1033.686
2022-12-28 17:39:05,452 - 0:29:48 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1086.727
Langevin prior  36/ 40: energy=-1086.727
2022-12-28 17:39:05,458 - 0:29:48 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1107.771
Langevin prior  40/ 40: energy=-1107.771
2022-12-28 17:39:08,046 - 0:29:51 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-009-test-greedy.txt
Generation: 188 batches
2022-12-28 17:39:13,744 - 0:29:56 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:39:15,233 - 0:29:58 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 17:39:15,233 - 0:29:58 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:39:17,630 - 0:30:00 - 2.4s - INFO - root - --- bleu: BLEU = 31.66, 56.1/34.2/27.6/24.4 (BP=0.938, ratio=0.940, hyp_len=159644, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.66, 56.1/34.2/27.6/24.4 (BP=0.938, ratio=0.940, hyp_len=159644, ref_len=169777)
--- bleu: BLEU = 31.66, 56.1/34.2/27.6/24.4 (BP=0.938, ratio=0.940, hyp_len=159644, ref_len=169777)

2022-12-28 17:39:17,631 - 0:30:00 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:39:17,639 - 0:30:00 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:39:26,932 - 0:30:10 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    10 rec:   26.832 mi: 1.97933757 zkl:  109.456 cd:    6.194 pos_prob:   43.006 prob_neg:   36.811 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    10 rec:   26.832 mi: 1.97933757 zkl:  109.456 cd:    6.194 pos_prob:   43.006 prob_neg:   36.811 kl_weight:    0.062 do_ae_train: False
2022-12-28 17:39:36,350 - 0:30:19 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    10 rec:   17.976 mi: 1.87573040 zkl:   82.083 cd:    1.954 pos_prob:   40.631 prob_neg:   38.677 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    10 rec:   17.976 mi: 1.87573040 zkl:   82.083 cd:    1.954 pos_prob:   40.631 prob_neg:   38.677 kl_weight:    0.125 do_ae_train: False
2022-12-28 17:39:45,526 - 0:30:28 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    10 rec:   25.815 mi: 1.99062073 zkl:   82.505 cd:    0.882 pos_prob:   42.254 prob_neg:   41.371 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    10 rec:   25.815 mi: 1.99062073 zkl:   82.505 cd:    0.882 pos_prob:   42.254 prob_neg:   41.371 kl_weight:    0.188 do_ae_train: False
2022-12-28 17:39:55,136 - 0:30:38 - 9.6s - INFO - root - batch/max_batch/ep:   400/   529/    10 rec:   34.358 mi: 1.81052458 zkl:   66.172 cd:   -0.838 pos_prob:   41.919 prob_neg:   42.757 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    10 rec:   34.358 mi: 1.81052458 zkl:   66.172 cd:   -0.838 pos_prob:   41.919 prob_neg:   42.757 kl_weight:    0.251 do_ae_train: False
2022-12-28 17:40:04,521 - 0:30:47 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    10 rec:   29.726 mi: 1.91419208 zkl:   59.468 cd:   -2.375 pos_prob:   44.608 prob_neg:   46.983 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    10 rec:   29.726 mi: 1.91419208 zkl:   59.468 cd:   -2.375 pos_prob:   44.608 prob_neg:   46.983 kl_weight:    0.314 do_ae_train: False
2022-12-28 17:40:04,534 - 0:30:47 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-182.199
Langevin prior   1/ 40: energy=-182.199
2022-12-28 17:40:04,543 - 0:30:47 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-615.239
Langevin prior   6/ 40: energy=-615.239
2022-12-28 17:40:04,552 - 0:30:47 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-959.227
Langevin prior  11/ 40: energy=-959.227
2022-12-28 17:40:04,562 - 0:30:47 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1192.422
Langevin prior  16/ 40: energy=-1192.422
2022-12-28 17:40:04,572 - 0:30:47 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1237.756
Langevin prior  21/ 40: energy=-1237.756
2022-12-28 17:40:04,584 - 0:30:47 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1357.534
Langevin prior  26/ 40: energy=-1357.534
2022-12-28 17:40:04,596 - 0:30:47 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1398.544
Langevin prior  31/ 40: energy=-1398.544
2022-12-28 17:40:04,609 - 0:30:47 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1387.707
Langevin prior  36/ 40: energy=-1387.707
2022-12-28 17:40:04,619 - 0:30:47 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1346.576
Langevin prior  40/ 40: energy=-1346.576
2022-12-28 17:40:07,399 - 0:30:50 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-010-test-greedy.txt
Generation: 188 batches
2022-12-28 17:40:13,077 - 0:30:56 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:40:14,534 - 0:30:57 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 17:40:14,534 - 0:30:57 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:40:16,955 - 0:31:00 - 2.4s - INFO - root - --- bleu: BLEU = 30.40, 56.3/34.2/27.8/24.6 (BP=0.897, ratio=0.902, hyp_len=153074, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.40, 56.3/34.2/27.8/24.6 (BP=0.897, ratio=0.902, hyp_len=153074, ref_len=169777)
--- bleu: BLEU = 30.40, 56.3/34.2/27.8/24.6 (BP=0.897, ratio=0.902, hyp_len=153074, ref_len=169777)

2022-12-28 17:40:16,955 - 0:31:00 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:40:16,963 - 0:31:00 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:40:26,291 - 0:31:09 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    11 rec:   34.897 mi: 2.02813864 zkl:   58.375 cd:   -0.749 pos_prob:   46.487 prob_neg:   47.235 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    11 rec:   34.897 mi: 2.02813864 zkl:   58.375 cd:   -0.749 pos_prob:   46.487 prob_neg:   47.235 kl_weight:    0.396 do_ae_train: False
2022-12-28 17:40:35,603 - 0:31:18 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    11 rec:   39.555 mi: 1.76765251 zkl:   60.443 cd:    2.237 pos_prob:   48.855 prob_neg:   46.617 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    11 rec:   39.555 mi: 1.76765251 zkl:   60.443 cd:    2.237 pos_prob:   48.855 prob_neg:   46.617 kl_weight:    0.459 do_ae_train: False
2022-12-28 17:40:44,824 - 0:31:28 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    11 rec:   18.856 mi: 1.81068611 zkl:   53.120 cd:    1.929 pos_prob:   54.825 prob_neg:   52.896 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    11 rec:   18.856 mi: 1.81068611 zkl:   53.120 cd:    1.929 pos_prob:   54.825 prob_neg:   52.896 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:40:54,079 - 0:31:37 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    11 rec:   24.314 mi: 1.97614241 zkl:   58.639 cd:    2.651 pos_prob:   55.302 prob_neg:   52.651 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    11 rec:   24.314 mi: 1.97614241 zkl:   58.639 cd:    2.651 pos_prob:   55.302 prob_neg:   52.651 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:41:03,411 - 0:31:46 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    11 rec:   36.433 mi: 1.99399662 zkl:   54.308 cd:   -0.864 pos_prob:   52.822 prob_neg:   53.686 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    11 rec:   36.433 mi: 1.99399662 zkl:   54.308 cd:   -0.864 pos_prob:   52.822 prob_neg:   53.686 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:41:03,423 - 0:31:46 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-205.088
Langevin prior   1/ 40: energy=-205.088
2022-12-28 17:41:03,429 - 0:31:46 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-707.992
Langevin prior   6/ 40: energy=-707.992
2022-12-28 17:41:03,434 - 0:31:46 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1162.620
Langevin prior  11/ 40: energy=-1162.620
2022-12-28 17:41:03,439 - 0:31:46 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1385.169
Langevin prior  16/ 40: energy=-1385.169
2022-12-28 17:41:03,444 - 0:31:46 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1543.317
Langevin prior  21/ 40: energy=-1543.317
2022-12-28 17:41:03,451 - 0:31:46 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1615.107
Langevin prior  26/ 40: energy=-1615.107
2022-12-28 17:41:03,459 - 0:31:46 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1589.952
Langevin prior  31/ 40: energy=-1589.952
2022-12-28 17:41:03,467 - 0:31:46 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1621.428
Langevin prior  36/ 40: energy=-1621.428
2022-12-28 17:41:03,474 - 0:31:46 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1662.617
Langevin prior  40/ 40: energy=-1662.617
2022-12-28 17:50:48,931 - 0:41:32 - 585.5s - INFO - root - Negative Log-likehood -142.529263
200 done. -124.6571407488249
400 done. -119.86969212460741
600 done. -119.5595510177151
800 done. -120.1102931922272
1000 done. -119.296274297693
1200 done. -119.92032701360444
1400 done. -120.90588430590536
1600 done. -119.56483740306686
1800 done. -119.1912362671654
2000 done. -132.4416036846901
2200 done. -141.27494859294757
2400 done. -150.59336213845447
2600 done. -159.0307634505111
2800 done. -164.62419361518573
3000 done. -171.79342174470776
3200 done. -176.1414805994657
3400 done. -181.56926513024072
3600 done. -185.08589838096526
3800 done. -189.410810510061
4000 done. -192.47144107711074
4200 done. -185.00363130141997
4400 done. -177.35710310946862
4600 done. -170.56939365563375
4800 done. -164.28530330710376
5000 done. -158.6584417651415
5200 done. -153.34484851022006
5400 done. -148.519410450273
5600 done. -144.02084650292528
Negative Log-likehood -142.529263
2022-12-28 17:50:48,931 - 0:41:32 - 0.0s - INFO - root - log-likelihood:   -142.529
log-likelihood:   -142.529
2022-12-28 17:51:34,200 - 0:42:17 - 45.3s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-011-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 17:51:34,202 - 0:42:17 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-011-test-greedy.txt
Generation: 188 batches
2022-12-28 17:51:39,904 - 0:42:23 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:51:41,380 - 0:42:24 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 17:51:41,380 - 0:42:24 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 17:51:44,003 - 0:42:27 - 2.6s - INFO - root - --- bleu: BLEU = 29.70, 55.2/33.2/26.8/23.7 (BP=0.904, ratio=0.908, hyp_len=154241, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.70, 55.2/33.2/26.8/23.7 (BP=0.904, ratio=0.908, hyp_len=154241, ref_len=169777)
--- bleu: BLEU = 29.70, 55.2/33.2/26.8/23.7 (BP=0.904, ratio=0.908, hyp_len=154241, ref_len=169777)

2022-12-28 17:51:44,004 - 0:42:27 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 17:51:44,012 - 0:42:27 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 17:51:53,196 - 0:42:36 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    12 rec:   32.574 mi: 1.82844770 zkl:   60.105 cd:    5.793 pos_prob:   63.243 prob_neg:   57.450 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    12 rec:   32.574 mi: 1.82844770 zkl:   60.105 cd:    5.793 pos_prob:   63.243 prob_neg:   57.450 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:52:02,419 - 0:42:45 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    12 rec:   32.242 mi: 1.95633638 zkl:   59.306 cd:   -2.041 pos_prob:   58.107 prob_neg:   60.148 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    12 rec:   32.242 mi: 1.95633638 zkl:   59.306 cd:   -2.041 pos_prob:   58.107 prob_neg:   60.148 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:52:11,638 - 0:42:54 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    12 rec:   34.197 mi: 1.96634388 zkl:   59.996 cd:   -0.008 pos_prob:   60.117 prob_neg:   60.125 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    12 rec:   34.197 mi: 1.96634388 zkl:   59.996 cd:   -0.008 pos_prob:   60.117 prob_neg:   60.125 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:52:20,928 - 0:43:04 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    12 rec:   27.372 mi: 1.92013979 zkl:   58.400 cd:    5.169 pos_prob:   65.966 prob_neg:   60.798 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    12 rec:   27.372 mi: 1.92013979 zkl:   58.400 cd:    5.169 pos_prob:   65.966 prob_neg:   60.798 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:52:30,251 - 0:43:13 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    12 rec:   32.200 mi: 1.93377948 zkl:   58.756 cd:    4.401 pos_prob:   66.681 prob_neg:   62.280 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    12 rec:   32.200 mi: 1.93377948 zkl:   58.756 cd:    4.401 pos_prob:   66.681 prob_neg:   62.280 kl_weight:    0.500 do_ae_train: False
2022-12-28 17:52:30,264 - 0:43:13 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-210.098
Langevin prior   1/ 40: energy=-210.098
2022-12-28 17:52:30,269 - 0:43:13 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-972.943
Langevin prior   6/ 40: energy=-972.943
2022-12-28 17:52:30,275 - 0:43:13 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1325.258
Langevin prior  11/ 40: energy=-1325.258
2022-12-28 17:52:30,280 - 0:43:13 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1636.195
Langevin prior  16/ 40: energy=-1636.195
2022-12-28 17:52:30,285 - 0:43:13 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1788.303
Langevin prior  21/ 40: energy=-1788.303
2022-12-28 17:52:30,291 - 0:43:13 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1888.452
Langevin prior  26/ 40: energy=-1888.452
2022-12-28 17:52:30,298 - 0:43:13 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1957.398
Langevin prior  31/ 40: energy=-1957.398
2022-12-28 17:52:30,306 - 0:43:13 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1912.917
Langevin prior  36/ 40: energy=-1912.917
2022-12-28 17:52:30,313 - 0:43:13 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1998.321
Langevin prior  40/ 40: energy=-1998.321
2022-12-28 18:02:15,541 - 0:52:58 - 585.2s - INFO - root - Negative Log-likehood -144.311726
200 done. -127.04676289198893
400 done. -122.12909526633213
600 done. -121.63627901763878
800 done. -122.32794089583396
1000 done. -121.44146293871492
1200 done. -122.06199421124953
1400 done. -123.15252766907439
1600 done. -121.73034389205073
1800 done. -121.34787036275577
2000 done. -134.8420934364519
2200 done. -143.71072728240742
2400 done. -153.23892451451107
2600 done. -161.7440762104782
2800 done. -167.46574414296433
3000 done. -174.65706157306562
3200 done. -179.18155931602956
3400 done. -184.5393934938018
3600 done. -188.11585026415568
3800 done. -192.4851239364581
4000 done. -195.57161379344657
4200 done. -187.9246429235862
4400 done. -180.07597924185873
4600 done. -173.11040342240798
4800 done. -166.6578527110334
5000 done. -160.8730741430171
5200 done. -155.42881653956303
5400 done. -150.47745265704796
5600 done. -145.85961686162943
Negative Log-likehood -144.311726
2022-12-28 18:02:15,542 - 0:52:58 - 0.0s - INFO - root - log-likelihood:   -144.312
log-likelihood:   -144.312
2022-12-28 18:03:00,466 - 0:53:43 - 44.9s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-012-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 18:03:00,468 - 0:53:43 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-012-test-greedy.txt
Generation: 188 batches
2022-12-28 18:03:06,168 - 0:53:49 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:03:07,642 - 0:53:50 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:03:07,642 - 0:53:50 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:03:10,019 - 0:53:53 - 2.4s - INFO - root - --- bleu: BLEU = 28.95, 53.5/31.9/25.6/22.6 (BP=0.919, ratio=0.922, hyp_len=156482, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 28.95, 53.5/31.9/25.6/22.6 (BP=0.919, ratio=0.922, hyp_len=156482, ref_len=169777)
--- bleu: BLEU = 28.95, 53.5/31.9/25.6/22.6 (BP=0.919, ratio=0.922, hyp_len=156482, ref_len=169777)

2022-12-28 18:03:10,019 - 0:53:53 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:03:10,027 - 0:53:53 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:03:19,016 - 0:54:02 - 9.0s - INFO - root - batch/max_batch/ep:   100/   529/    13 rec:   17.006 mi: 1.96040893 zkl:  144.784 cd:   -2.764 pos_prob:   58.708 prob_neg:   61.473 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    13 rec:   17.006 mi: 1.96040893 zkl:  144.784 cd:   -2.764 pos_prob:   58.708 prob_neg:   61.473 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:03:27,975 - 0:54:11 - 9.0s - INFO - root - batch/max_batch/ep:   200/   529/    13 rec:   17.839 mi: 1.90978432 zkl:  148.521 cd:  -13.782 pos_prob:   51.031 prob_neg:   64.813 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    13 rec:   17.839 mi: 1.90978432 zkl:  148.521 cd:  -13.782 pos_prob:   51.031 prob_neg:   64.813 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:03:37,039 - 0:54:20 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    13 rec:   20.150 mi: 1.99533510 zkl:  149.428 cd:  -18.362 pos_prob:   44.803 prob_neg:   63.165 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    13 rec:   20.150 mi: 1.99533510 zkl:  149.428 cd:  -18.362 pos_prob:   44.803 prob_neg:   63.165 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:03:45,935 - 0:54:29 - 8.9s - INFO - root - batch/max_batch/ep:   400/   529/    13 rec:   20.260 mi: 1.86742353 zkl:  155.911 cd:  -16.363 pos_prob:   44.759 prob_neg:   61.123 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    13 rec:   20.260 mi: 1.86742353 zkl:  155.911 cd:  -16.363 pos_prob:   44.759 prob_neg:   61.123 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:03:55,072 - 0:54:38 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    13 rec:   28.010 mi: 2.01726508 zkl:  158.674 cd:  -21.532 pos_prob:   42.685 prob_neg:   64.217 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    13 rec:   28.010 mi: 2.01726508 zkl:  158.674 cd:  -21.532 pos_prob:   42.685 prob_neg:   64.217 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:03:55,084 - 0:54:38 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-200.007
Langevin prior   1/ 40: energy=-200.007
2022-12-28 18:03:55,089 - 0:54:38 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-875.497
Langevin prior   6/ 40: energy=-875.497
2022-12-28 18:03:55,094 - 0:54:38 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1382.653
Langevin prior  11/ 40: energy=-1382.653
2022-12-28 18:03:55,099 - 0:54:38 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1659.996
Langevin prior  16/ 40: energy=-1659.996
2022-12-28 18:03:55,105 - 0:54:38 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1842.829
Langevin prior  21/ 40: energy=-1842.829
2022-12-28 18:03:55,111 - 0:54:38 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1942.067
Langevin prior  26/ 40: energy=-1942.067
2022-12-28 18:03:55,119 - 0:54:38 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1930.063
Langevin prior  31/ 40: energy=-1930.063
2022-12-28 18:03:55,127 - 0:54:38 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1924.227
Langevin prior  36/ 40: energy=-1924.227
2022-12-28 18:03:55,135 - 0:54:38 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1896.242
Langevin prior  40/ 40: energy=-1896.242
2022-12-28 18:03:57,708 - 0:54:40 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-013-test-greedy.txt
Generation: 188 batches
2022-12-28 18:04:03,400 - 0:54:46 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:04:04,868 - 0:54:48 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:04:04,868 - 0:54:48 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:04:07,250 - 0:54:50 - 2.4s - INFO - root - --- bleu: BLEU = 31.24, 56.3/34.3/28.0/24.8 (BP=0.918, ratio=0.921, hyp_len=156426, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.24, 56.3/34.3/28.0/24.8 (BP=0.918, ratio=0.921, hyp_len=156426, ref_len=169777)
--- bleu: BLEU = 31.24, 56.3/34.3/28.0/24.8 (BP=0.918, ratio=0.921, hyp_len=156426, ref_len=169777)

2022-12-28 18:04:07,251 - 0:54:50 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:04:07,259 - 0:54:50 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:04:16,193 - 0:54:59 - 8.9s - INFO - root - batch/max_batch/ep:   100/   529/    14 rec:   23.186 mi: 1.86277592 zkl:  160.422 cd:  -22.107 pos_prob:   39.331 prob_neg:   61.438 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    14 rec:   23.186 mi: 1.86277592 zkl:  160.422 cd:  -22.107 pos_prob:   39.331 prob_neg:   61.438 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:04:25,179 - 0:55:08 - 9.0s - INFO - root - batch/max_batch/ep:   200/   529/    14 rec:   15.768 mi: 1.98312283 zkl:  162.422 cd:  -19.487 pos_prob:   41.596 prob_neg:   61.084 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    14 rec:   15.768 mi: 1.98312283 zkl:  162.422 cd:  -19.487 pos_prob:   41.596 prob_neg:   61.084 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:04:34,153 - 0:55:17 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    14 rec:   14.479 mi: 1.75907588 zkl:  162.758 cd:  -24.989 pos_prob:   37.701 prob_neg:   62.691 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    14 rec:   14.479 mi: 1.75907588 zkl:  162.758 cd:  -24.989 pos_prob:   37.701 prob_neg:   62.691 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:04:43,122 - 0:55:26 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    14 rec:   17.688 mi: 1.94482267 zkl:  165.563 cd:  -25.610 pos_prob:   39.128 prob_neg:   64.738 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    14 rec:   17.688 mi: 1.94482267 zkl:  165.563 cd:  -25.610 pos_prob:   39.128 prob_neg:   64.738 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:04:52,084 - 0:55:35 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/    14 rec:   24.194 mi: 1.97513986 zkl:  164.570 cd:  -25.149 pos_prob:   36.739 prob_neg:   61.888 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    14 rec:   24.194 mi: 1.97513986 zkl:  164.570 cd:  -25.149 pos_prob:   36.739 prob_neg:   61.888 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:04:52,097 - 0:55:35 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-224.368
Langevin prior   1/ 40: energy=-224.368
2022-12-28 18:04:52,102 - 0:55:35 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-941.470
Langevin prior   6/ 40: energy=-941.470
2022-12-28 18:04:52,107 - 0:55:35 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1396.849
Langevin prior  11/ 40: energy=-1396.849
2022-12-28 18:04:52,112 - 0:55:35 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1669.784
Langevin prior  16/ 40: energy=-1669.784
2022-12-28 18:04:52,118 - 0:55:35 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1799.926
Langevin prior  21/ 40: energy=-1799.926
2022-12-28 18:04:52,124 - 0:55:35 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1814.830
Langevin prior  26/ 40: energy=-1814.830
2022-12-28 18:04:52,131 - 0:55:35 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1873.643
Langevin prior  31/ 40: energy=-1873.643
2022-12-28 18:04:52,139 - 0:55:35 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1761.881
Langevin prior  36/ 40: energy=-1761.881
2022-12-28 18:04:52,146 - 0:55:35 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1817.607
Langevin prior  40/ 40: energy=-1817.607
2022-12-28 18:04:54,684 - 0:55:37 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-014-test-greedy.txt
Generation: 188 batches
2022-12-28 18:05:00,353 - 0:55:43 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:05:01,831 - 0:55:45 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:05:01,831 - 0:55:45 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:05:04,221 - 0:55:47 - 2.4s - INFO - root - --- bleu: BLEU = 31.65, 56.5/34.5/28.1/25.0 (BP=0.925, ratio=0.928, hyp_len=157472, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.65, 56.5/34.5/28.1/25.0 (BP=0.925, ratio=0.928, hyp_len=157472, ref_len=169777)
--- bleu: BLEU = 31.65, 56.5/34.5/28.1/25.0 (BP=0.925, ratio=0.928, hyp_len=157472, ref_len=169777)

2022-12-28 18:05:04,222 - 0:55:47 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:05:04,230 - 0:55:47 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:05:13,169 - 0:55:56 - 8.9s - INFO - root - batch/max_batch/ep:   100/   529/    15 rec:   16.886 mi: 1.84730411 zkl:  170.368 cd:  -26.645 pos_prob:   37.712 prob_neg:   64.357 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    15 rec:   16.886 mi: 1.84730411 zkl:  170.368 cd:  -26.645 pos_prob:   37.712 prob_neg:   64.357 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:05:22,236 - 0:56:05 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    15 rec:   15.802 mi: 1.93770027 zkl:  169.613 cd:  -22.878 pos_prob:   39.694 prob_neg:   62.572 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    15 rec:   15.802 mi: 1.93770027 zkl:  169.613 cd:  -22.878 pos_prob:   39.694 prob_neg:   62.572 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:05:31,203 - 0:56:14 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    15 rec:   15.556 mi: 1.84448719 zkl:  175.288 cd:  -21.517 pos_prob:   40.273 prob_neg:   61.790 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    15 rec:   15.556 mi: 1.84448719 zkl:  175.288 cd:  -21.517 pos_prob:   40.273 prob_neg:   61.790 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:05:40,205 - 0:56:23 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    15 rec:   17.875 mi: 1.76127315 zkl:  169.377 cd:  -26.466 pos_prob:   38.369 prob_neg:   64.835 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    15 rec:   17.875 mi: 1.76127315 zkl:  169.377 cd:  -26.466 pos_prob:   38.369 prob_neg:   64.835 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:05:49,327 - 0:56:32 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    15 rec:   17.982 mi: 1.93474782 zkl:  170.997 cd:  -25.918 pos_prob:   36.792 prob_neg:   62.710 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    15 rec:   17.982 mi: 1.93474782 zkl:  170.997 cd:  -25.918 pos_prob:   36.792 prob_neg:   62.710 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:05:49,340 - 0:56:32 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-201.973
Langevin prior   1/ 40: energy=-201.973
2022-12-28 18:05:49,345 - 0:56:32 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-830.573
Langevin prior   6/ 40: energy=-830.573
2022-12-28 18:05:49,351 - 0:56:32 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1317.114
Langevin prior  11/ 40: energy=-1317.114
2022-12-28 18:05:49,356 - 0:56:32 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1532.600
Langevin prior  16/ 40: energy=-1532.600
2022-12-28 18:05:49,363 - 0:56:32 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1667.408
Langevin prior  21/ 40: energy=-1667.408
2022-12-28 18:05:49,371 - 0:56:32 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1759.077
Langevin prior  26/ 40: energy=-1759.077
2022-12-28 18:05:49,380 - 0:56:32 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1842.009
Langevin prior  31/ 40: energy=-1842.009
2022-12-28 18:05:49,389 - 0:56:32 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1852.387
Langevin prior  36/ 40: energy=-1852.387
2022-12-28 18:05:49,396 - 0:56:32 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1828.555
Langevin prior  40/ 40: energy=-1828.555
2022-12-28 18:05:51,969 - 0:56:35 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-015-test-greedy.txt
Generation: 188 batches
2022-12-28 18:05:57,631 - 0:56:40 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:05:59,128 - 0:56:42 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:05:59,128 - 0:56:42 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:06:01,544 - 0:56:44 - 2.4s - INFO - root - --- bleu: BLEU = 31.73, 55.7/33.8/27.4/24.2 (BP=0.949, ratio=0.950, hyp_len=161319, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.73, 55.7/33.8/27.4/24.2 (BP=0.949, ratio=0.950, hyp_len=161319, ref_len=169777)
--- bleu: BLEU = 31.73, 55.7/33.8/27.4/24.2 (BP=0.949, ratio=0.950, hyp_len=161319, ref_len=169777)

2022-12-28 18:06:01,544 - 0:56:44 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:06:01,552 - 0:56:44 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:06:10,748 - 0:56:53 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    16 rec:   12.949 mi: 1.88621616 zkl:  109.052 cd:   -8.723 pos_prob:   57.847 prob_neg:   66.570 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    16 rec:   12.949 mi: 1.88621616 zkl:  109.052 cd:   -8.723 pos_prob:   57.847 prob_neg:   66.570 kl_weight:    0.062 do_ae_train: False
2022-12-28 18:06:19,982 - 0:57:03 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    16 rec:   23.391 mi: 1.85197771 zkl:   86.306 cd:   -9.840 pos_prob:   57.720 prob_neg:   67.560 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    16 rec:   23.391 mi: 1.85197771 zkl:   86.306 cd:   -9.840 pos_prob:   57.720 prob_neg:   67.560 kl_weight:    0.125 do_ae_train: False
2022-12-28 18:06:29,172 - 0:57:12 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    16 rec:   18.571 mi: 1.88255548 zkl:   83.575 cd:   -4.415 pos_prob:   59.672 prob_neg:   64.087 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    16 rec:   18.571 mi: 1.88255548 zkl:   83.575 cd:   -4.415 pos_prob:   59.672 prob_neg:   64.087 kl_weight:    0.188 do_ae_train: False
2022-12-28 18:06:38,374 - 0:57:21 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    16 rec:   21.594 mi: 1.95950270 zkl:   79.680 cd:   -0.661 pos_prob:   62.314 prob_neg:   62.975 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    16 rec:   21.594 mi: 1.95950270 zkl:   79.680 cd:   -0.661 pos_prob:   62.314 prob_neg:   62.975 kl_weight:    0.251 do_ae_train: False
2022-12-28 18:06:47,565 - 0:57:30 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    16 rec:   21.586 mi: 1.82960081 zkl:   70.138 cd:   -0.759 pos_prob:   63.178 prob_neg:   63.937 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    16 rec:   21.586 mi: 1.82960081 zkl:   70.138 cd:   -0.759 pos_prob:   63.178 prob_neg:   63.937 kl_weight:    0.314 do_ae_train: False
2022-12-28 18:06:47,578 - 0:57:30 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-251.755
Langevin prior   1/ 40: energy=-251.755
2022-12-28 18:06:47,583 - 0:57:30 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-877.102
Langevin prior   6/ 40: energy=-877.102
2022-12-28 18:06:47,588 - 0:57:30 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1430.222
Langevin prior  11/ 40: energy=-1430.222
2022-12-28 18:06:47,594 - 0:57:30 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1644.274
Langevin prior  16/ 40: energy=-1644.274
2022-12-28 18:06:47,599 - 0:57:30 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1806.582
Langevin prior  21/ 40: energy=-1806.582
2022-12-28 18:06:47,606 - 0:57:30 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1846.962
Langevin prior  26/ 40: energy=-1846.962
2022-12-28 18:06:47,615 - 0:57:30 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1846.924
Langevin prior  31/ 40: energy=-1846.924
2022-12-28 18:06:47,624 - 0:57:30 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1854.437
Langevin prior  36/ 40: energy=-1854.437
2022-12-28 18:06:47,632 - 0:57:30 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1869.212
Langevin prior  40/ 40: energy=-1869.212
2022-12-28 18:06:50,223 - 0:57:33 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-016-test-greedy.txt
Generation: 188 batches
2022-12-28 18:06:55,928 - 0:57:39 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:06:57,440 - 0:57:40 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:06:57,441 - 0:57:40 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:06:59,874 - 0:57:43 - 2.4s - INFO - root - --- bleu: BLEU = 30.03, 53.5/31.8/25.6/22.6 (BP=0.953, ratio=0.954, hyp_len=161944, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.03, 53.5/31.8/25.6/22.6 (BP=0.953, ratio=0.954, hyp_len=161944, ref_len=169777)
--- bleu: BLEU = 30.03, 53.5/31.8/25.6/22.6 (BP=0.953, ratio=0.954, hyp_len=161944, ref_len=169777)

2022-12-28 18:06:59,874 - 0:57:43 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:06:59,882 - 0:57:43 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:07:09,111 - 0:57:52 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    17 rec:   19.128 mi: 1.89773858 zkl:   62.874 cd:   -2.894 pos_prob:   62.015 prob_neg:   64.910 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    17 rec:   19.128 mi: 1.89773858 zkl:   62.874 cd:   -2.894 pos_prob:   62.015 prob_neg:   64.910 kl_weight:    0.396 do_ae_train: False
2022-12-28 18:07:18,389 - 0:58:01 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    17 rec:   24.422 mi: 1.99770856 zkl:   63.590 cd:   -1.709 pos_prob:   63.130 prob_neg:   64.839 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    17 rec:   24.422 mi: 1.99770856 zkl:   63.590 cd:   -1.709 pos_prob:   63.130 prob_neg:   64.839 kl_weight:    0.459 do_ae_train: False
2022-12-28 18:07:27,580 - 0:58:10 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    17 rec:   21.370 mi: 2.03755045 zkl:   62.136 cd:    5.235 pos_prob:   68.000 prob_neg:   62.764 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    17 rec:   21.370 mi: 2.03755045 zkl:   62.136 cd:    5.235 pos_prob:   68.000 prob_neg:   62.764 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:07:36,809 - 0:58:20 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    17 rec:   29.381 mi: 1.94422901 zkl:   61.935 cd:    2.471 pos_prob:   69.366 prob_neg:   66.895 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    17 rec:   29.381 mi: 1.94422901 zkl:   61.935 cd:    2.471 pos_prob:   69.366 prob_neg:   66.895 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:07:45,951 - 0:58:29 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    17 rec:   22.755 mi: 1.71261549 zkl:   69.745 cd:    5.818 pos_prob:   78.195 prob_neg:   72.377 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    17 rec:   22.755 mi: 1.71261549 zkl:   69.745 cd:    5.818 pos_prob:   78.195 prob_neg:   72.377 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:07:45,963 - 0:58:29 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-268.448
Langevin prior   1/ 40: energy=-268.448
2022-12-28 18:07:45,969 - 0:58:29 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1045.433
Langevin prior   6/ 40: energy=-1045.433
2022-12-28 18:07:45,975 - 0:58:29 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1652.037
Langevin prior  11/ 40: energy=-1652.037
2022-12-28 18:07:45,980 - 0:58:29 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1879.153
Langevin prior  16/ 40: energy=-1879.153
2022-12-28 18:07:45,987 - 0:58:29 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1979.888
Langevin prior  21/ 40: energy=-1979.888
2022-12-28 18:07:45,994 - 0:58:29 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2043.313
Langevin prior  26/ 40: energy=-2043.313
2022-12-28 18:07:46,003 - 0:58:29 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2136.813
Langevin prior  31/ 40: energy=-2136.813
2022-12-28 18:07:46,013 - 0:58:29 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2185.212
Langevin prior  36/ 40: energy=-2185.212
2022-12-28 18:07:46,020 - 0:58:29 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2120.142
Langevin prior  40/ 40: energy=-2120.142
2022-12-28 18:17:31,630 - 1:08:14 - 585.6s - INFO - root - Negative Log-likehood -150.790854
200 done. -130.29278727673827
400 done. -125.46845819244457
600 done. -125.29782610699031
800 done. -125.91378397933627
1000 done. -124.99818708525115
1200 done. -125.638391688287
1400 done. -126.79496475249236
1600 done. -125.35645052926569
1800 done. -124.94247390445899
2000 done. -139.22423803787385
2200 done. -148.9271833881813
2400 done. -159.0009448508139
2600 done. -168.01893358561617
2800 done. -174.2322576315305
3000 done. -181.88088228766304
3200 done. -186.73908320410885
3400 done. -192.51089527492582
3600 done. -196.2651569185177
3800 done. -200.96869118847343
4000 done. -204.34175206388474
4200 done. -196.3629348273358
4400 done. -188.16012803628888
4600 done. -180.87375788412965
4800 done. -174.1160571462801
5000 done. -168.07878258960207
5200 done. -162.38888659517448
5400 done. -157.20425603956963
5600 done. -152.40116743444753
Negative Log-likehood -150.790854
2022-12-28 18:17:31,631 - 1:08:14 - 0.0s - INFO - root - log-likelihood:   -150.791
log-likelihood:   -150.791
2022-12-28 18:18:17,046 - 1:09:00 - 45.4s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-017-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 18:18:17,048 - 1:09:00 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-017-test-greedy.txt
Generation: 188 batches
2022-12-28 18:18:22,732 - 1:09:05 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:18:24,247 - 1:09:07 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:18:24,247 - 1:09:07 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:18:26,678 - 1:09:09 - 2.4s - INFO - root - --- bleu: BLEU = 29.47, 53.8/32.2/26.0/22.9 (BP=0.925, ratio=0.927, hyp_len=157438, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.47, 53.8/32.2/26.0/22.9 (BP=0.925, ratio=0.927, hyp_len=157438, ref_len=169777)
--- bleu: BLEU = 29.47, 53.8/32.2/26.0/22.9 (BP=0.925, ratio=0.927, hyp_len=157438, ref_len=169777)

2022-12-28 18:18:26,679 - 1:09:09 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:18:26,687 - 1:09:09 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:18:35,944 - 1:09:19 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    18 rec:   19.181 mi: 1.81928384 zkl:   62.716 cd:    2.195 pos_prob:   71.293 prob_neg:   69.097 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    18 rec:   19.181 mi: 1.81928384 zkl:   62.716 cd:    2.195 pos_prob:   71.293 prob_neg:   69.097 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:18:45,107 - 1:09:28 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    18 rec:   29.529 mi: 2.01742339 zkl:   64.580 cd:   -1.636 pos_prob:   70.602 prob_neg:   72.238 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    18 rec:   29.529 mi: 2.01742339 zkl:   64.580 cd:   -1.636 pos_prob:   70.602 prob_neg:   72.238 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:18:54,373 - 1:09:37 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    18 rec:   17.157 mi: 1.90199006 zkl:   61.227 cd:    5.776 pos_prob:   74.731 prob_neg:   68.954 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    18 rec:   17.157 mi: 1.90199006 zkl:   61.227 cd:    5.776 pos_prob:   74.731 prob_neg:   68.954 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:19:03,703 - 1:09:46 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    18 rec:   32.796 mi: 1.89110589 zkl:   63.319 cd:    4.164 pos_prob:   74.728 prob_neg:   70.564 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    18 rec:   32.796 mi: 1.89110589 zkl:   63.319 cd:    4.164 pos_prob:   74.728 prob_neg:   70.564 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:19:13,206 - 1:09:56 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    18 rec:   22.022 mi: 1.90356171 zkl:   65.589 cd:   -3.336 pos_prob:   74.687 prob_neg:   78.024 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    18 rec:   22.022 mi: 1.90356171 zkl:   65.589 cd:   -3.336 pos_prob:   74.687 prob_neg:   78.024 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:19:13,219 - 1:09:56 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-252.520
Langevin prior   1/ 40: energy=-252.520
2022-12-28 18:19:13,224 - 1:09:56 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1104.214
Langevin prior   6/ 40: energy=-1104.214
2022-12-28 18:19:13,230 - 1:09:56 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1708.631
Langevin prior  11/ 40: energy=-1708.631
2022-12-28 18:19:13,235 - 1:09:56 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2019.438
Langevin prior  16/ 40: energy=-2019.438
2022-12-28 18:19:13,241 - 1:09:56 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2173.322
Langevin prior  21/ 40: energy=-2173.322
2022-12-28 18:19:13,246 - 1:09:56 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2217.100
Langevin prior  26/ 40: energy=-2217.100
2022-12-28 18:19:13,254 - 1:09:56 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2278.303
Langevin prior  31/ 40: energy=-2278.303
2022-12-28 18:19:13,262 - 1:09:56 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2294.442
Langevin prior  36/ 40: energy=-2294.442
2022-12-28 18:19:13,269 - 1:09:56 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2366.368
Langevin prior  40/ 40: energy=-2366.368
2022-12-28 18:28:59,345 - 1:19:42 - 586.1s - INFO - root - Negative Log-likehood -152.871704
200 done. -132.09056936309574
400 done. -127.16004562682433
600 done. -126.88339310518528
800 done. -127.58897695940065
1000 done. -126.56828496255117
1200 done. -127.25916981209872
1400 done. -128.45106111833536
1600 done. -127.00492185511959
1800 done. -126.62276891594598
2000 done. -141.26073722785435
2200 done. -151.13181687265399
2400 done. -161.37991962276826
2600 done. -170.5457958556743
2800 done. -176.77202770417713
3000 done. -184.61010189323326
3200 done. -189.4866635894604
3400 done. -195.42826785303768
3600 done. -199.22839901171398
3800 done. -203.9935544742407
4000 done. -207.3690897200721
4200 done. -199.23211873439746
4400 done. -190.90310284259806
4600 done. -183.50314016058337
4800 done. -176.63068075676387
5000 done. -170.48295079813374
5200 done. -164.6948107008701
5400 done. -159.41292467283878
5600 done. -154.51063732425942
Negative Log-likehood -152.871704
2022-12-28 18:28:59,345 - 1:19:42 - 0.0s - INFO - root - log-likelihood:   -152.872
log-likelihood:   -152.872
2022-12-28 18:29:44,603 - 1:20:27 - 45.3s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-018-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 18:29:44,606 - 1:20:27 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-018-test-greedy.txt
Generation: 188 batches
2022-12-28 18:29:50,283 - 1:20:33 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:29:51,770 - 1:20:34 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:29:51,770 - 1:20:34 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:29:54,409 - 1:20:37 - 2.6s - INFO - root - --- bleu: BLEU = 29.25, 53.9/32.3/26.1/23.0 (BP=0.915, ratio=0.919, hyp_len=155966, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.25, 53.9/32.3/26.1/23.0 (BP=0.915, ratio=0.919, hyp_len=155966, ref_len=169777)
--- bleu: BLEU = 29.25, 53.9/32.3/26.1/23.0 (BP=0.915, ratio=0.919, hyp_len=155966, ref_len=169777)

2022-12-28 18:29:54,410 - 1:20:37 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:29:54,418 - 1:20:37 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:30:03,374 - 1:20:46 - 9.0s - INFO - root - batch/max_batch/ep:   100/   529/    19 rec:   16.577 mi: 2.05068588 zkl:  147.766 cd:   -3.824 pos_prob:   64.582 prob_neg:   68.406 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    19 rec:   16.577 mi: 2.05068588 zkl:  147.766 cd:   -3.824 pos_prob:   64.582 prob_neg:   68.406 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:30:12,529 - 1:20:55 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    19 rec:   17.089 mi: 1.94834900 zkl:  155.955 cd:  -16.213 pos_prob:   61.720 prob_neg:   77.933 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    19 rec:   17.089 mi: 1.94834900 zkl:  155.955 cd:  -16.213 pos_prob:   61.720 prob_neg:   77.933 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:30:21,629 - 1:21:04 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    19 rec:   16.650 mi: 1.80763054 zkl:  157.837 cd:  -15.912 pos_prob:   57.564 prob_neg:   73.477 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    19 rec:   16.650 mi: 1.80763054 zkl:  157.837 cd:  -15.912 pos_prob:   57.564 prob_neg:   73.477 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:30:30,669 - 1:21:13 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    19 rec:   16.770 mi: 1.78831220 zkl:  159.669 cd:  -22.927 pos_prob:   56.011 prob_neg:   78.938 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    19 rec:   16.770 mi: 1.78831220 zkl:  159.669 cd:  -22.927 pos_prob:   56.011 prob_neg:   78.938 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:30:39,658 - 1:21:22 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/    19 rec:   19.844 mi: 2.00595450 zkl:  161.327 cd:  -27.114 pos_prob:   47.422 prob_neg:   74.536 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    19 rec:   19.844 mi: 2.00595450 zkl:  161.327 cd:  -27.114 pos_prob:   47.422 prob_neg:   74.536 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:30:39,671 - 1:21:22 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-250.457
Langevin prior   1/ 40: energy=-250.457
2022-12-28 18:30:39,676 - 1:21:22 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1149.150
Langevin prior   6/ 40: energy=-1149.150
2022-12-28 18:30:39,681 - 1:21:22 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1782.531
Langevin prior  11/ 40: energy=-1782.531
2022-12-28 18:30:39,686 - 1:21:22 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2033.644
Langevin prior  16/ 40: energy=-2033.644
2022-12-28 18:30:39,692 - 1:21:22 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2136.979
Langevin prior  21/ 40: energy=-2136.979
2022-12-28 18:30:39,712 - 1:21:22 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2267.949
Langevin prior  26/ 40: energy=-2267.949
2022-12-28 18:30:39,723 - 1:21:22 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2366.779
Langevin prior  31/ 40: energy=-2366.779
2022-12-28 18:30:39,733 - 1:21:22 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2365.591
Langevin prior  36/ 40: energy=-2365.591
2022-12-28 18:30:39,742 - 1:21:22 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2392.783
Langevin prior  40/ 40: energy=-2392.783
2022-12-28 18:30:42,290 - 1:21:25 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-019-test-greedy.txt
Generation: 188 batches
2022-12-28 18:30:47,997 - 1:21:31 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:30:49,513 - 1:21:32 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:30:49,513 - 1:21:32 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:30:51,945 - 1:21:35 - 2.4s - INFO - root - --- bleu: BLEU = 31.58, 55.6/34.1/27.8/24.7 (BP=0.935, ratio=0.937, hyp_len=159121, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.58, 55.6/34.1/27.8/24.7 (BP=0.935, ratio=0.937, hyp_len=159121, ref_len=169777)
--- bleu: BLEU = 31.58, 55.6/34.1/27.8/24.7 (BP=0.935, ratio=0.937, hyp_len=159121, ref_len=169777)

2022-12-28 18:30:51,946 - 1:21:35 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:30:51,954 - 1:21:35 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:31:01,066 - 1:21:44 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    20 rec:   13.021 mi: 1.89304841 zkl:  162.632 cd:  -21.331 pos_prob:   52.920 prob_neg:   74.251 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    20 rec:   13.021 mi: 1.89304841 zkl:  162.632 cd:  -21.331 pos_prob:   52.920 prob_neg:   74.251 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:31:10,104 - 1:21:53 - 9.0s - INFO - root - batch/max_batch/ep:   200/   529/    20 rec:   11.887 mi: 1.79560912 zkl:  163.719 cd:  -25.726 pos_prob:   51.113 prob_neg:   76.839 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    20 rec:   11.887 mi: 1.79560912 zkl:  163.719 cd:  -25.726 pos_prob:   51.113 prob_neg:   76.839 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:31:19,059 - 1:22:02 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    20 rec:   20.462 mi: 1.94359124 zkl:  162.293 cd:  -34.571 pos_prob:   45.821 prob_neg:   80.392 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    20 rec:   20.462 mi: 1.94359124 zkl:  162.293 cd:  -34.571 pos_prob:   45.821 prob_neg:   80.392 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:31:28,099 - 1:22:11 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    20 rec:   14.001 mi: 1.87887096 zkl:  162.804 cd:  -34.244 pos_prob:   41.435 prob_neg:   75.679 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    20 rec:   14.001 mi: 1.87887096 zkl:  162.804 cd:  -34.244 pos_prob:   41.435 prob_neg:   75.679 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:31:37,292 - 1:22:20 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    20 rec:   10.953 mi: 1.77206111 zkl:  167.149 cd:  -26.876 pos_prob:   46.265 prob_neg:   73.141 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    20 rec:   10.953 mi: 1.77206111 zkl:  167.149 cd:  -26.876 pos_prob:   46.265 prob_neg:   73.141 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:31:37,305 - 1:22:20 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-285.443
Langevin prior   1/ 40: energy=-285.443
2022-12-28 18:31:37,311 - 1:22:20 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1253.233
Langevin prior   6/ 40: energy=-1253.233
2022-12-28 18:31:37,316 - 1:22:20 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1861.740
Langevin prior  11/ 40: energy=-1861.740
2022-12-28 18:31:37,321 - 1:22:20 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2166.696
Langevin prior  16/ 40: energy=-2166.696
2022-12-28 18:31:37,327 - 1:22:20 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2358.814
Langevin prior  21/ 40: energy=-2358.814
2022-12-28 18:31:37,333 - 1:22:20 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2388.315
Langevin prior  26/ 40: energy=-2388.315
2022-12-28 18:31:37,340 - 1:22:20 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2376.181
Langevin prior  31/ 40: energy=-2376.181
2022-12-28 18:31:37,347 - 1:22:20 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2506.994
Langevin prior  36/ 40: energy=-2506.994
2022-12-28 18:31:37,354 - 1:22:20 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2488.229
Langevin prior  40/ 40: energy=-2488.229
2022-12-28 18:31:39,964 - 1:22:23 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-020-test-greedy.txt
Generation: 188 batches
2022-12-28 18:31:45,660 - 1:22:28 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:31:47,183 - 1:22:30 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:31:47,183 - 1:22:30 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:31:49,631 - 1:22:32 - 2.4s - INFO - root - --- bleu: BLEU = 31.67, 55.6/34.2/27.8/24.6 (BP=0.938, ratio=0.940, hyp_len=159596, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.67, 55.6/34.2/27.8/24.6 (BP=0.938, ratio=0.940, hyp_len=159596, ref_len=169777)
--- bleu: BLEU = 31.67, 55.6/34.2/27.8/24.6 (BP=0.938, ratio=0.940, hyp_len=159596, ref_len=169777)

2022-12-28 18:31:49,631 - 1:22:32 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:31:49,640 - 1:22:32 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:31:58,638 - 1:22:41 - 9.0s - INFO - root - batch/max_batch/ep:   100/   529/    21 rec:   12.712 mi: 1.94644022 zkl:  165.225 cd:  -34.824 pos_prob:   40.659 prob_neg:   75.483 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    21 rec:   12.712 mi: 1.94644022 zkl:  165.225 cd:  -34.824 pos_prob:   40.659 prob_neg:   75.483 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:32:07,606 - 1:22:50 - 9.0s - INFO - root - batch/max_batch/ep:   200/   529/    21 rec:   11.301 mi: 1.80054057 zkl:  168.512 cd:  -29.385 pos_prob:   44.144 prob_neg:   73.529 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    21 rec:   11.301 mi: 1.80054057 zkl:  168.512 cd:  -29.385 pos_prob:   44.144 prob_neg:   73.529 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:32:16,616 - 1:22:59 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    21 rec:   18.515 mi: 1.93677688 zkl:  170.406 cd:  -37.564 pos_prob:   39.222 prob_neg:   76.786 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    21 rec:   18.515 mi: 1.93677688 zkl:  170.406 cd:  -37.564 pos_prob:   39.222 prob_neg:   76.786 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:32:25,631 - 1:23:08 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    21 rec:   23.038 mi: 1.90868604 zkl:  167.432 cd:  -33.417 pos_prob:   38.968 prob_neg:   72.385 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    21 rec:   23.038 mi: 1.90868604 zkl:  167.432 cd:  -33.417 pos_prob:   38.968 prob_neg:   72.385 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:32:34,659 - 1:23:17 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/    21 rec:   18.254 mi: 1.99174237 zkl:  169.810 cd:  -35.506 pos_prob:   43.513 prob_neg:   79.019 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    21 rec:   18.254 mi: 1.99174237 zkl:  169.810 cd:  -35.506 pos_prob:   43.513 prob_neg:   79.019 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:32:34,672 - 1:23:17 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-280.261
Langevin prior   1/ 40: energy=-280.261
2022-12-28 18:32:34,677 - 1:23:17 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1119.604
Langevin prior   6/ 40: energy=-1119.604
2022-12-28 18:32:34,683 - 1:23:17 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1737.590
Langevin prior  11/ 40: energy=-1737.590
2022-12-28 18:32:34,688 - 1:23:17 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2107.030
Langevin prior  16/ 40: energy=-2107.030
2022-12-28 18:32:34,693 - 1:23:17 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2196.464
Langevin prior  21/ 40: energy=-2196.464
2022-12-28 18:32:34,700 - 1:23:17 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2306.960
Langevin prior  26/ 40: energy=-2306.960
2022-12-28 18:32:34,707 - 1:23:17 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2322.796
Langevin prior  31/ 40: energy=-2322.796
2022-12-28 18:32:34,716 - 1:23:17 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2355.821
Langevin prior  36/ 40: energy=-2355.821
2022-12-28 18:32:34,723 - 1:23:17 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2330.575
Langevin prior  40/ 40: energy=-2330.575
2022-12-28 18:32:37,270 - 1:23:20 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-021-test-greedy.txt
Generation: 188 batches
2022-12-28 18:32:42,936 - 1:23:26 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:32:44,438 - 1:23:27 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:32:44,438 - 1:23:27 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:32:46,846 - 1:23:30 - 2.4s - INFO - root - --- bleu: BLEU = 31.80, 56.2/34.5/28.3/25.2 (BP=0.928, ratio=0.930, hyp_len=157928, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.80, 56.2/34.5/28.3/25.2 (BP=0.928, ratio=0.930, hyp_len=157928, ref_len=169777)
--- bleu: BLEU = 31.80, 56.2/34.5/28.3/25.2 (BP=0.928, ratio=0.930, hyp_len=157928, ref_len=169777)

2022-12-28 18:32:46,847 - 1:23:30 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:32:46,855 - 1:23:30 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:32:56,079 - 1:23:39 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    22 rec:   10.746 mi: 1.94280267 zkl:   90.642 cd:  -10.838 pos_prob:   65.688 prob_neg:   76.527 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    22 rec:   10.746 mi: 1.94280267 zkl:   90.642 cd:  -10.838 pos_prob:   65.688 prob_neg:   76.527 kl_weight:    0.062 do_ae_train: False
2022-12-28 18:33:05,277 - 1:23:48 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    22 rec:   14.322 mi: 1.80118632 zkl:   88.458 cd:   -1.642 pos_prob:   68.201 prob_neg:   69.843 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    22 rec:   14.322 mi: 1.80118632 zkl:   88.458 cd:   -1.642 pos_prob:   68.201 prob_neg:   69.843 kl_weight:    0.125 do_ae_train: False
2022-12-28 18:33:14,506 - 1:23:57 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    22 rec:   12.988 mi: 2.02704024 zkl:   76.554 cd:   -2.539 pos_prob:   62.520 prob_neg:   65.059 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    22 rec:   12.988 mi: 2.02704024 zkl:   76.554 cd:   -2.539 pos_prob:   62.520 prob_neg:   65.059 kl_weight:    0.188 do_ae_train: False
2022-12-28 18:33:23,693 - 1:24:06 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    22 rec:   21.393 mi: 1.70344770 zkl:   73.124 cd:   -5.415 pos_prob:   57.330 prob_neg:   62.745 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    22 rec:   21.393 mi: 1.70344770 zkl:   73.124 cd:   -5.415 pos_prob:   57.330 prob_neg:   62.745 kl_weight:    0.251 do_ae_train: False
2022-12-28 18:33:33,057 - 1:24:16 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    22 rec:   19.649 mi: 1.86793399 zkl:   69.641 cd:    2.357 pos_prob:   65.286 prob_neg:   62.928 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    22 rec:   19.649 mi: 1.86793399 zkl:   69.641 cd:    2.357 pos_prob:   65.286 prob_neg:   62.928 kl_weight:    0.314 do_ae_train: False
2022-12-28 18:33:33,069 - 1:24:16 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-247.627
Langevin prior   1/ 40: energy=-247.627
2022-12-28 18:33:33,075 - 1:24:16 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1068.553
Langevin prior   6/ 40: energy=-1068.553
2022-12-28 18:33:33,080 - 1:24:16 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1503.201
Langevin prior  11/ 40: energy=-1503.201
2022-12-28 18:33:33,086 - 1:24:16 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1772.243
Langevin prior  16/ 40: energy=-1772.243
2022-12-28 18:33:33,091 - 1:24:16 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1925.512
Langevin prior  21/ 40: energy=-1925.512
2022-12-28 18:33:33,097 - 1:24:16 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2010.508
Langevin prior  26/ 40: energy=-2010.508
2022-12-28 18:33:33,105 - 1:24:16 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1996.903
Langevin prior  31/ 40: energy=-1996.903
2022-12-28 18:33:33,114 - 1:24:16 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1970.228
Langevin prior  36/ 40: energy=-1970.228
2022-12-28 18:33:33,121 - 1:24:16 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1982.845
Langevin prior  40/ 40: energy=-1982.845
2022-12-28 18:33:35,736 - 1:24:18 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-022-test-greedy.txt
Generation: 188 batches
2022-12-28 18:33:41,425 - 1:24:24 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:33:42,921 - 1:24:26 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:33:42,921 - 1:24:26 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:33:45,328 - 1:24:28 - 2.4s - INFO - root - --- bleu: BLEU = 30.15, 54.8/33.0/26.7/23.6 (BP=0.922, ratio=0.925, hyp_len=157042, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.15, 54.8/33.0/26.7/23.6 (BP=0.922, ratio=0.925, hyp_len=157042, ref_len=169777)
--- bleu: BLEU = 30.15, 54.8/33.0/26.7/23.6 (BP=0.922, ratio=0.925, hyp_len=157042, ref_len=169777)

2022-12-28 18:33:45,329 - 1:24:28 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:33:45,337 - 1:24:28 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:33:54,633 - 1:24:37 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    23 rec:   17.464 mi: 1.81039202 zkl:   58.034 cd:   -1.497 pos_prob:   62.632 prob_neg:   64.129 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    23 rec:   17.464 mi: 1.81039202 zkl:   58.034 cd:   -1.497 pos_prob:   62.632 prob_neg:   64.129 kl_weight:    0.396 do_ae_train: False
2022-12-28 18:34:03,876 - 1:24:47 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    23 rec:   17.808 mi: 2.01613402 zkl:   57.962 cd:   -0.345 pos_prob:   63.198 prob_neg:   63.544 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    23 rec:   17.808 mi: 2.01613402 zkl:   57.962 cd:   -0.345 pos_prob:   63.198 prob_neg:   63.544 kl_weight:    0.459 do_ae_train: False
2022-12-28 18:34:13,131 - 1:24:56 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    23 rec:   24.069 mi: 1.91722739 zkl:   63.080 cd:    1.849 pos_prob:   69.632 prob_neg:   67.783 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    23 rec:   24.069 mi: 1.91722739 zkl:   63.080 cd:    1.849 pos_prob:   69.632 prob_neg:   67.783 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:34:22,353 - 1:25:05 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    23 rec:   17.773 mi: 1.95566094 zkl:   60.627 cd:    3.202 pos_prob:   69.420 prob_neg:   66.218 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    23 rec:   17.773 mi: 1.95566094 zkl:   60.627 cd:    3.202 pos_prob:   69.420 prob_neg:   66.218 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:34:31,902 - 1:25:15 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    23 rec:   21.750 mi: 1.97239470 zkl:   63.201 cd:    2.008 pos_prob:   75.568 prob_neg:   73.559 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    23 rec:   21.750 mi: 1.97239470 zkl:   63.201 cd:    2.008 pos_prob:   75.568 prob_neg:   73.559 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:34:31,914 - 1:25:15 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-280.050
Langevin prior   1/ 40: energy=-280.050
2022-12-28 18:34:31,919 - 1:25:15 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1057.367
Langevin prior   6/ 40: energy=-1057.367
2022-12-28 18:34:31,925 - 1:25:15 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1647.819
Langevin prior  11/ 40: energy=-1647.819
2022-12-28 18:34:31,930 - 1:25:15 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1876.195
Langevin prior  16/ 40: energy=-1876.195
2022-12-28 18:34:31,936 - 1:25:15 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2046.924
Langevin prior  21/ 40: energy=-2046.924
2022-12-28 18:34:31,944 - 1:25:15 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2101.306
Langevin prior  26/ 40: energy=-2101.306
2022-12-28 18:34:31,952 - 1:25:15 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2102.607
Langevin prior  31/ 40: energy=-2102.607
2022-12-28 18:34:31,961 - 1:25:15 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2136.234
Langevin prior  36/ 40: energy=-2136.234
2022-12-28 18:34:31,969 - 1:25:15 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2150.249
Langevin prior  40/ 40: energy=-2150.249
2022-12-28 18:44:18,313 - 1:35:01 - 586.3s - INFO - root - Negative Log-likehood -158.984690
200 done. -136.8638688205244
400 done. -132.02708532831068
600 done. -131.7290970025914
800 done. -132.4486743214379
1000 done. -131.3474895377407
1200 done. -132.13198711585238
1400 done. -133.4541125012192
1600 done. -131.9355917141224
1800 done. -131.48016918122173
2000 done. -146.66344761395894
2200 done. -157.0398073142653
2400 done. -167.82093370918378
2600 done. -177.3001607861215
2800 done. -183.86918230692234
3000 done. -191.87170022956872
3200 done. -197.07125738480332
3400 done. -203.3006511645835
3600 done. -207.31631530941883
3800 done. -212.23496267855805
4000 done. -215.83096633898538
4200 done. -207.360105251726
4400 done. -198.67592728870838
4600 done. -190.94176539113838
4800 done. -183.77656328106775
5000 done. -177.35995292593296
5200 done. -171.3204891533861
5400 done. -165.82047405539197
5600 done. -160.6969269280597
Negative Log-likehood -158.984690
2022-12-28 18:44:18,313 - 1:35:01 - 0.0s - INFO - root - log-likelihood:   -158.985
log-likelihood:   -158.985
2022-12-28 18:45:05,329 - 1:35:48 - 47.0s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-023-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 18:45:05,331 - 1:35:48 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-023-test-greedy.txt
Generation: 188 batches
2022-12-28 18:45:11,016 - 1:35:54 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:45:12,510 - 1:35:55 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:45:12,511 - 1:35:55 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:45:14,925 - 1:35:58 - 2.4s - INFO - root - --- bleu: BLEU = 29.71, 54.1/32.8/26.6/23.6 (BP=0.915, ratio=0.919, hyp_len=155952, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.71, 54.1/32.8/26.6/23.6 (BP=0.915, ratio=0.919, hyp_len=155952, ref_len=169777)
--- bleu: BLEU = 29.71, 54.1/32.8/26.6/23.6 (BP=0.915, ratio=0.919, hyp_len=155952, ref_len=169777)

2022-12-28 18:45:14,925 - 1:35:58 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:45:14,933 - 1:35:58 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:45:24,442 - 1:36:07 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    24 rec:   21.556 mi: 2.01394486 zkl:   65.156 cd:    5.594 pos_prob:   76.541 prob_neg:   70.947 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    24 rec:   21.556 mi: 2.01394486 zkl:   65.156 cd:    5.594 pos_prob:   76.541 prob_neg:   70.947 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:45:33,896 - 1:36:17 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    24 rec:   20.902 mi: 1.93670607 zkl:   66.603 cd:   -2.824 pos_prob:   77.469 prob_neg:   80.293 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    24 rec:   20.902 mi: 1.93670607 zkl:   66.603 cd:   -2.824 pos_prob:   77.469 prob_neg:   80.293 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:45:43,466 - 1:36:26 - 9.6s - INFO - root - batch/max_batch/ep:   300/   529/    24 rec:   19.656 mi: 1.88394988 zkl:   65.258 cd:   -7.148 pos_prob:   80.743 prob_neg:   87.891 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    24 rec:   19.656 mi: 1.88394988 zkl:   65.258 cd:   -7.148 pos_prob:   80.743 prob_neg:   87.891 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:45:52,877 - 1:36:36 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    24 rec:   20.298 mi: 2.01961517 zkl:   70.675 cd:    0.986 pos_prob:   80.385 prob_neg:   79.398 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    24 rec:   20.298 mi: 2.01961517 zkl:   70.675 cd:    0.986 pos_prob:   80.385 prob_neg:   79.398 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:46:02,223 - 1:36:45 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    24 rec:   20.929 mi: 1.90411603 zkl:   65.247 cd:   -6.480 pos_prob:   74.679 prob_neg:   81.159 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    24 rec:   20.929 mi: 1.90411603 zkl:   65.247 cd:   -6.480 pos_prob:   74.679 prob_neg:   81.159 kl_weight:    0.500 do_ae_train: False
2022-12-28 18:46:02,236 - 1:36:45 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-318.061
Langevin prior   1/ 40: energy=-318.061
2022-12-28 18:46:02,241 - 1:36:45 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1188.606
Langevin prior   6/ 40: energy=-1188.606
2022-12-28 18:46:02,246 - 1:36:45 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1874.852
Langevin prior  11/ 40: energy=-1874.852
2022-12-28 18:46:02,251 - 1:36:45 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2086.143
Langevin prior  16/ 40: energy=-2086.143
2022-12-28 18:46:02,257 - 1:36:45 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2306.930
Langevin prior  21/ 40: energy=-2306.930
2022-12-28 18:46:02,262 - 1:36:45 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2271.939
Langevin prior  26/ 40: energy=-2271.939
2022-12-28 18:46:02,268 - 1:36:45 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2379.975
Langevin prior  31/ 40: energy=-2379.975
2022-12-28 18:46:02,276 - 1:36:45 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2315.985
Langevin prior  36/ 40: energy=-2315.985
2022-12-28 18:46:02,282 - 1:36:45 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2280.098
Langevin prior  40/ 40: energy=-2280.098
2022-12-28 18:55:48,606 - 1:46:31 - 586.3s - INFO - root - Negative Log-likehood -159.372325
200 done. -137.68747830564803
400 done. -132.40530715362468
600 done. -132.0797811409337
800 done. -132.80397246571874
1000 done. -131.79252606293832
1200 done. -132.41649428194663
1400 done. -133.74522599594997
1600 done. -132.22049351100424
1800 done. -131.73763991952922
2000 done. -146.93677936422193
2200 done. -157.47966087647993
2400 done. -168.21655069957671
2600 done. -177.74111401897042
2800 done. -184.31776665542907
3000 done. -192.22065238393577
3200 done. -197.3451678584007
3400 done. -203.55658820054458
3600 done. -207.69572944512694
3800 done. -212.70504562417588
4000 done. -216.26022487912837
4200 done. -207.79422751589598
4400 done. -199.09813020049842
4600 done. -191.36448632261337
4800 done. -184.19333235899586
5000 done. -177.76341760479963
5200 done. -171.71112655843945
5400 done. -166.20693775367667
5600 done. -161.08170786650058
Negative Log-likehood -159.372325
2022-12-28 18:55:48,606 - 1:46:31 - 0.0s - INFO - root - log-likelihood:   -159.372
log-likelihood:   -159.372
2022-12-28 18:56:35,478 - 1:47:18 - 46.9s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-024-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 18:56:35,480 - 1:47:18 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-024-test-greedy.txt
Generation: 188 batches
2022-12-28 18:56:41,171 - 1:47:24 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:56:42,686 - 1:47:25 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:56:42,687 - 1:47:25 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:56:45,172 - 1:47:28 - 2.5s - INFO - root - --- bleu: BLEU = 29.71, 52.5/31.5/25.6/22.6 (BP=0.950, ratio=0.951, hyp_len=161480, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.71, 52.5/31.5/25.6/22.6 (BP=0.950, ratio=0.951, hyp_len=161480, ref_len=169777)
--- bleu: BLEU = 29.71, 52.5/31.5/25.6/22.6 (BP=0.950, ratio=0.951, hyp_len=161480, ref_len=169777)

2022-12-28 18:56:45,172 - 1:47:28 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:56:45,180 - 1:47:28 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:56:54,200 - 1:47:37 - 9.0s - INFO - root - batch/max_batch/ep:   100/   529/    25 rec:   10.282 mi: 1.91728032 zkl:  146.796 cd:  -12.612 pos_prob:   63.174 prob_neg:   75.787 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    25 rec:   10.282 mi: 1.91728032 zkl:  146.796 cd:  -12.612 pos_prob:   63.174 prob_neg:   75.787 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:57:03,397 - 1:47:46 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    25 rec:   22.526 mi: 1.96837807 zkl:  154.630 cd:  -16.685 pos_prob:   53.660 prob_neg:   70.345 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    25 rec:   22.526 mi: 1.96837807 zkl:  154.630 cd:  -16.685 pos_prob:   53.660 prob_neg:   70.345 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:57:12,502 - 1:47:55 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    25 rec:   16.028 mi: 1.96086824 zkl:  157.007 cd:  -19.925 pos_prob:   53.993 prob_neg:   73.918 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    25 rec:   16.028 mi: 1.96086824 zkl:  157.007 cd:  -19.925 pos_prob:   53.993 prob_neg:   73.918 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:57:21,641 - 1:48:04 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    25 rec:   11.464 mi: 1.85229421 zkl:  159.036 cd:  -22.106 pos_prob:   55.108 prob_neg:   77.214 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    25 rec:   11.464 mi: 1.85229421 zkl:  159.036 cd:  -22.106 pos_prob:   55.108 prob_neg:   77.214 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:57:30,752 - 1:48:13 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    25 rec:   16.608 mi: 1.85679877 zkl:  163.160 cd:  -28.732 pos_prob:   47.851 prob_neg:   76.583 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    25 rec:   16.608 mi: 1.85679877 zkl:  163.160 cd:  -28.732 pos_prob:   47.851 prob_neg:   76.583 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:57:30,765 - 1:48:13 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-232.722
Langevin prior   1/ 40: energy=-232.722
2022-12-28 18:57:30,771 - 1:48:13 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1179.536
Langevin prior   6/ 40: energy=-1179.536
2022-12-28 18:57:30,776 - 1:48:13 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1791.114
Langevin prior  11/ 40: energy=-1791.114
2022-12-28 18:57:30,781 - 1:48:13 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2119.714
Langevin prior  16/ 40: energy=-2119.714
2022-12-28 18:57:30,789 - 1:48:13 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2273.882
Langevin prior  21/ 40: energy=-2273.882
2022-12-28 18:57:30,797 - 1:48:13 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2205.109
Langevin prior  26/ 40: energy=-2205.109
2022-12-28 18:57:30,805 - 1:48:14 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2234.704
Langevin prior  31/ 40: energy=-2234.704
2022-12-28 18:57:30,814 - 1:48:14 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2291.419
Langevin prior  36/ 40: energy=-2291.419
2022-12-28 18:57:30,822 - 1:48:14 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2361.336
Langevin prior  40/ 40: energy=-2361.336
2022-12-28 18:57:33,380 - 1:48:16 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-025-test-greedy.txt
Generation: 188 batches
2022-12-28 18:57:39,070 - 1:48:22 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:57:40,565 - 1:48:23 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:57:40,566 - 1:48:23 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:57:42,968 - 1:48:26 - 2.4s - INFO - root - --- bleu: BLEU = 31.43, 55.5/33.9/27.7/24.6 (BP=0.934, ratio=0.936, hyp_len=158978, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.43, 55.5/33.9/27.7/24.6 (BP=0.934, ratio=0.936, hyp_len=158978, ref_len=169777)
--- bleu: BLEU = 31.43, 55.5/33.9/27.7/24.6 (BP=0.934, ratio=0.936, hyp_len=158978, ref_len=169777)

2022-12-28 18:57:42,969 - 1:48:26 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:57:42,976 - 1:48:26 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:57:52,147 - 1:48:35 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    26 rec:   13.582 mi: 1.83580136 zkl:  162.275 cd:  -32.320 pos_prob:   46.085 prob_neg:   78.405 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    26 rec:   13.582 mi: 1.83580136 zkl:  162.275 cd:  -32.320 pos_prob:   46.085 prob_neg:   78.405 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:58:01,425 - 1:48:44 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    26 rec:   11.298 mi: 2.04836106 zkl:  162.828 cd:  -26.136 pos_prob:   48.347 prob_neg:   74.483 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    26 rec:   11.298 mi: 2.04836106 zkl:  162.828 cd:  -26.136 pos_prob:   48.347 prob_neg:   74.483 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:58:10,609 - 1:48:53 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    26 rec:   13.478 mi: 1.95646179 zkl:  161.915 cd:  -35.320 pos_prob:   43.315 prob_neg:   78.635 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    26 rec:   13.478 mi: 1.95646179 zkl:  161.915 cd:  -35.320 pos_prob:   43.315 prob_neg:   78.635 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:58:19,723 - 1:49:02 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    26 rec:    9.182 mi: 1.90489483 zkl:  165.499 cd:  -25.419 pos_prob:   46.920 prob_neg:   72.338 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    26 rec:    9.182 mi: 1.90489483 zkl:  165.499 cd:  -25.419 pos_prob:   46.920 prob_neg:   72.338 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:58:28,726 - 1:49:11 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/    26 rec:   18.388 mi: 1.92068005 zkl:  168.404 cd:  -32.397 pos_prob:   38.991 prob_neg:   71.388 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    26 rec:   18.388 mi: 1.92068005 zkl:  168.404 cd:  -32.397 pos_prob:   38.991 prob_neg:   71.388 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:58:28,738 - 1:49:11 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-262.133
Langevin prior   1/ 40: energy=-262.133
2022-12-28 18:58:28,744 - 1:49:11 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1257.068
Langevin prior   6/ 40: energy=-1257.068
2022-12-28 18:58:28,749 - 1:49:11 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1817.992
Langevin prior  11/ 40: energy=-1817.992
2022-12-28 18:58:28,755 - 1:49:11 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2044.633
Langevin prior  16/ 40: energy=-2044.633
2022-12-28 18:58:28,761 - 1:49:11 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2188.572
Langevin prior  21/ 40: energy=-2188.572
2022-12-28 18:58:28,769 - 1:49:11 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2312.412
Langevin prior  26/ 40: energy=-2312.412
2022-12-28 18:58:28,777 - 1:49:11 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2282.607
Langevin prior  31/ 40: energy=-2282.607
2022-12-28 18:58:28,786 - 1:49:11 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2254.246
Langevin prior  36/ 40: energy=-2254.246
2022-12-28 18:58:28,794 - 1:49:11 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2283.274
Langevin prior  40/ 40: energy=-2283.274
2022-12-28 18:58:31,355 - 1:49:14 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-026-test-greedy.txt
Generation: 188 batches
2022-12-28 18:58:37,007 - 1:49:20 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:58:38,494 - 1:49:21 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:58:38,494 - 1:49:21 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:58:40,894 - 1:49:24 - 2.4s - INFO - root - --- bleu: BLEU = 31.42, 55.7/34.1/27.9/24.8 (BP=0.928, ratio=0.931, hyp_len=157993, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.42, 55.7/34.1/27.9/24.8 (BP=0.928, ratio=0.931, hyp_len=157993, ref_len=169777)
--- bleu: BLEU = 31.42, 55.7/34.1/27.9/24.8 (BP=0.928, ratio=0.931, hyp_len=157993, ref_len=169777)

2022-12-28 18:58:40,895 - 1:49:24 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:58:40,903 - 1:49:24 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:58:49,929 - 1:49:33 - 9.0s - INFO - root - batch/max_batch/ep:   100/   529/    27 rec:    4.961 mi: 1.77628136 zkl:  166.620 cd:  -19.075 pos_prob:   49.364 prob_neg:   68.439 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    27 rec:    4.961 mi: 1.77628136 zkl:  166.620 cd:  -19.075 pos_prob:   49.364 prob_neg:   68.439 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:58:59,154 - 1:49:42 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    27 rec:   10.069 mi: 1.94089055 zkl:  168.497 cd:  -31.105 pos_prob:   43.368 prob_neg:   74.472 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    27 rec:   10.069 mi: 1.94089055 zkl:  168.497 cd:  -31.105 pos_prob:   43.368 prob_neg:   74.472 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:59:08,345 - 1:49:51 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    27 rec:    8.665 mi: 1.75838208 zkl:  172.248 cd:  -34.204 pos_prob:   43.314 prob_neg:   77.518 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    27 rec:    8.665 mi: 1.75838208 zkl:  172.248 cd:  -34.204 pos_prob:   43.314 prob_neg:   77.518 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:59:17,420 - 1:50:00 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    27 rec:    9.779 mi: 1.93872535 zkl:  169.064 cd:  -35.889 pos_prob:   43.092 prob_neg:   78.982 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    27 rec:    9.779 mi: 1.93872535 zkl:  169.064 cd:  -35.889 pos_prob:   43.092 prob_neg:   78.982 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:59:26,407 - 1:50:09 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/    27 rec:   10.857 mi: 1.96393955 zkl:  168.959 cd:  -37.639 pos_prob:   40.709 prob_neg:   78.348 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    27 rec:   10.857 mi: 1.96393955 zkl:  168.959 cd:  -37.639 pos_prob:   40.709 prob_neg:   78.348 kl_weight:    0.000 do_ae_train: True
2022-12-28 18:59:26,420 - 1:50:09 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-262.099
Langevin prior   1/ 40: energy=-262.099
2022-12-28 18:59:26,425 - 1:50:09 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1204.558
Langevin prior   6/ 40: energy=-1204.558
2022-12-28 18:59:26,431 - 1:50:09 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1759.687
Langevin prior  11/ 40: energy=-1759.687
2022-12-28 18:59:26,436 - 1:50:09 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2054.515
Langevin prior  16/ 40: energy=-2054.515
2022-12-28 18:59:26,442 - 1:50:09 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2264.647
Langevin prior  21/ 40: energy=-2264.647
2022-12-28 18:59:26,448 - 1:50:09 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2263.565
Langevin prior  26/ 40: energy=-2263.565
2022-12-28 18:59:26,457 - 1:50:09 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2355.686
Langevin prior  31/ 40: energy=-2355.686
2022-12-28 18:59:26,465 - 1:50:09 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2321.593
Langevin prior  36/ 40: energy=-2321.593
2022-12-28 18:59:26,472 - 1:50:09 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2407.530
Langevin prior  40/ 40: energy=-2407.530
2022-12-28 18:59:29,236 - 1:50:12 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-027-test-greedy.txt
Generation: 188 batches
2022-12-28 18:59:34,906 - 1:50:18 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:59:36,402 - 1:50:19 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 18:59:36,402 - 1:50:19 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 18:59:38,850 - 1:50:22 - 2.4s - INFO - root - --- bleu: BLEU = 31.74, 55.1/33.7/27.5/24.5 (BP=0.949, ratio=0.950, hyp_len=161342, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.74, 55.1/33.7/27.5/24.5 (BP=0.949, ratio=0.950, hyp_len=161342, ref_len=169777)
--- bleu: BLEU = 31.74, 55.1/33.7/27.5/24.5 (BP=0.949, ratio=0.950, hyp_len=161342, ref_len=169777)

2022-12-28 18:59:38,850 - 1:50:22 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 18:59:38,858 - 1:50:22 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 18:59:48,156 - 1:50:31 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    28 rec:   10.426 mi: 1.86841607 zkl:  104.402 cd:   -7.649 pos_prob:   67.067 prob_neg:   74.716 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    28 rec:   10.426 mi: 1.86841607 zkl:  104.402 cd:   -7.649 pos_prob:   67.067 prob_neg:   74.716 kl_weight:    0.062 do_ae_train: False
2022-12-28 18:59:57,468 - 1:50:40 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    28 rec:   12.638 mi: 1.91593468 zkl:   85.305 cd:   -3.117 pos_prob:   65.123 prob_neg:   68.240 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    28 rec:   12.638 mi: 1.91593468 zkl:   85.305 cd:   -3.117 pos_prob:   65.123 prob_neg:   68.240 kl_weight:    0.125 do_ae_train: False
2022-12-28 19:00:06,928 - 1:50:50 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    28 rec:   14.886 mi: 1.91592002 zkl:   75.932 cd:   -5.009 pos_prob:   62.625 prob_neg:   67.634 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    28 rec:   14.886 mi: 1.91592002 zkl:   75.932 cd:   -5.009 pos_prob:   62.625 prob_neg:   67.634 kl_weight:    0.188 do_ae_train: False
2022-12-28 19:00:16,465 - 1:50:59 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    28 rec:   16.245 mi: 1.88560104 zkl:   68.724 cd:   -7.691 pos_prob:   58.253 prob_neg:   65.944 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    28 rec:   16.245 mi: 1.88560104 zkl:   68.724 cd:   -7.691 pos_prob:   58.253 prob_neg:   65.944 kl_weight:    0.251 do_ae_train: False
2022-12-28 19:00:25,919 - 1:51:09 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    28 rec:   17.629 mi: 1.93680668 zkl:   68.307 cd:   -2.698 pos_prob:   60.047 prob_neg:   62.745 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    28 rec:   17.629 mi: 1.93680668 zkl:   68.307 cd:   -2.698 pos_prob:   60.047 prob_neg:   62.745 kl_weight:    0.314 do_ae_train: False
2022-12-28 19:00:25,932 - 1:51:09 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-230.848
Langevin prior   1/ 40: energy=-230.848
2022-12-28 19:00:25,937 - 1:51:09 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1042.736
Langevin prior   6/ 40: energy=-1042.736
2022-12-28 19:00:25,943 - 1:51:09 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1526.380
Langevin prior  11/ 40: energy=-1526.380
2022-12-28 19:00:25,948 - 1:51:09 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1734.146
Langevin prior  16/ 40: energy=-1734.146
2022-12-28 19:00:25,955 - 1:51:09 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1907.010
Langevin prior  21/ 40: energy=-1907.010
2022-12-28 19:00:25,963 - 1:51:09 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1956.395
Langevin prior  26/ 40: energy=-1956.395
2022-12-28 19:00:25,971 - 1:51:09 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2059.123
Langevin prior  31/ 40: energy=-2059.123
2022-12-28 19:00:25,980 - 1:51:09 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2064.343
Langevin prior  36/ 40: energy=-2064.343
2022-12-28 19:00:25,988 - 1:51:09 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2078.445
Langevin prior  40/ 40: energy=-2078.445
2022-12-28 19:00:28,652 - 1:51:11 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-028-test-greedy.txt
Generation: 188 batches
2022-12-28 19:00:34,312 - 1:51:17 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:00:35,827 - 1:51:19 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:00:35,827 - 1:51:19 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:00:38,289 - 1:51:21 - 2.5s - INFO - root - --- bleu: BLEU = 30.12, 53.4/32.0/26.0/23.0 (BP=0.947, ratio=0.948, hyp_len=160981, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.12, 53.4/32.0/26.0/23.0 (BP=0.947, ratio=0.948, hyp_len=160981, ref_len=169777)
--- bleu: BLEU = 30.12, 53.4/32.0/26.0/23.0 (BP=0.947, ratio=0.948, hyp_len=160981, ref_len=169777)

2022-12-28 19:00:38,290 - 1:51:21 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:00:38,298 - 1:51:21 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:00:47,716 - 1:51:30 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    29 rec:   14.777 mi: 1.99194193 zkl:   66.031 cd:   -4.469 pos_prob:   64.675 prob_neg:   69.144 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    29 rec:   14.777 mi: 1.99194193 zkl:   66.031 cd:   -4.469 pos_prob:   64.675 prob_neg:   69.144 kl_weight:    0.396 do_ae_train: False
2022-12-28 19:00:57,102 - 1:51:40 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    29 rec:   18.576 mi: 1.82418752 zkl:   56.692 cd:   -7.497 pos_prob:   57.738 prob_neg:   65.235 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    29 rec:   18.576 mi: 1.82418752 zkl:   56.692 cd:   -7.497 pos_prob:   57.738 prob_neg:   65.235 kl_weight:    0.459 do_ae_train: False
2022-12-28 19:01:06,414 - 1:51:49 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    29 rec:   16.745 mi: 1.99681354 zkl:   65.439 cd:    8.604 pos_prob:   72.965 prob_neg:   64.361 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    29 rec:   16.745 mi: 1.99681354 zkl:   65.439 cd:    8.604 pos_prob:   72.965 prob_neg:   64.361 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:01:15,784 - 1:51:58 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    29 rec:   17.457 mi: 1.89468575 zkl:   58.821 cd:    2.339 pos_prob:   66.716 prob_neg:   64.378 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    29 rec:   17.457 mi: 1.89468575 zkl:   58.821 cd:    2.339 pos_prob:   66.716 prob_neg:   64.378 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:01:25,216 - 1:52:08 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    29 rec:   15.815 mi: 1.92286229 zkl:   65.961 cd:    2.542 pos_prob:   84.254 prob_neg:   81.712 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    29 rec:   15.815 mi: 1.92286229 zkl:   65.961 cd:    2.542 pos_prob:   84.254 prob_neg:   81.712 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:01:25,228 - 1:52:08 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-351.395
Langevin prior   1/ 40: energy=-351.395
2022-12-28 19:01:25,234 - 1:52:08 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1194.078
Langevin prior   6/ 40: energy=-1194.078
2022-12-28 19:01:25,239 - 1:52:08 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1673.161
Langevin prior  11/ 40: energy=-1673.161
2022-12-28 19:01:25,245 - 1:52:08 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1856.953
Langevin prior  16/ 40: energy=-1856.953
2022-12-28 19:01:25,250 - 1:52:08 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1987.670
Langevin prior  21/ 40: energy=-1987.670
2022-12-28 19:01:25,256 - 1:52:08 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2096.102
Langevin prior  26/ 40: energy=-2096.102
2022-12-28 19:01:25,261 - 1:52:08 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2107.074
Langevin prior  31/ 40: energy=-2107.074
2022-12-28 19:01:25,269 - 1:52:08 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2188.172
Langevin prior  36/ 40: energy=-2188.172
2022-12-28 19:01:25,276 - 1:52:08 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2209.655
Langevin prior  40/ 40: energy=-2209.655
2022-12-28 19:11:11,095 - 2:01:54 - 585.8s - INFO - root - Negative Log-likehood -164.218219
200 done. -141.04172271729735
400 done. -136.0324684205888
600 done. -135.86105854572608
800 done. -136.45987115249883
1000 done. -135.32594702454293
1200 done. -135.99327310515574
1400 done. -137.27844913996242
1600 done. -135.74229545528587
1800 done. -135.23749975425187
2000 done. -151.21923469443368
2200 done. -161.93273352059344
2400 done. -173.09950278345153
2600 done. -183.0518526129805
2800 done. -189.74995614446712
3000 done. -198.03159625022982
3200 done. -203.45972151330793
3400 done. -209.775273735877
3600 done. -213.92124519529662
3800 done. -219.232394126774
4000 done. -222.89254872162624
4200 done. -214.14729303844155
4400 done. -205.17387124505404
4600 done. -197.2053546140836
4800 done. -189.81850652756077
5000 done. -183.19519194573928
5200 done. -176.95935280902737
5400 done. -171.27037426414952
5600 done. -165.9857195764675
Negative Log-likehood -164.218219
2022-12-28 19:11:11,096 - 2:01:54 - 0.0s - INFO - root - log-likelihood:   -164.218
log-likelihood:   -164.218
2022-12-28 19:11:57,795 - 2:02:40 - 46.7s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-029-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 19:11:57,797 - 2:02:40 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-029-test-greedy.txt
Generation: 188 batches
2022-12-28 19:12:03,480 - 2:02:46 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:12:05,014 - 2:02:48 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:12:05,015 - 2:02:48 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:12:07,740 - 2:02:50 - 2.7s - INFO - root - --- bleu: BLEU = 29.88, 52.5/31.6/25.7/22.6 (BP=0.953, ratio=0.955, hyp_len=162060, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.88, 52.5/31.6/25.7/22.6 (BP=0.953, ratio=0.955, hyp_len=162060, ref_len=169777)
--- bleu: BLEU = 29.88, 52.5/31.6/25.7/22.6 (BP=0.953, ratio=0.955, hyp_len=162060, ref_len=169777)

2022-12-28 19:12:07,740 - 2:02:50 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:12:07,748 - 2:02:50 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:12:17,237 - 2:03:00 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    30 rec:   16.020 mi: 1.72127664 zkl:   71.035 cd:    5.029 pos_prob:   83.645 prob_neg:   78.616 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    30 rec:   16.020 mi: 1.72127664 zkl:   71.035 cd:    5.029 pos_prob:   83.645 prob_neg:   78.616 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:12:26,519 - 2:03:09 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    30 rec:   17.406 mi: 1.84068000 zkl:   62.798 cd:   -2.440 pos_prob:   69.637 prob_neg:   72.077 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    30 rec:   17.406 mi: 1.84068000 zkl:   62.798 cd:   -2.440 pos_prob:   69.637 prob_neg:   72.077 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:12:36,194 - 2:03:19 - 9.7s - INFO - root - batch/max_batch/ep:   300/   529/    30 rec:   15.285 mi: 1.91598582 zkl:   63.310 cd:    3.641 pos_prob:   76.509 prob_neg:   72.868 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    30 rec:   15.285 mi: 1.91598582 zkl:   63.310 cd:    3.641 pos_prob:   76.509 prob_neg:   72.868 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:12:45,674 - 2:03:28 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    30 rec:   20.410 mi: 2.00771523 zkl:   65.815 cd:    4.841 pos_prob:   76.534 prob_neg:   71.693 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    30 rec:   20.410 mi: 2.00771523 zkl:   65.815 cd:    4.841 pos_prob:   76.534 prob_neg:   71.693 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:12:55,383 - 2:03:38 - 9.7s - INFO - root - batch/max_batch/ep:   500/   529/    30 rec:   19.009 mi: 1.97317541 zkl:   60.866 cd:   -4.086 pos_prob:   68.560 prob_neg:   72.647 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    30 rec:   19.009 mi: 1.97317541 zkl:   60.866 cd:   -4.086 pos_prob:   68.560 prob_neg:   72.647 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:12:55,396 - 2:03:38 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-300.251
Langevin prior   1/ 40: energy=-300.251
2022-12-28 19:12:55,402 - 2:03:38 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1279.510
Langevin prior   6/ 40: energy=-1279.510
2022-12-28 19:12:55,407 - 2:03:38 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1937.459
Langevin prior  11/ 40: energy=-1937.459
2022-12-28 19:12:55,412 - 2:03:38 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2294.723
Langevin prior  16/ 40: energy=-2294.723
2022-12-28 19:12:55,419 - 2:03:38 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2396.275
Langevin prior  21/ 40: energy=-2396.275
2022-12-28 19:12:55,427 - 2:03:38 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2512.615
Langevin prior  26/ 40: energy=-2512.615
2022-12-28 19:12:55,436 - 2:03:38 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2502.953
Langevin prior  31/ 40: energy=-2502.953
2022-12-28 19:12:55,445 - 2:03:38 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2520.157
Langevin prior  36/ 40: energy=-2520.157
2022-12-28 19:12:55,453 - 2:03:38 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2556.266
Langevin prior  40/ 40: energy=-2556.266
2022-12-28 19:22:41,090 - 2:13:24 - 585.6s - INFO - root - Negative Log-likehood -165.646007
200 done. -142.8988214646233
400 done. -137.76271317545138
600 done. -137.569357825224
800 done. -138.12899455303466
1000 done. -137.05208898122612
1200 done. -137.7748183603306
1400 done. -139.00346693215957
1600 done. -137.46724839135032
1800 done. -136.92972108098604
2000 done. -153.14067294867564
2200 done. -163.704372392635
2400 done. -175.0238511335438
2600 done. -185.1333620771036
2800 done. -192.00335718814813
3000 done. -200.24520880394445
3200 done. -205.7404532461882
3400 done. -212.14028881754282
3600 done. -216.27741996657926
3800 done. -221.42151574054037
4000 done. -225.13313125081618
4200 done. -216.26758660331396
4400 done. -207.18024562808108
4600 done. -199.09796848165075
4800 done. -191.60469638884862
5000 done. -184.89665034042375
5200 done. -178.56941840046048
5400 done. -172.8035764784492
5600 done. -167.43586526770872
Negative Log-likehood -165.646007
2022-12-28 19:22:41,090 - 2:13:24 - 0.0s - INFO - root - log-likelihood:   -165.646
log-likelihood:   -165.646
2022-12-28 19:23:28,217 - 2:14:11 - 47.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-030-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 19:23:28,219 - 2:14:11 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-030-test-greedy.txt
Generation: 188 batches
2022-12-28 19:23:33,886 - 2:14:17 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:23:35,383 - 2:14:18 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:23:35,383 - 2:14:18 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:23:37,845 - 2:14:21 - 2.5s - INFO - root - --- bleu: BLEU = 29.53, 52.9/32.0/26.0/23.0 (BP=0.932, ratio=0.934, hyp_len=158534, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.53, 52.9/32.0/26.0/23.0 (BP=0.932, ratio=0.934, hyp_len=158534, ref_len=169777)
--- bleu: BLEU = 29.53, 52.9/32.0/26.0/23.0 (BP=0.932, ratio=0.934, hyp_len=158534, ref_len=169777)

2022-12-28 19:23:37,845 - 2:14:21 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:23:37,853 - 2:14:21 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:23:47,092 - 2:14:30 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    31 rec:   16.866 mi: 1.88259041 zkl:  152.577 cd:  -20.767 pos_prob:   55.441 prob_neg:   76.208 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    31 rec:   16.866 mi: 1.88259041 zkl:  152.577 cd:  -20.767 pos_prob:   55.441 prob_neg:   76.208 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:23:56,217 - 2:14:39 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    31 rec:   10.528 mi: 1.87321484 zkl:  158.649 cd:  -16.590 pos_prob:   57.263 prob_neg:   73.853 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    31 rec:   10.528 mi: 1.87321484 zkl:  158.649 cd:  -16.590 pos_prob:   57.263 prob_neg:   73.853 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:24:05,396 - 2:14:48 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    31 rec:   10.736 mi: 1.91380906 zkl:  160.997 cd:  -23.325 pos_prob:   51.669 prob_neg:   74.994 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    31 rec:   10.736 mi: 1.91380906 zkl:  160.997 cd:  -23.325 pos_prob:   51.669 prob_neg:   74.994 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:24:14,463 - 2:14:57 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    31 rec:   13.378 mi: 2.03048635 zkl:  164.286 cd:  -30.963 pos_prob:   49.100 prob_neg:   80.063 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    31 rec:   13.378 mi: 2.03048635 zkl:  164.286 cd:  -30.963 pos_prob:   49.100 prob_neg:   80.063 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:24:23,816 - 2:15:07 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    31 rec:   15.553 mi: 1.99610245 zkl:  163.286 cd:  -32.918 pos_prob:   46.956 prob_neg:   79.875 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    31 rec:   15.553 mi: 1.99610245 zkl:  163.286 cd:  -32.918 pos_prob:   46.956 prob_neg:   79.875 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:24:23,829 - 2:15:07 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-306.022
Langevin prior   1/ 40: energy=-306.022
2022-12-28 19:24:23,834 - 2:15:07 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1203.275
Langevin prior   6/ 40: energy=-1203.275
2022-12-28 19:24:23,840 - 2:15:07 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1766.647
Langevin prior  11/ 40: energy=-1766.647
2022-12-28 19:24:23,846 - 2:15:07 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2055.452
Langevin prior  16/ 40: energy=-2055.452
2022-12-28 19:24:23,854 - 2:15:07 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2226.921
Langevin prior  21/ 40: energy=-2226.921
2022-12-28 19:24:23,863 - 2:15:07 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2308.383
Langevin prior  26/ 40: energy=-2308.383
2022-12-28 19:24:23,872 - 2:15:07 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2292.189
Langevin prior  31/ 40: energy=-2292.189
2022-12-28 19:24:23,882 - 2:15:07 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2354.055
Langevin prior  36/ 40: energy=-2354.055
2022-12-28 19:24:23,891 - 2:15:07 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2254.794
Langevin prior  40/ 40: energy=-2254.794
2022-12-28 19:24:26,530 - 2:15:09 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-031-test-greedy.txt
Generation: 188 batches
2022-12-28 19:24:32,175 - 2:15:15 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:24:33,662 - 2:15:16 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:24:33,662 - 2:15:16 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:24:36,065 - 2:15:19 - 2.4s - INFO - root - --- bleu: BLEU = 31.06, 55.6/34.0/27.8/24.7 (BP=0.921, ratio=0.924, hyp_len=156868, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.06, 55.6/34.0/27.8/24.7 (BP=0.921, ratio=0.924, hyp_len=156868, ref_len=169777)
--- bleu: BLEU = 31.06, 55.6/34.0/27.8/24.7 (BP=0.921, ratio=0.924, hyp_len=156868, ref_len=169777)

2022-12-28 19:24:36,066 - 2:15:19 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:24:36,074 - 2:15:19 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:24:45,239 - 2:15:28 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    32 rec:   16.720 mi: 1.96766496 zkl:  163.326 cd:  -32.103 pos_prob:   41.573 prob_neg:   73.676 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    32 rec:   16.720 mi: 1.96766496 zkl:  163.326 cd:  -32.103 pos_prob:   41.573 prob_neg:   73.676 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:24:54,366 - 2:15:37 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    32 rec:   15.836 mi: 1.94935226 zkl:  159.631 cd:  -38.122 pos_prob:   43.879 prob_neg:   82.001 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    32 rec:   15.836 mi: 1.94935226 zkl:  159.631 cd:  -38.122 pos_prob:   43.879 prob_neg:   82.001 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:25:03,534 - 2:15:46 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    32 rec:   14.002 mi: 1.86183763 zkl:  161.141 cd:  -26.087 pos_prob:   46.161 prob_neg:   72.248 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    32 rec:   14.002 mi: 1.86183763 zkl:  161.141 cd:  -26.087 pos_prob:   46.161 prob_neg:   72.248 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:25:12,812 - 2:15:56 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    32 rec:   13.354 mi: 1.96539938 zkl:  166.930 cd:  -33.068 pos_prob:   44.387 prob_neg:   77.455 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    32 rec:   13.354 mi: 1.96539938 zkl:  166.930 cd:  -33.068 pos_prob:   44.387 prob_neg:   77.455 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:25:22,033 - 2:16:05 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    32 rec:   11.266 mi: 1.81839216 zkl:  161.328 cd:  -31.306 pos_prob:   42.672 prob_neg:   73.978 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    32 rec:   11.266 mi: 1.81839216 zkl:  161.328 cd:  -31.306 pos_prob:   42.672 prob_neg:   73.978 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:25:22,045 - 2:16:05 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-283.958
Langevin prior   1/ 40: energy=-283.958
2022-12-28 19:25:22,051 - 2:16:05 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1149.279
Langevin prior   6/ 40: energy=-1149.279
2022-12-28 19:25:22,056 - 2:16:05 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1741.577
Langevin prior  11/ 40: energy=-1741.577
2022-12-28 19:25:22,062 - 2:16:05 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2003.686
Langevin prior  16/ 40: energy=-2003.686
2022-12-28 19:25:22,068 - 2:16:05 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2104.872
Langevin prior  21/ 40: energy=-2104.872
2022-12-28 19:25:22,075 - 2:16:05 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2280.122
Langevin prior  26/ 40: energy=-2280.122
2022-12-28 19:25:22,084 - 2:16:05 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2324.151
Langevin prior  31/ 40: energy=-2324.151
2022-12-28 19:25:22,093 - 2:16:05 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2311.237
Langevin prior  36/ 40: energy=-2311.237
2022-12-28 19:25:22,101 - 2:16:05 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2345.084
Langevin prior  40/ 40: energy=-2345.084
2022-12-28 19:25:24,635 - 2:16:07 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-032-test-greedy.txt
Generation: 188 batches
2022-12-28 19:25:30,295 - 2:16:13 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:25:31,782 - 2:16:14 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:25:31,782 - 2:16:14 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:25:34,179 - 2:16:17 - 2.4s - INFO - root - --- bleu: BLEU = 31.56, 55.8/34.3/28.2/25.1 (BP=0.926, ratio=0.928, hyp_len=157634, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.56, 55.8/34.3/28.2/25.1 (BP=0.926, ratio=0.928, hyp_len=157634, ref_len=169777)
--- bleu: BLEU = 31.56, 55.8/34.3/28.2/25.1 (BP=0.926, ratio=0.928, hyp_len=157634, ref_len=169777)

2022-12-28 19:25:34,179 - 2:16:17 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:25:34,187 - 2:16:17 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:25:43,326 - 2:16:26 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    33 rec:    8.692 mi: 1.93950665 zkl:  169.871 cd:  -28.342 pos_prob:   47.480 prob_neg:   75.822 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    33 rec:    8.692 mi: 1.93950665 zkl:  169.871 cd:  -28.342 pos_prob:   47.480 prob_neg:   75.822 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:25:52,441 - 2:16:35 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    33 rec:   10.657 mi: 1.94775736 zkl:  169.916 cd:  -32.512 pos_prob:   43.038 prob_neg:   75.550 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    33 rec:   10.657 mi: 1.94775736 zkl:  169.916 cd:  -32.512 pos_prob:   43.038 prob_neg:   75.550 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:26:01,812 - 2:16:45 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    33 rec:    8.415 mi: 2.01082373 zkl:  171.391 cd:  -39.039 pos_prob:   44.978 prob_neg:   84.018 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    33 rec:    8.415 mi: 2.01082373 zkl:  171.391 cd:  -39.039 pos_prob:   44.978 prob_neg:   84.018 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:26:11,226 - 2:16:54 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    33 rec:    9.898 mi: 1.83334398 zkl:  169.516 cd:  -36.401 pos_prob:   43.935 prob_neg:   80.337 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    33 rec:    9.898 mi: 1.83334398 zkl:  169.516 cd:  -36.401 pos_prob:   43.935 prob_neg:   80.337 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:26:20,431 - 2:17:03 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    33 rec:   11.853 mi: 1.99478209 zkl:  173.184 cd:  -36.745 pos_prob:   40.402 prob_neg:   77.147 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    33 rec:   11.853 mi: 1.99478209 zkl:  173.184 cd:  -36.745 pos_prob:   40.402 prob_neg:   77.147 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:26:20,443 - 2:17:03 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-268.959
Langevin prior   1/ 40: energy=-268.959
2022-12-28 19:26:20,448 - 2:17:03 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1266.488
Langevin prior   6/ 40: energy=-1266.488
2022-12-28 19:26:20,453 - 2:17:03 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1811.790
Langevin prior  11/ 40: energy=-1811.790
2022-12-28 19:26:20,458 - 2:17:03 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2106.780
Langevin prior  16/ 40: energy=-2106.780
2022-12-28 19:26:20,463 - 2:17:03 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2188.465
Langevin prior  21/ 40: energy=-2188.465
2022-12-28 19:26:20,468 - 2:17:03 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2271.988
Langevin prior  26/ 40: energy=-2271.988
2022-12-28 19:26:20,474 - 2:17:03 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2287.412
Langevin prior  31/ 40: energy=-2287.412
2022-12-28 19:26:20,484 - 2:17:03 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2256.339
Langevin prior  36/ 40: energy=-2256.339
2022-12-28 19:26:20,493 - 2:17:03 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2296.170
Langevin prior  40/ 40: energy=-2296.170
2022-12-28 19:26:23,061 - 2:17:06 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-033-test-greedy.txt
Generation: 188 batches
2022-12-28 19:26:28,704 - 2:17:11 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:26:30,185 - 2:17:13 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:26:30,185 - 2:17:13 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:26:32,565 - 2:17:15 - 2.4s - INFO - root - --- bleu: BLEU = 31.65, 56.3/34.9/28.7/25.6 (BP=0.914, ratio=0.917, hyp_len=155701, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.65, 56.3/34.9/28.7/25.6 (BP=0.914, ratio=0.917, hyp_len=155701, ref_len=169777)
--- bleu: BLEU = 31.65, 56.3/34.9/28.7/25.6 (BP=0.914, ratio=0.917, hyp_len=155701, ref_len=169777)

2022-12-28 19:26:32,566 - 2:17:15 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:26:32,574 - 2:17:15 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:26:42,054 - 2:17:25 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    34 rec:   18.497 mi: 1.60180485 zkl:   99.097 cd:   -8.694 pos_prob:   62.920 prob_neg:   71.613 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    34 rec:   18.497 mi: 1.60180485 zkl:   99.097 cd:   -8.694 pos_prob:   62.920 prob_neg:   71.613 kl_weight:    0.062 do_ae_train: False
2022-12-28 19:26:51,586 - 2:17:34 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    34 rec:   13.334 mi: 1.77369738 zkl:   86.945 cd:   -2.459 pos_prob:   66.340 prob_neg:   68.799 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    34 rec:   13.334 mi: 1.77369738 zkl:   86.945 cd:   -2.459 pos_prob:   66.340 prob_neg:   68.799 kl_weight:    0.125 do_ae_train: False
2022-12-28 19:27:01,090 - 2:17:44 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    34 rec:   12.451 mi: 1.86961949 zkl:   71.647 cd:   -7.590 pos_prob:   65.073 prob_neg:   72.662 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    34 rec:   12.451 mi: 1.86961949 zkl:   71.647 cd:   -7.590 pos_prob:   65.073 prob_neg:   72.662 kl_weight:    0.188 do_ae_train: False
2022-12-28 19:27:10,495 - 2:17:53 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    34 rec:   18.375 mi: 1.98407149 zkl:   75.333 cd:   -5.645 pos_prob:   60.428 prob_neg:   66.073 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    34 rec:   18.375 mi: 1.98407149 zkl:   75.333 cd:   -5.645 pos_prob:   60.428 prob_neg:   66.073 kl_weight:    0.251 do_ae_train: False
2022-12-28 19:27:20,021 - 2:18:03 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    34 rec:   14.285 mi: 1.88708019 zkl:   68.421 cd:    2.323 pos_prob:   61.858 prob_neg:   59.535 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    34 rec:   14.285 mi: 1.88708019 zkl:   68.421 cd:    2.323 pos_prob:   61.858 prob_neg:   59.535 kl_weight:    0.314 do_ae_train: False
2022-12-28 19:27:20,034 - 2:18:03 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-320.666
Langevin prior   1/ 40: energy=-320.666
2022-12-28 19:27:20,040 - 2:18:03 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1019.305
Langevin prior   6/ 40: energy=-1019.305
2022-12-28 19:27:20,045 - 2:18:03 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1454.394
Langevin prior  11/ 40: energy=-1454.394
2022-12-28 19:27:20,050 - 2:18:03 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1696.202
Langevin prior  16/ 40: energy=-1696.202
2022-12-28 19:27:20,056 - 2:18:03 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1796.910
Langevin prior  21/ 40: energy=-1796.910
2022-12-28 19:27:20,062 - 2:18:03 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1853.818
Langevin prior  26/ 40: energy=-1853.818
2022-12-28 19:27:20,069 - 2:18:03 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1866.553
Langevin prior  31/ 40: energy=-1866.553
2022-12-28 19:27:20,078 - 2:18:03 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1861.499
Langevin prior  36/ 40: energy=-1861.499
2022-12-28 19:27:20,085 - 2:18:03 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1848.593
Langevin prior  40/ 40: energy=-1848.593
2022-12-28 19:27:22,674 - 2:18:05 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-034-test-greedy.txt
Generation: 188 batches
2022-12-28 19:27:28,335 - 2:18:11 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:27:29,825 - 2:18:13 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:27:29,825 - 2:18:13 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:27:32,222 - 2:18:15 - 2.4s - INFO - root - --- bleu: BLEU = 30.00, 54.2/32.6/26.6/23.6 (BP=0.924, ratio=0.927, hyp_len=157357, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.00, 54.2/32.6/26.6/23.6 (BP=0.924, ratio=0.927, hyp_len=157357, ref_len=169777)
--- bleu: BLEU = 30.00, 54.2/32.6/26.6/23.6 (BP=0.924, ratio=0.927, hyp_len=157357, ref_len=169777)

2022-12-28 19:27:32,223 - 2:18:15 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:27:32,230 - 2:18:15 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:27:41,502 - 2:18:24 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    35 rec:   17.108 mi: 1.96778667 zkl:   61.911 cd:    1.441 pos_prob:   63.394 prob_neg:   61.953 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    35 rec:   17.108 mi: 1.96778667 zkl:   61.911 cd:    1.441 pos_prob:   63.394 prob_neg:   61.953 kl_weight:    0.396 do_ae_train: False
2022-12-28 19:27:50,933 - 2:18:34 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    35 rec:   14.532 mi: 1.85729504 zkl:   61.350 cd:    3.242 pos_prob:   68.481 prob_neg:   65.239 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    35 rec:   14.532 mi: 1.85729504 zkl:   61.350 cd:    3.242 pos_prob:   68.481 prob_neg:   65.239 kl_weight:    0.459 do_ae_train: False
2022-12-28 19:28:00,378 - 2:18:43 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    35 rec:   14.724 mi: 1.89050269 zkl:   59.095 cd:    1.864 pos_prob:   65.297 prob_neg:   63.433 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    35 rec:   14.724 mi: 1.89050269 zkl:   59.095 cd:    1.864 pos_prob:   65.297 prob_neg:   63.433 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:28:09,819 - 2:18:53 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    35 rec:   17.893 mi: 1.94870496 zkl:   56.525 cd:   -2.949 pos_prob:   59.545 prob_neg:   62.494 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    35 rec:   17.893 mi: 1.94870496 zkl:   56.525 cd:   -2.949 pos_prob:   59.545 prob_neg:   62.494 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:28:19,352 - 2:19:02 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    35 rec:   22.181 mi: 2.02816582 zkl:   64.552 cd:    2.574 pos_prob:   68.697 prob_neg:   66.123 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    35 rec:   22.181 mi: 2.02816582 zkl:   64.552 cd:    2.574 pos_prob:   68.697 prob_neg:   66.123 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:28:19,365 - 2:19:02 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-254.109
Langevin prior   1/ 40: energy=-254.109
2022-12-28 19:28:19,371 - 2:19:02 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-996.257
Langevin prior   6/ 40: energy=-996.257
2022-12-28 19:28:19,376 - 2:19:02 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1510.747
Langevin prior  11/ 40: energy=-1510.747
2022-12-28 19:28:19,382 - 2:19:02 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1754.455
Langevin prior  16/ 40: energy=-1754.455
2022-12-28 19:28:19,387 - 2:19:02 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1850.207
Langevin prior  21/ 40: energy=-1850.207
2022-12-28 19:28:19,394 - 2:19:02 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1898.752
Langevin prior  26/ 40: energy=-1898.752
2022-12-28 19:28:19,402 - 2:19:02 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1950.944
Langevin prior  31/ 40: energy=-1950.944
2022-12-28 19:28:19,410 - 2:19:02 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1961.723
Langevin prior  36/ 40: energy=-1961.723
2022-12-28 19:28:19,418 - 2:19:02 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2041.602
Langevin prior  40/ 40: energy=-2041.602
2022-12-28 19:38:05,343 - 2:28:48 - 585.9s - INFO - root - Negative Log-likehood -169.419679
200 done. -147.56070716959505
400 done. -141.7220810067782
600 done. -141.38572700946074
800 done. -142.02698795118513
1000 done. -140.84502021577836
1200 done. -141.43742211856258
1400 done. -142.75072319342453
1600 done. -141.07091476633258
1800 done. -140.52065183263116
2000 done. -157.06962856135397
2200 done. -167.8406342754753
2400 done. -179.52034095145
2600 done. -189.75585142542275
2800 done. -196.54016371029837
3000 done. -205.10625992134703
3200 done. -210.72224141103442
3400 done. -217.1268696813151
3600 done. -221.41621433501732
3800 done. -226.70342389744738
4000 done. -230.41082364542618
4200 done. -221.3441620838296
4400 done. -212.02403928506263
4600 done. -203.7303571489592
4800 done. -196.0533292361014
5000 done. -189.16454133401447
5200 done. -182.6681621533343
5400 done. -176.75739491685877
5600 done. -171.26443554986983
Negative Log-likehood -169.419679
2022-12-28 19:38:05,343 - 2:28:48 - 0.0s - INFO - root - log-likelihood:   -169.420
log-likelihood:   -169.420
2022-12-28 19:38:52,708 - 2:29:35 - 47.4s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-035-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 19:38:52,710 - 2:29:35 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-035-test-greedy.txt
Generation: 188 batches
2022-12-28 19:38:58,369 - 2:29:41 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:38:59,867 - 2:29:43 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:38:59,868 - 2:29:43 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:39:02,286 - 2:29:45 - 2.4s - INFO - root - --- bleu: BLEU = 29.51, 52.8/31.8/25.9/22.9 (BP=0.934, ratio=0.936, hyp_len=158995, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.51, 52.8/31.8/25.9/22.9 (BP=0.934, ratio=0.936, hyp_len=158995, ref_len=169777)
--- bleu: BLEU = 29.51, 52.8/31.8/25.9/22.9 (BP=0.934, ratio=0.936, hyp_len=158995, ref_len=169777)

2022-12-28 19:39:02,287 - 2:29:45 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:39:02,295 - 2:29:45 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:39:11,825 - 2:29:55 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    36 rec:   10.490 mi: 1.84658873 zkl:   57.695 cd:    2.646 pos_prob:   73.257 prob_neg:   70.611 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    36 rec:   10.490 mi: 1.84658873 zkl:   57.695 cd:    2.646 pos_prob:   73.257 prob_neg:   70.611 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:39:21,601 - 2:30:04 - 9.8s - INFO - root - batch/max_batch/ep:   200/   529/    36 rec:   16.628 mi: 1.97224307 zkl:   62.041 cd:    2.103 pos_prob:   67.555 prob_neg:   65.452 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    36 rec:   16.628 mi: 1.97224307 zkl:   62.041 cd:    2.103 pos_prob:   67.555 prob_neg:   65.452 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:39:30,945 - 2:30:14 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    36 rec:   15.621 mi: 1.87916100 zkl:   62.686 cd:    3.051 pos_prob:   74.455 prob_neg:   71.404 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    36 rec:   15.621 mi: 1.87916100 zkl:   62.686 cd:    3.051 pos_prob:   74.455 prob_neg:   71.404 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:39:40,475 - 2:30:23 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    36 rec:   17.469 mi: 1.97865129 zkl:   67.661 cd:  -23.082 pos_prob:   75.659 prob_neg:   98.741 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    36 rec:   17.469 mi: 1.97865129 zkl:   67.661 cd:  -23.082 pos_prob:   75.659 prob_neg:   98.741 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:39:49,853 - 2:30:33 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    36 rec:   18.102 mi: 1.75706601 zkl:   65.478 cd:   -2.401 pos_prob:   79.372 prob_neg:   81.773 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    36 rec:   18.102 mi: 1.75706601 zkl:   65.478 cd:   -2.401 pos_prob:   79.372 prob_neg:   81.773 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:39:49,866 - 2:30:33 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-243.534
Langevin prior   1/ 40: energy=-243.534
2022-12-28 19:39:49,871 - 2:30:33 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1168.483
Langevin prior   6/ 40: energy=-1168.483
2022-12-28 19:39:49,877 - 2:30:33 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1810.435
Langevin prior  11/ 40: energy=-1810.435
2022-12-28 19:39:49,882 - 2:30:33 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2165.076
Langevin prior  16/ 40: energy=-2165.076
2022-12-28 19:39:49,888 - 2:30:33 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2364.211
Langevin prior  21/ 40: energy=-2364.211
2022-12-28 19:39:49,896 - 2:30:33 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2497.511
Langevin prior  26/ 40: energy=-2497.511
2022-12-28 19:39:49,904 - 2:30:33 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2540.115
Langevin prior  31/ 40: energy=-2540.115
2022-12-28 19:39:49,913 - 2:30:33 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2585.864
Langevin prior  36/ 40: energy=-2585.864
2022-12-28 19:39:49,921 - 2:30:33 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2507.789
Langevin prior  40/ 40: energy=-2507.789
2022-12-28 19:49:35,933 - 2:40:19 - 586.0s - INFO - root - Negative Log-likehood -170.517028
200 done. -148.2355752499594
400 done. -142.65838303747654
600 done. -142.3115756078101
800 done. -142.85270540193773
1000 done. -141.7139341131871
1200 done. -142.2838829522146
1400 done. -143.57990704871438
1600 done. -141.9461103199007
1800 done. -141.40198406577318
2000 done. -158.17248818216365
2200 done. -169.19009943799708
2400 done. -181.03936345396409
2600 done. -191.35383240962923
2800 done. -198.2571773333552
3000 done. -206.8893389402664
3200 done. -212.45911324662654
3400 done. -218.95936880882306
3600 done. -223.28975867963803
3800 done. -228.5338510671304
4000 done. -232.143458165237
4200 done. -222.9618312360592
4400 done. -213.54732599034338
4600 done. -205.15336199253466
4800 done. -197.3940622678983
5000 done. -190.4373377494026
5200 done. -183.8934758758938
5400 done. -177.9167601577172
5600 done. -172.37037974774447
Negative Log-likehood -170.517028
2022-12-28 19:49:35,933 - 2:40:19 - 0.0s - INFO - root - log-likelihood:   -170.517
log-likelihood:   -170.517
2022-12-28 19:50:22,726 - 2:41:05 - 46.8s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-036-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 19:50:22,728 - 2:41:05 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-036-test-greedy.txt
Generation: 188 batches
2022-12-28 19:50:28,354 - 2:41:11 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:50:29,843 - 2:41:13 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:50:29,843 - 2:41:13 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:50:32,488 - 2:41:15 - 2.6s - INFO - root - --- bleu: BLEU = 29.59, 53.2/32.3/26.3/23.3 (BP=0.924, ratio=0.926, hyp_len=157296, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.59, 53.2/32.3/26.3/23.3 (BP=0.924, ratio=0.926, hyp_len=157296, ref_len=169777)
--- bleu: BLEU = 29.59, 53.2/32.3/26.3/23.3 (BP=0.924, ratio=0.926, hyp_len=157296, ref_len=169777)

2022-12-28 19:50:32,489 - 2:41:15 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:50:32,497 - 2:41:15 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:50:41,603 - 2:41:24 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    37 rec:   10.683 mi: 1.89853871 zkl:  152.612 cd:  -20.322 pos_prob:   63.498 prob_neg:   83.819 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    37 rec:   10.683 mi: 1.89853871 zkl:  152.612 cd:  -20.322 pos_prob:   63.498 prob_neg:   83.819 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:50:50,839 - 2:41:34 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    37 rec:   12.099 mi: 1.88548815 zkl:  159.211 cd:  -20.828 pos_prob:   55.863 prob_neg:   76.690 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    37 rec:   12.099 mi: 1.88548815 zkl:  159.211 cd:  -20.828 pos_prob:   55.863 prob_neg:   76.690 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:50:59,794 - 2:41:42 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    37 rec:   13.257 mi: 1.74531674 zkl:  158.299 cd:  -13.434 pos_prob:   54.115 prob_neg:   67.549 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    37 rec:   13.257 mi: 1.74531674 zkl:  158.299 cd:  -13.434 pos_prob:   54.115 prob_neg:   67.549 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:51:09,030 - 2:41:52 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    37 rec:   18.188 mi: 1.96420813 zkl:  163.088 cd:  -26.455 pos_prob:   50.104 prob_neg:   76.559 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    37 rec:   18.188 mi: 1.96420813 zkl:  163.088 cd:  -26.455 pos_prob:   50.104 prob_neg:   76.559 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:51:18,146 - 2:42:01 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    37 rec:   16.083 mi: 1.85145652 zkl:  162.404 cd:  -37.413 pos_prob:   45.253 prob_neg:   82.666 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    37 rec:   16.083 mi: 1.85145652 zkl:  162.404 cd:  -37.413 pos_prob:   45.253 prob_neg:   82.666 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:51:18,159 - 2:42:01 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-239.421
Langevin prior   1/ 40: energy=-239.421
2022-12-28 19:51:18,164 - 2:42:01 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1166.751
Langevin prior   6/ 40: energy=-1166.751
2022-12-28 19:51:18,169 - 2:42:01 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1791.347
Langevin prior  11/ 40: energy=-1791.347
2022-12-28 19:51:18,175 - 2:42:01 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2125.820
Langevin prior  16/ 40: energy=-2125.820
2022-12-28 19:51:18,180 - 2:42:01 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2298.838
Langevin prior  21/ 40: energy=-2298.838
2022-12-28 19:51:18,187 - 2:42:01 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2338.290
Langevin prior  26/ 40: energy=-2338.290
2022-12-28 19:51:18,196 - 2:42:01 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2331.627
Langevin prior  31/ 40: energy=-2331.627
2022-12-28 19:51:18,205 - 2:42:01 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2393.086
Langevin prior  36/ 40: energy=-2393.086
2022-12-28 19:51:18,212 - 2:42:01 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2436.508
Langevin prior  40/ 40: energy=-2436.508
2022-12-28 19:51:20,863 - 2:42:04 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-037-test-greedy.txt
Generation: 188 batches
2022-12-28 19:51:26,549 - 2:42:09 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:51:28,053 - 2:42:11 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:51:28,053 - 2:42:11 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:51:30,470 - 2:42:13 - 2.4s - INFO - root - --- bleu: BLEU = 31.26, 54.7/33.5/27.4/24.3 (BP=0.941, ratio=0.942, hyp_len=160006, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.26, 54.7/33.5/27.4/24.3 (BP=0.941, ratio=0.942, hyp_len=160006, ref_len=169777)
--- bleu: BLEU = 31.26, 54.7/33.5/27.4/24.3 (BP=0.941, ratio=0.942, hyp_len=160006, ref_len=169777)

2022-12-28 19:51:30,470 - 2:42:13 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:51:30,478 - 2:42:13 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:51:39,560 - 2:42:22 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    38 rec:   12.950 mi: 1.96933138 zkl:  162.125 cd:  -38.055 pos_prob:   40.925 prob_neg:   78.980 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    38 rec:   12.950 mi: 1.96933138 zkl:  162.125 cd:  -38.055 pos_prob:   40.925 prob_neg:   78.980 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:51:49,014 - 2:42:32 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    38 rec:   13.682 mi: 1.91183531 zkl:  165.139 cd:  -33.069 pos_prob:   40.360 prob_neg:   73.428 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    38 rec:   13.682 mi: 1.91183531 zkl:  165.139 cd:  -33.069 pos_prob:   40.360 prob_neg:   73.428 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:51:58,316 - 2:42:41 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    38 rec:   11.802 mi: 1.85162997 zkl:  163.840 cd:  -42.516 pos_prob:   45.828 prob_neg:   88.344 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    38 rec:   11.802 mi: 1.85162997 zkl:  163.840 cd:  -42.516 pos_prob:   45.828 prob_neg:   88.344 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:52:07,630 - 2:42:50 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    38 rec:   10.351 mi: 1.96243465 zkl:  167.046 cd:  -29.678 pos_prob:   46.584 prob_neg:   76.263 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    38 rec:   10.351 mi: 1.96243465 zkl:  167.046 cd:  -29.678 pos_prob:   46.584 prob_neg:   76.263 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:52:16,879 - 2:43:00 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    38 rec:    7.131 mi: 1.54623485 zkl:  163.998 cd:  -33.272 pos_prob:   46.707 prob_neg:   79.979 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    38 rec:    7.131 mi: 1.54623485 zkl:  163.998 cd:  -33.272 pos_prob:   46.707 prob_neg:   79.979 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:52:16,892 - 2:43:00 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-247.368
Langevin prior   1/ 40: energy=-247.368
2022-12-28 19:52:16,897 - 2:43:00 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1254.868
Langevin prior   6/ 40: energy=-1254.868
2022-12-28 19:52:16,902 - 2:43:00 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1882.532
Langevin prior  11/ 40: energy=-1882.532
2022-12-28 19:52:16,908 - 2:43:00 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2009.182
Langevin prior  16/ 40: energy=-2009.182
2022-12-28 19:52:16,914 - 2:43:00 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2207.070
Langevin prior  21/ 40: energy=-2207.070
2022-12-28 19:52:16,921 - 2:43:00 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2334.479
Langevin prior  26/ 40: energy=-2334.479
2022-12-28 19:52:16,929 - 2:43:00 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2404.234
Langevin prior  31/ 40: energy=-2404.234
2022-12-28 19:52:16,939 - 2:43:00 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2415.725
Langevin prior  36/ 40: energy=-2415.725
2022-12-28 19:52:16,946 - 2:43:00 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2368.750
Langevin prior  40/ 40: energy=-2368.750
2022-12-28 19:52:19,545 - 2:43:02 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-038-test-greedy.txt
Generation: 188 batches
2022-12-28 19:52:25,190 - 2:43:08 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:52:26,686 - 2:43:09 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:52:26,686 - 2:43:09 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:52:29,086 - 2:43:12 - 2.4s - INFO - root - --- bleu: BLEU = 31.43, 55.7/34.0/27.9/24.8 (BP=0.928, ratio=0.931, hyp_len=158013, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.43, 55.7/34.0/27.9/24.8 (BP=0.928, ratio=0.931, hyp_len=158013, ref_len=169777)
--- bleu: BLEU = 31.43, 55.7/34.0/27.9/24.8 (BP=0.928, ratio=0.931, hyp_len=158013, ref_len=169777)

2022-12-28 19:52:29,086 - 2:43:12 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:52:29,094 - 2:43:12 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:52:38,334 - 2:43:21 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    39 rec:   10.985 mi: 2.01288271 zkl:  169.260 cd:  -28.531 pos_prob:   42.057 prob_neg:   70.588 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    39 rec:   10.985 mi: 2.01288271 zkl:  169.260 cd:  -28.531 pos_prob:   42.057 prob_neg:   70.588 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:52:47,482 - 2:43:30 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    39 rec:    9.160 mi: 1.90387189 zkl:  168.947 cd:  -29.961 pos_prob:   42.130 prob_neg:   72.091 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    39 rec:    9.160 mi: 1.90387189 zkl:  168.947 cd:  -29.961 pos_prob:   42.130 prob_neg:   72.091 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:52:56,645 - 2:43:39 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    39 rec:   11.012 mi: 2.01453638 zkl:  169.264 cd:  -41.224 pos_prob:   44.426 prob_neg:   85.650 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    39 rec:   11.012 mi: 2.01453638 zkl:  169.264 cd:  -41.224 pos_prob:   44.426 prob_neg:   85.650 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:53:05,608 - 2:43:48 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    39 rec:   14.050 mi: 1.92427170 zkl:  169.119 cd:  -30.275 pos_prob:   41.981 prob_neg:   72.256 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    39 rec:   14.050 mi: 1.92427170 zkl:  169.119 cd:  -30.275 pos_prob:   41.981 prob_neg:   72.256 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:53:14,760 - 2:43:57 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    39 rec:   13.347 mi: 1.91757464 zkl:  176.645 cd:  -35.088 pos_prob:   40.128 prob_neg:   75.216 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    39 rec:   13.347 mi: 1.91757464 zkl:  176.645 cd:  -35.088 pos_prob:   40.128 prob_neg:   75.216 kl_weight:    0.000 do_ae_train: True
2022-12-28 19:53:14,772 - 2:43:57 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-273.107
Langevin prior   1/ 40: energy=-273.107
2022-12-28 19:53:14,778 - 2:43:57 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1110.252
Langevin prior   6/ 40: energy=-1110.252
2022-12-28 19:53:14,784 - 2:43:57 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1620.018
Langevin prior  11/ 40: energy=-1620.018
2022-12-28 19:53:14,789 - 2:43:57 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1992.783
Langevin prior  16/ 40: energy=-1992.783
2022-12-28 19:53:14,796 - 2:43:57 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2089.406
Langevin prior  21/ 40: energy=-2089.406
2022-12-28 19:53:14,804 - 2:43:58 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2140.940
Langevin prior  26/ 40: energy=-2140.940
2022-12-28 19:53:14,813 - 2:43:58 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2260.387
Langevin prior  31/ 40: energy=-2260.387
2022-12-28 19:53:14,822 - 2:43:58 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2235.476
Langevin prior  36/ 40: energy=-2235.476
2022-12-28 19:53:14,830 - 2:43:58 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2309.453
Langevin prior  40/ 40: energy=-2309.453
2022-12-28 19:53:17,427 - 2:44:00 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-039-test-greedy.txt
Generation: 188 batches
2022-12-28 19:53:23,073 - 2:44:06 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:53:24,571 - 2:44:07 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:53:24,571 - 2:44:07 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:53:27,006 - 2:44:10 - 2.4s - INFO - root - --- bleu: BLEU = 31.41, 55.9/34.2/28.1/25.0 (BP=0.923, ratio=0.926, hyp_len=157252, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.41, 55.9/34.2/28.1/25.0 (BP=0.923, ratio=0.926, hyp_len=157252, ref_len=169777)
--- bleu: BLEU = 31.41, 55.9/34.2/28.1/25.0 (BP=0.923, ratio=0.926, hyp_len=157252, ref_len=169777)

2022-12-28 19:53:27,007 - 2:44:10 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:53:27,015 - 2:44:10 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:53:36,438 - 2:44:19 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    40 rec:    9.133 mi: 1.99975526 zkl:   90.537 cd:   -8.994 pos_prob:   55.864 prob_neg:   64.858 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    40 rec:    9.133 mi: 1.99975526 zkl:   90.537 cd:   -8.994 pos_prob:   55.864 prob_neg:   64.858 kl_weight:    0.062 do_ae_train: False
2022-12-28 19:53:45,735 - 2:44:28 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    40 rec:   13.659 mi: 2.01518369 zkl:   79.552 cd:  -11.331 pos_prob:   50.795 prob_neg:   62.126 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    40 rec:   13.659 mi: 2.01518369 zkl:   79.552 cd:  -11.331 pos_prob:   50.795 prob_neg:   62.126 kl_weight:    0.125 do_ae_train: False
2022-12-28 19:53:55,171 - 2:44:38 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    40 rec:   16.140 mi: 1.75081575 zkl:   70.035 cd:   -1.901 pos_prob:   61.278 prob_neg:   63.179 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    40 rec:   16.140 mi: 1.75081575 zkl:   70.035 cd:   -1.901 pos_prob:   61.278 prob_neg:   63.179 kl_weight:    0.188 do_ae_train: False
2022-12-28 19:54:04,476 - 2:44:47 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    40 rec:   15.638 mi: 1.92403674 zkl:   72.335 cd:  -10.164 pos_prob:   52.324 prob_neg:   62.487 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    40 rec:   15.638 mi: 1.92403674 zkl:   72.335 cd:  -10.164 pos_prob:   52.324 prob_neg:   62.487 kl_weight:    0.251 do_ae_train: False
2022-12-28 19:54:13,791 - 2:44:56 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    40 rec:   14.678 mi: 1.94613647 zkl:   63.614 cd:   -4.822 pos_prob:   57.142 prob_neg:   61.964 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    40 rec:   14.678 mi: 1.94613647 zkl:   63.614 cd:   -4.822 pos_prob:   57.142 prob_neg:   61.964 kl_weight:    0.314 do_ae_train: False
2022-12-28 19:54:13,803 - 2:44:56 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-233.356
Langevin prior   1/ 40: energy=-233.356
2022-12-28 19:54:13,809 - 2:44:57 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-797.025
Langevin prior   6/ 40: energy=-797.025
2022-12-28 19:54:13,814 - 2:44:57 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1240.874
Langevin prior  11/ 40: energy=-1240.874
2022-12-28 19:54:13,819 - 2:44:57 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1521.757
Langevin prior  16/ 40: energy=-1521.757
2022-12-28 19:54:13,825 - 2:44:57 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1610.278
Langevin prior  21/ 40: energy=-1610.278
2022-12-28 19:54:13,832 - 2:44:57 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1704.443
Langevin prior  26/ 40: energy=-1704.443
2022-12-28 19:54:13,841 - 2:44:57 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1735.596
Langevin prior  31/ 40: energy=-1735.596
2022-12-28 19:54:13,850 - 2:44:57 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1806.622
Langevin prior  36/ 40: energy=-1806.622
2022-12-28 19:54:13,857 - 2:44:57 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1788.079
Langevin prior  40/ 40: energy=-1788.079
2022-12-28 19:54:16,524 - 2:44:59 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-040-test-greedy.txt
Generation: 188 batches
2022-12-28 19:54:22,183 - 2:45:05 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:54:23,697 - 2:45:06 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 19:54:23,697 - 2:45:06 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 19:54:26,117 - 2:45:09 - 2.4s - INFO - root - --- bleu: BLEU = 30.38, 54.0/32.7/26.7/23.6 (BP=0.936, ratio=0.938, hyp_len=159186, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.38, 54.0/32.7/26.7/23.6 (BP=0.936, ratio=0.938, hyp_len=159186, ref_len=169777)
--- bleu: BLEU = 30.38, 54.0/32.7/26.7/23.6 (BP=0.936, ratio=0.938, hyp_len=159186, ref_len=169777)

2022-12-28 19:54:26,118 - 2:45:09 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 19:54:26,126 - 2:45:09 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 19:54:35,630 - 2:45:18 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    41 rec:   12.204 mi: 1.87091470 zkl:   50.506 cd:   -5.085 pos_prob:   55.018 prob_neg:   60.103 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    41 rec:   12.204 mi: 1.87091470 zkl:   50.506 cd:   -5.085 pos_prob:   55.018 prob_neg:   60.103 kl_weight:    0.396 do_ae_train: False
2022-12-28 19:54:45,028 - 2:45:28 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    41 rec:   15.407 mi: 1.85554290 zkl:   59.756 cd:    0.528 pos_prob:   60.840 prob_neg:   60.312 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    41 rec:   15.407 mi: 1.85554290 zkl:   59.756 cd:    0.528 pos_prob:   60.840 prob_neg:   60.312 kl_weight:    0.459 do_ae_train: False
2022-12-28 19:54:54,306 - 2:45:37 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    41 rec:   16.200 mi: 2.05189204 zkl:   58.840 cd:   -5.905 pos_prob:   57.933 prob_neg:   63.838 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    41 rec:   16.200 mi: 2.05189204 zkl:   58.840 cd:   -5.905 pos_prob:   57.933 prob_neg:   63.838 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:55:03,499 - 2:45:46 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    41 rec:   13.099 mi: 1.90563726 zkl:   58.438 cd:    4.342 pos_prob:   66.044 prob_neg:   61.702 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    41 rec:   13.099 mi: 1.90563726 zkl:   58.438 cd:    4.342 pos_prob:   66.044 prob_neg:   61.702 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:55:12,785 - 2:45:55 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    41 rec:   17.542 mi: 1.94280028 zkl:   61.672 cd:   -3.474 pos_prob:   66.027 prob_neg:   69.502 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    41 rec:   17.542 mi: 1.94280028 zkl:   61.672 cd:   -3.474 pos_prob:   66.027 prob_neg:   69.502 kl_weight:    0.500 do_ae_train: False
2022-12-28 19:55:12,797 - 2:45:55 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-289.231
Langevin prior   1/ 40: energy=-289.231
2022-12-28 19:55:12,802 - 2:45:55 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1070.794
Langevin prior   6/ 40: energy=-1070.794
2022-12-28 19:55:12,807 - 2:45:56 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1551.183
Langevin prior  11/ 40: energy=-1551.183
2022-12-28 19:55:12,813 - 2:45:56 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1779.360
Langevin prior  16/ 40: energy=-1779.360
2022-12-28 19:55:12,818 - 2:45:56 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1889.789
Langevin prior  21/ 40: energy=-1889.789
2022-12-28 19:55:12,824 - 2:45:56 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1919.124
Langevin prior  26/ 40: energy=-1919.124
2022-12-28 19:55:12,831 - 2:45:56 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2025.324
Langevin prior  31/ 40: energy=-2025.324
2022-12-28 19:55:12,840 - 2:45:56 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2024.206
Langevin prior  36/ 40: energy=-2024.206
2022-12-28 19:55:12,847 - 2:45:56 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2051.111
Langevin prior  40/ 40: energy=-2051.111
2022-12-28 20:04:58,369 - 2:55:41 - 585.5s - INFO - root - Negative Log-likehood -176.087884
200 done. -153.42800200791638
400 done. -147.66949118428323
600 done. -147.04292217323558
800 done. -147.66788205066544
1000 done. -146.48543216774326
1200 done. -147.08547948051648
1400 done. -148.32800714497876
1600 done. -146.64968914091347
1800 done. -146.1104453302098
2000 done. -163.0889382183617
2200 done. -174.57960905934195
2400 done. -186.7475168388506
2600 done. -197.26209862892256
2800 done. -204.56759727320227
3000 done. -213.30643902921287
3200 done. -219.29546458885324
3400 done. -225.86916263169476
3600 done. -230.44167123325286
3800 done. -235.84354877073133
4000 done. -239.67796143548023
4200 done. -230.23295181904416
4400 done. -220.52763412943625
4600 done. -211.8672251568041
4800 done. -203.85075756558052
5000 done. -196.66779120916308
5200 done. -189.90630093230587
5400 done. -183.72981550100837
5600 done. -178.00523195415406
Negative Log-likehood -176.087884
2022-12-28 20:04:58,369 - 2:55:41 - 0.0s - INFO - root - log-likelihood:   -176.088
log-likelihood:   -176.088
2022-12-28 20:05:45,029 - 2:56:28 - 46.7s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-041-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 20:05:45,031 - 2:56:28 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-041-test-greedy.txt
Generation: 188 batches
2022-12-28 20:05:50,684 - 2:56:33 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:05:52,200 - 2:56:35 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:05:52,200 - 2:56:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:05:54,630 - 2:56:37 - 2.4s - INFO - root - --- bleu: BLEU = 29.75, 52.5/31.8/25.9/22.9 (BP=0.942, ratio=0.944, hyp_len=160268, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.75, 52.5/31.8/25.9/22.9 (BP=0.942, ratio=0.944, hyp_len=160268, ref_len=169777)
--- bleu: BLEU = 29.75, 52.5/31.8/25.9/22.9 (BP=0.942, ratio=0.944, hyp_len=160268, ref_len=169777)

2022-12-28 20:05:54,630 - 2:56:37 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:05:54,638 - 2:56:37 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:06:04,072 - 2:56:47 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    42 rec:   16.380 mi: 1.92055035 zkl:   59.364 cd:    1.156 pos_prob:   67.754 prob_neg:   66.597 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    42 rec:   16.380 mi: 1.92055035 zkl:   59.364 cd:    1.156 pos_prob:   67.754 prob_neg:   66.597 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:06:13,626 - 2:56:56 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/    42 rec:   20.854 mi: 1.94235647 zkl:   60.211 cd:   -5.763 pos_prob:   71.405 prob_neg:   77.169 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    42 rec:   20.854 mi: 1.94235647 zkl:   60.211 cd:   -5.763 pos_prob:   71.405 prob_neg:   77.169 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:06:22,943 - 2:57:06 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    42 rec:   17.747 mi: 1.93949103 zkl:   65.334 cd:    1.930 pos_prob:   76.548 prob_neg:   74.619 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    42 rec:   17.747 mi: 1.93949103 zkl:   65.334 cd:    1.930 pos_prob:   76.548 prob_neg:   74.619 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:06:32,586 - 2:57:15 - 9.6s - INFO - root - batch/max_batch/ep:   400/   529/    42 rec:   17.370 mi: 1.93840063 zkl:   64.315 cd:    0.636 pos_prob:   75.449 prob_neg:   74.814 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    42 rec:   17.370 mi: 1.93840063 zkl:   64.315 cd:    0.636 pos_prob:   75.449 prob_neg:   74.814 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:06:42,152 - 2:57:25 - 9.6s - INFO - root - batch/max_batch/ep:   500/   529/    42 rec:   13.724 mi: 1.86978567 zkl:   61.494 cd:   -3.234 pos_prob:   75.470 prob_neg:   78.704 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    42 rec:   13.724 mi: 1.86978567 zkl:   61.494 cd:   -3.234 pos_prob:   75.470 prob_neg:   78.704 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:06:42,164 - 2:57:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-263.197
Langevin prior   1/ 40: energy=-263.197
2022-12-28 20:06:42,170 - 2:57:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1214.530
Langevin prior   6/ 40: energy=-1214.530
2022-12-28 20:06:42,175 - 2:57:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1736.409
Langevin prior  11/ 40: energy=-1736.409
2022-12-28 20:06:42,180 - 2:57:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2005.552
Langevin prior  16/ 40: energy=-2005.552
2022-12-28 20:06:42,185 - 2:57:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2171.490
Langevin prior  21/ 40: energy=-2171.490
2022-12-28 20:06:42,191 - 2:57:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2262.454
Langevin prior  26/ 40: energy=-2262.454
2022-12-28 20:06:42,196 - 2:57:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2368.374
Langevin prior  31/ 40: energy=-2368.374
2022-12-28 20:06:42,203 - 2:57:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2310.915
Langevin prior  36/ 40: energy=-2310.915
2022-12-28 20:06:42,210 - 2:57:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2357.011
Langevin prior  40/ 40: energy=-2357.011
2022-12-28 20:16:28,016 - 3:07:11 - 585.8s - INFO - root - Negative Log-likehood -174.251898
200 done. -151.39275422595225
400 done. -145.6643913287884
600 done. -145.1327344639867
800 done. -145.82895965031855
1000 done. -144.83536400684432
1200 done. -145.52113579568342
1400 done. -146.83752872830064
1600 done. -145.10586478520915
1800 done. -144.5421672608239
2000 done. -161.51181601261757
2200 done. -173.1076226009812
2400 done. -184.93524221652402
2600 done. -195.34542905818373
2800 done. -202.6624094290313
3000 done. -211.26762110125648
3200 done. -217.15473587080416
3400 done. -223.90990290148443
3600 done. -228.45371572561655
3800 done. -233.96738323839105
4000 done. -237.7127462159352
4200 done. -228.29663189103843
4400 done. -218.6253449019848
4600 done. -209.97190789186678
4800 done. -201.97275265317717
5000 done. -194.80663116125962
5200 done. -188.05684918626116
5400 done. -181.8909954916296
5600 done. -176.169446929445
Negative Log-likehood -174.251898
2022-12-28 20:16:28,016 - 3:07:11 - 0.0s - INFO - root - log-likelihood:   -174.252
log-likelihood:   -174.252
2022-12-28 20:17:14,391 - 3:07:57 - 46.4s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-042-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 20:17:14,393 - 3:07:57 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-042-test-greedy.txt
Generation: 188 batches
2022-12-28 20:17:20,049 - 3:08:03 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:17:21,550 - 3:08:04 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:17:21,551 - 3:08:04 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:17:24,006 - 3:08:07 - 2.5s - INFO - root - --- bleu: BLEU = 29.77, 53.0/32.0/26.2/23.2 (BP=0.935, ratio=0.937, hyp_len=159086, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.77, 53.0/32.0/26.2/23.2 (BP=0.935, ratio=0.937, hyp_len=159086, ref_len=169777)
--- bleu: BLEU = 29.77, 53.0/32.0/26.2/23.2 (BP=0.935, ratio=0.937, hyp_len=159086, ref_len=169777)

2022-12-28 20:17:24,007 - 3:08:07 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:17:24,015 - 3:08:07 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:17:33,181 - 3:08:16 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    43 rec:    9.733 mi: 1.91315711 zkl:  146.934 cd:   -4.532 pos_prob:   65.833 prob_neg:   70.364 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    43 rec:    9.733 mi: 1.91315711 zkl:  146.934 cd:   -4.532 pos_prob:   65.833 prob_neg:   70.364 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:17:42,387 - 3:08:25 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    43 rec:    9.804 mi: 1.88290370 zkl:  155.527 cd:   -9.921 pos_prob:   62.268 prob_neg:   72.188 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    43 rec:    9.804 mi: 1.88290370 zkl:  155.527 cd:   -9.921 pos_prob:   62.268 prob_neg:   72.188 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:17:51,432 - 3:08:34 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    43 rec:   10.638 mi: 2.06675959 zkl:  159.972 cd:  -23.150 pos_prob:   49.493 prob_neg:   72.643 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    43 rec:   10.638 mi: 2.06675959 zkl:  159.972 cd:  -23.150 pos_prob:   49.493 prob_neg:   72.643 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:18:00,627 - 3:08:43 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    43 rec:    9.625 mi: 2.03184032 zkl:  163.373 cd:  -23.146 pos_prob:   53.330 prob_neg:   76.476 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    43 rec:    9.625 mi: 2.03184032 zkl:  163.373 cd:  -23.146 pos_prob:   53.330 prob_neg:   76.476 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:18:09,736 - 3:08:52 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    43 rec:   12.629 mi: 1.96591711 zkl:  168.239 cd:  -26.369 pos_prob:   50.439 prob_neg:   76.809 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    43 rec:   12.629 mi: 1.96591711 zkl:  168.239 cd:  -26.369 pos_prob:   50.439 prob_neg:   76.809 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:18:09,748 - 3:08:52 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-263.059
Langevin prior   1/ 40: energy=-263.059
2022-12-28 20:18:09,754 - 3:08:52 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1097.570
Langevin prior   6/ 40: energy=-1097.570
2022-12-28 20:18:09,759 - 3:08:52 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1610.463
Langevin prior  11/ 40: energy=-1610.463
2022-12-28 20:18:09,764 - 3:08:52 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1882.772
Langevin prior  16/ 40: energy=-1882.772
2022-12-28 20:18:09,770 - 3:08:52 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2003.533
Langevin prior  21/ 40: energy=-2003.533
2022-12-28 20:18:09,775 - 3:08:52 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2013.546
Langevin prior  26/ 40: energy=-2013.546
2022-12-28 20:18:09,783 - 3:08:52 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2015.545
Langevin prior  31/ 40: energy=-2015.545
2022-12-28 20:18:09,791 - 3:08:52 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2060.968
Langevin prior  36/ 40: energy=-2060.968
2022-12-28 20:18:09,798 - 3:08:52 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2068.325
Langevin prior  40/ 40: energy=-2068.325
2022-12-28 20:18:12,370 - 3:08:55 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-043-test-greedy.txt
Generation: 188 batches
2022-12-28 20:18:18,029 - 3:09:01 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:18:19,529 - 3:09:02 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:18:19,529 - 3:09:02 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:18:21,979 - 3:09:05 - 2.4s - INFO - root - --- bleu: BLEU = 31.21, 54.6/33.3/27.2/24.2 (BP=0.944, ratio=0.946, hyp_len=160528, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.21, 54.6/33.3/27.2/24.2 (BP=0.944, ratio=0.946, hyp_len=160528, ref_len=169777)
--- bleu: BLEU = 31.21, 54.6/33.3/27.2/24.2 (BP=0.944, ratio=0.946, hyp_len=160528, ref_len=169777)

2022-12-28 20:18:21,980 - 3:09:05 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:18:21,988 - 3:09:05 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:18:31,171 - 3:09:14 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    44 rec:    7.992 mi: 1.79808915 zkl:  164.352 cd:  -27.527 pos_prob:   50.059 prob_neg:   77.586 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    44 rec:    7.992 mi: 1.79808915 zkl:  164.352 cd:  -27.527 pos_prob:   50.059 prob_neg:   77.586 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:18:40,400 - 3:09:23 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    44 rec:    8.081 mi: 1.65951526 zkl:  166.851 cd:  -29.438 pos_prob:   47.396 prob_neg:   76.834 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    44 rec:    8.081 mi: 1.65951526 zkl:  166.851 cd:  -29.438 pos_prob:   47.396 prob_neg:   76.834 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:18:49,380 - 3:09:32 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    44 rec:    8.143 mi: 2.02282548 zkl:  166.385 cd:  -30.191 pos_prob:   44.076 prob_neg:   74.267 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    44 rec:    8.143 mi: 2.02282548 zkl:  166.385 cd:  -30.191 pos_prob:   44.076 prob_neg:   74.267 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:18:58,435 - 3:09:41 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    44 rec:    9.801 mi: 2.07167053 zkl:  166.963 cd:  -30.774 pos_prob:   42.095 prob_neg:   72.869 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    44 rec:    9.801 mi: 2.07167053 zkl:  166.963 cd:  -30.774 pos_prob:   42.095 prob_neg:   72.869 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:19:07,466 - 3:09:50 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/    44 rec:   12.043 mi: 1.85952604 zkl:  164.657 cd:  -31.072 pos_prob:   41.214 prob_neg:   72.286 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    44 rec:   12.043 mi: 1.85952604 zkl:  164.657 cd:  -31.072 pos_prob:   41.214 prob_neg:   72.286 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:19:07,478 - 3:09:50 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-277.424
Langevin prior   1/ 40: energy=-277.424
2022-12-28 20:19:07,484 - 3:09:50 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1090.800
Langevin prior   6/ 40: energy=-1090.800
2022-12-28 20:19:07,489 - 3:09:50 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1622.138
Langevin prior  11/ 40: energy=-1622.138
2022-12-28 20:19:07,494 - 3:09:50 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1976.206
Langevin prior  16/ 40: energy=-1976.206
2022-12-28 20:19:07,500 - 3:09:50 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2063.195
Langevin prior  21/ 40: energy=-2063.195
2022-12-28 20:19:07,507 - 3:09:50 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2118.072
Langevin prior  26/ 40: energy=-2118.072
2022-12-28 20:19:07,516 - 3:09:50 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2109.072
Langevin prior  31/ 40: energy=-2109.072
2022-12-28 20:19:07,525 - 3:09:50 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2166.494
Langevin prior  36/ 40: energy=-2166.494
2022-12-28 20:19:07,533 - 3:09:50 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2113.481
Langevin prior  40/ 40: energy=-2113.481
2022-12-28 20:19:10,130 - 3:09:53 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-044-test-greedy.txt
Generation: 188 batches
2022-12-28 20:19:15,786 - 3:09:58 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:19:17,293 - 3:10:00 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:19:17,293 - 3:10:00 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:19:19,719 - 3:10:02 - 2.4s - INFO - root - --- bleu: BLEU = 31.15, 54.2/33.1/27.1/24.0 (BP=0.948, ratio=0.949, hyp_len=161156, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.15, 54.2/33.1/27.1/24.0 (BP=0.948, ratio=0.949, hyp_len=161156, ref_len=169777)
--- bleu: BLEU = 31.15, 54.2/33.1/27.1/24.0 (BP=0.948, ratio=0.949, hyp_len=161156, ref_len=169777)

2022-12-28 20:19:19,719 - 3:10:02 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:19:19,727 - 3:10:02 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:19:28,718 - 3:10:11 - 9.0s - INFO - root - batch/max_batch/ep:   100/   529/    45 rec:    9.972 mi: 1.88302469 zkl:  165.712 cd:  -34.746 pos_prob:   40.818 prob_neg:   75.565 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    45 rec:    9.972 mi: 1.88302469 zkl:  165.712 cd:  -34.746 pos_prob:   40.818 prob_neg:   75.565 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:19:37,774 - 3:10:20 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    45 rec:    9.604 mi: 2.02139950 zkl:  169.878 cd:  -30.700 pos_prob:   45.096 prob_neg:   75.796 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    45 rec:    9.604 mi: 2.02139950 zkl:  169.878 cd:  -30.700 pos_prob:   45.096 prob_neg:   75.796 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:19:46,822 - 3:10:30 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    45 rec:   12.338 mi: 1.92071974 zkl:  171.605 cd:  -35.932 pos_prob:   35.158 prob_neg:   71.090 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    45 rec:   12.338 mi: 1.92071974 zkl:  171.605 cd:  -35.932 pos_prob:   35.158 prob_neg:   71.090 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:19:55,801 - 3:10:38 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    45 rec:   10.804 mi: 1.84859586 zkl:  172.864 cd:  -28.447 pos_prob:   43.119 prob_neg:   71.567 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    45 rec:   10.804 mi: 1.84859586 zkl:  172.864 cd:  -28.447 pos_prob:   43.119 prob_neg:   71.567 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:20:04,898 - 3:10:48 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    45 rec:    6.378 mi: 1.86546361 zkl:  166.135 cd:  -24.999 pos_prob:   44.086 prob_neg:   69.085 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    45 rec:    6.378 mi: 1.86546361 zkl:  166.135 cd:  -24.999 pos_prob:   44.086 prob_neg:   69.085 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:20:04,911 - 3:10:48 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-265.710
Langevin prior   1/ 40: energy=-265.710
2022-12-28 20:20:04,916 - 3:10:48 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1068.316
Langevin prior   6/ 40: energy=-1068.316
2022-12-28 20:20:04,921 - 3:10:48 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1695.724
Langevin prior  11/ 40: energy=-1695.724
2022-12-28 20:20:04,926 - 3:10:48 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1968.242
Langevin prior  16/ 40: energy=-1968.242
2022-12-28 20:20:04,931 - 3:10:48 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2147.256
Langevin prior  21/ 40: energy=-2147.256
2022-12-28 20:20:04,937 - 3:10:48 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2163.532
Langevin prior  26/ 40: energy=-2163.532
2022-12-28 20:20:04,945 - 3:10:48 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2157.440
Langevin prior  31/ 40: energy=-2157.440
2022-12-28 20:20:04,953 - 3:10:48 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2072.623
Langevin prior  36/ 40: energy=-2072.623
2022-12-28 20:20:04,960 - 3:10:48 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2118.571
Langevin prior  40/ 40: energy=-2118.571
2022-12-28 20:20:07,509 - 3:10:50 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-045-test-greedy.txt
Generation: 188 batches
2022-12-28 20:20:13,151 - 3:10:56 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:20:14,637 - 3:10:57 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:20:14,637 - 3:10:57 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:20:17,026 - 3:11:00 - 2.4s - INFO - root - --- bleu: BLEU = 31.29, 55.9/34.3/28.1/24.9 (BP=0.919, ratio=0.923, hyp_len=156623, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.29, 55.9/34.3/28.1/24.9 (BP=0.919, ratio=0.923, hyp_len=156623, ref_len=169777)
--- bleu: BLEU = 31.29, 55.9/34.3/28.1/24.9 (BP=0.919, ratio=0.923, hyp_len=156623, ref_len=169777)

2022-12-28 20:20:17,026 - 3:11:00 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:20:17,034 - 3:11:00 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:20:26,273 - 3:11:09 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    46 rec:   11.759 mi: 1.81187999 zkl:   95.836 cd:  -16.120 pos_prob:   58.271 prob_neg:   74.391 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    46 rec:   11.759 mi: 1.81187999 zkl:   95.836 cd:  -16.120 pos_prob:   58.271 prob_neg:   74.391 kl_weight:    0.062 do_ae_train: False
2022-12-28 20:20:35,531 - 3:11:18 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    46 rec:   11.237 mi: 1.96310210 zkl:   84.589 cd:   -3.490 pos_prob:   59.262 prob_neg:   62.752 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    46 rec:   11.237 mi: 1.96310210 zkl:   84.589 cd:   -3.490 pos_prob:   59.262 prob_neg:   62.752 kl_weight:    0.125 do_ae_train: False
2022-12-28 20:20:44,825 - 3:11:28 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    46 rec:   13.094 mi: 2.01208639 zkl:   76.784 cd:    5.771 pos_prob:   65.638 prob_neg:   59.867 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    46 rec:   13.094 mi: 2.01208639 zkl:   76.784 cd:    5.771 pos_prob:   65.638 prob_neg:   59.867 kl_weight:    0.188 do_ae_train: False
2022-12-28 20:20:54,158 - 3:11:37 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    46 rec:   14.564 mi: 2.27315283 zkl:   81.485 cd:    4.744 pos_prob:   68.340 prob_neg:   63.596 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    46 rec:   14.564 mi: 2.27315283 zkl:   81.485 cd:    4.744 pos_prob:   68.340 prob_neg:   63.596 kl_weight:    0.251 do_ae_train: False
2022-12-28 20:21:03,412 - 3:11:46 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    46 rec:   14.530 mi: 2.32479882 zkl:   74.682 cd:    2.778 pos_prob:   67.490 prob_neg:   64.713 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    46 rec:   14.530 mi: 2.32479882 zkl:   74.682 cd:    2.778 pos_prob:   67.490 prob_neg:   64.713 kl_weight:    0.314 do_ae_train: False
2022-12-28 20:21:03,425 - 3:11:46 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-291.054
Langevin prior   1/ 40: energy=-291.054
2022-12-28 20:21:03,430 - 3:11:46 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-979.509
Langevin prior   6/ 40: energy=-979.509
2022-12-28 20:21:03,436 - 3:11:46 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1661.191
Langevin prior  11/ 40: energy=-1661.191
2022-12-28 20:21:03,441 - 3:11:46 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1875.946
Langevin prior  16/ 40: energy=-1875.946
2022-12-28 20:21:03,447 - 3:11:46 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1951.398
Langevin prior  21/ 40: energy=-1951.398
2022-12-28 20:21:03,454 - 3:11:46 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1971.285
Langevin prior  26/ 40: energy=-1971.285
2022-12-28 20:21:03,463 - 3:11:46 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2001.169
Langevin prior  31/ 40: energy=-2001.169
2022-12-28 20:21:03,472 - 3:11:46 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2001.281
Langevin prior  36/ 40: energy=-2001.281
2022-12-28 20:21:03,481 - 3:11:46 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1978.875
Langevin prior  40/ 40: energy=-1978.875
2022-12-28 20:21:06,117 - 3:11:49 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-046-test-greedy.txt
Generation: 188 batches
2022-12-28 20:21:11,766 - 3:11:54 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:21:13,263 - 3:11:56 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:21:13,263 - 3:11:56 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:21:15,720 - 3:11:58 - 2.5s - INFO - root - --- bleu: BLEU = 30.17, 54.0/32.8/26.7/23.7 (BP=0.927, ratio=0.929, hyp_len=157785, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.17, 54.0/32.8/26.7/23.7 (BP=0.927, ratio=0.929, hyp_len=157785, ref_len=169777)
--- bleu: BLEU = 30.17, 54.0/32.8/26.7/23.7 (BP=0.927, ratio=0.929, hyp_len=157785, ref_len=169777)

2022-12-28 20:21:15,721 - 3:11:58 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:21:15,729 - 3:11:58 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:21:25,042 - 3:12:08 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    47 rec:   18.250 mi: 2.28749990 zkl:   69.542 cd:   -0.450 pos_prob:   69.791 prob_neg:   70.241 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    47 rec:   18.250 mi: 2.28749990 zkl:   69.542 cd:   -0.450 pos_prob:   69.791 prob_neg:   70.241 kl_weight:    0.396 do_ae_train: False
2022-12-28 20:21:34,382 - 3:12:17 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    47 rec:   13.061 mi: 2.23104787 zkl:   62.283 cd:   -4.296 pos_prob:   67.164 prob_neg:   71.459 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    47 rec:   13.061 mi: 2.23104787 zkl:   62.283 cd:   -4.296 pos_prob:   67.164 prob_neg:   71.459 kl_weight:    0.459 do_ae_train: False
2022-12-28 20:21:43,832 - 3:12:27 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    47 rec:   18.519 mi: 2.31124973 zkl:   66.209 cd:    1.063 pos_prob:   72.930 prob_neg:   71.867 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    47 rec:   18.519 mi: 2.31124973 zkl:   66.209 cd:    1.063 pos_prob:   72.930 prob_neg:   71.867 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:21:53,241 - 3:12:36 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    47 rec:   16.590 mi: 2.27121282 zkl:   67.490 cd:    1.012 pos_prob:   75.265 prob_neg:   74.253 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    47 rec:   16.590 mi: 2.27121282 zkl:   67.490 cd:    1.012 pos_prob:   75.265 prob_neg:   74.253 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:22:02,615 - 3:12:45 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    47 rec:   14.563 mi: 2.07048321 zkl:   62.592 cd:   -3.124 pos_prob:   75.138 prob_neg:   78.263 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    47 rec:   14.563 mi: 2.07048321 zkl:   62.592 cd:   -3.124 pos_prob:   75.138 prob_neg:   78.263 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:22:02,627 - 3:12:45 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-310.568
Langevin prior   1/ 40: energy=-310.568
2022-12-28 20:22:02,633 - 3:12:45 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1287.311
Langevin prior   6/ 40: energy=-1287.311
2022-12-28 20:22:02,638 - 3:12:45 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1867.013
Langevin prior  11/ 40: energy=-1867.013
2022-12-28 20:22:02,643 - 3:12:45 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2236.947
Langevin prior  16/ 40: energy=-2236.947
2022-12-28 20:22:02,648 - 3:12:45 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2356.424
Langevin prior  21/ 40: energy=-2356.424
2022-12-28 20:22:02,654 - 3:12:45 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2354.193
Langevin prior  26/ 40: energy=-2354.193
2022-12-28 20:22:02,659 - 3:12:45 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2392.617
Langevin prior  31/ 40: energy=-2392.617
2022-12-28 20:22:02,665 - 3:12:45 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2343.043
Langevin prior  36/ 40: energy=-2343.043
2022-12-28 20:22:02,671 - 3:12:45 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2412.191
Langevin prior  40/ 40: energy=-2412.191
2022-12-28 20:31:48,006 - 3:22:31 - 585.3s - INFO - root - Negative Log-likehood -179.578784
200 done. -157.30948615556923
400 done. -151.30332833613875
600 done. -150.63068561849977
800 done. -151.01274610964015
1000 done. -149.73501449171923
1200 done. -150.35025549625902
1400 done. -151.78793114691697
1600 done. -149.97884032467172
1800 done. -149.41791342860049
2000 done. -166.65451472889487
2200 done. -178.3540603853996
2400 done. -190.60793315288734
2600 done. -201.49994403157018
2800 done. -208.70954292995205
3000 done. -217.80253386917846
3200 done. -223.80370455473354
3400 done. -230.76909512796055
3600 done. -235.33350183075729
3800 done. -240.97782425499224
4000 done. -244.809811001861
4200 done. -235.09411945106328
4400 done. -225.15889364493364
4600 done. -216.28979410826682
4800 done. -208.06389329080386
5000 done. -200.69785035444804
5200 done. -193.76738878165798
5400 done. -187.44383204739998
5600 done. -181.54707688385116
Negative Log-likehood -179.578784
2022-12-28 20:31:48,007 - 3:22:31 - 0.0s - INFO - root - log-likelihood:   -179.579
log-likelihood:   -179.579
2022-12-28 20:32:34,318 - 3:23:17 - 46.3s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-047-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 20:32:34,320 - 3:23:17 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-047-test-greedy.txt
Generation: 188 batches
2022-12-28 20:32:39,976 - 3:23:23 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:32:41,496 - 3:23:24 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:32:41,497 - 3:23:24 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:32:44,208 - 3:23:27 - 2.7s - INFO - root - --- bleu: BLEU = 29.67, 52.3/31.5/25.7/22.7 (BP=0.947, ratio=0.948, hyp_len=160985, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.67, 52.3/31.5/25.7/22.7 (BP=0.947, ratio=0.948, hyp_len=160985, ref_len=169777)
--- bleu: BLEU = 29.67, 52.3/31.5/25.7/22.7 (BP=0.947, ratio=0.948, hyp_len=160985, ref_len=169777)

2022-12-28 20:32:44,209 - 3:23:27 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:32:44,229 - 3:23:27 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:32:53,740 - 3:23:36 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    48 rec:   18.046 mi: 2.33099008 zkl:   68.642 cd:    0.979 pos_prob:   77.722 prob_neg:   76.743 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    48 rec:   18.046 mi: 2.33099008 zkl:   68.642 cd:    0.979 pos_prob:   77.722 prob_neg:   76.743 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:33:03,054 - 3:23:46 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    48 rec:   12.171 mi: 2.05237937 zkl:   66.253 cd:    2.637 pos_prob:   79.179 prob_neg:   76.542 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    48 rec:   12.171 mi: 2.05237937 zkl:   66.253 cd:    2.637 pos_prob:   79.179 prob_neg:   76.542 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:33:12,396 - 3:23:55 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    48 rec:   16.516 mi: 2.35719633 zkl:   66.273 cd:    1.468 pos_prob:   78.774 prob_neg:   77.306 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    48 rec:   16.516 mi: 2.35719633 zkl:   66.273 cd:    1.468 pos_prob:   78.774 prob_neg:   77.306 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:33:21,869 - 3:24:05 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    48 rec:   17.703 mi: 2.24393368 zkl:   70.415 cd:    1.394 pos_prob:   82.337 prob_neg:   80.943 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    48 rec:   17.703 mi: 2.24393368 zkl:   70.415 cd:    1.394 pos_prob:   82.337 prob_neg:   80.943 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:33:31,030 - 3:24:14 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    48 rec:   16.555 mi: 2.08379579 zkl:   65.800 cd:   -2.377 pos_prob:   80.156 prob_neg:   82.533 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    48 rec:   16.555 mi: 2.08379579 zkl:   65.800 cd:   -2.377 pos_prob:   80.156 prob_neg:   82.533 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:33:31,042 - 3:24:14 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-249.932
Langevin prior   1/ 40: energy=-249.932
2022-12-28 20:33:31,048 - 3:24:14 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1231.351
Langevin prior   6/ 40: energy=-1231.351
2022-12-28 20:33:31,053 - 3:24:14 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1833.203
Langevin prior  11/ 40: energy=-1833.203
2022-12-28 20:33:31,059 - 3:24:14 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2129.282
Langevin prior  16/ 40: energy=-2129.282
2022-12-28 20:33:31,064 - 3:24:14 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2264.483
Langevin prior  21/ 40: energy=-2264.483
2022-12-28 20:33:31,070 - 3:24:14 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2347.367
Langevin prior  26/ 40: energy=-2347.367
2022-12-28 20:33:31,077 - 3:24:14 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2318.605
Langevin prior  31/ 40: energy=-2318.605
2022-12-28 20:33:31,085 - 3:24:14 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2356.917
Langevin prior  36/ 40: energy=-2356.917
2022-12-28 20:33:31,092 - 3:24:14 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2427.590
Langevin prior  40/ 40: energy=-2427.590
2022-12-28 20:43:17,063 - 3:34:00 - 586.0s - INFO - root - Negative Log-likehood -180.058390
200 done. -156.4939260567759
400 done. -150.69740090289633
600 done. -149.91476097356028
800 done. -150.70994872246024
1000 done. -149.59929876777437
1200 done. -150.32538358341063
1400 done. -151.7685917000993
1600 done. -149.99971775520032
1800 done. -149.43008177816236
2000 done. -166.7843810300718
2200 done. -178.52072892874605
2400 done. -190.97090572218184
2600 done. -201.83488867317234
2800 done. -209.10315772367284
3000 done. -218.3851013079457
3200 done. -224.24884375544156
3400 done. -231.2945475211586
3600 done. -235.96393317767314
3800 done. -241.5742113784943
4000 done. -245.4329545474938
4200 done. -235.7173932176764
4400 done. -225.7505091855098
4600 done. -216.85822644723788
4800 done. -208.615874633545
5000 done. -201.22291154781306
5200 done. -194.26949893645744
5400 done. -187.93532197627493
5600 done. -182.03181799894512
Negative Log-likehood -180.058390
2022-12-28 20:43:17,063 - 3:34:00 - 0.0s - INFO - root - log-likelihood:   -180.058
log-likelihood:   -180.058
2022-12-28 20:44:03,358 - 3:34:46 - 46.3s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-048-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 20:44:03,360 - 3:34:46 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-048-test-greedy.txt
Generation: 188 batches
2022-12-28 20:44:09,012 - 3:34:52 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:44:10,516 - 3:34:53 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:44:10,516 - 3:34:53 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:44:12,970 - 3:34:56 - 2.5s - INFO - root - --- bleu: BLEU = 29.74, 53.1/32.1/26.3/23.4 (BP=0.929, ratio=0.931, hyp_len=158126, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.74, 53.1/32.1/26.3/23.4 (BP=0.929, ratio=0.931, hyp_len=158126, ref_len=169777)
--- bleu: BLEU = 29.74, 53.1/32.1/26.3/23.4 (BP=0.929, ratio=0.931, hyp_len=158126, ref_len=169777)

2022-12-28 20:44:12,971 - 3:34:56 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:44:12,979 - 3:34:56 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:44:22,016 - 3:35:05 - 9.0s - INFO - root - batch/max_batch/ep:   100/   529/    49 rec:   11.239 mi: 2.20968771 zkl:  152.002 cd:  -13.305 pos_prob:   65.783 prob_neg:   79.088 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    49 rec:   11.239 mi: 2.20968771 zkl:  152.002 cd:  -13.305 pos_prob:   65.783 prob_neg:   79.088 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:44:31,081 - 3:35:14 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    49 rec:    9.114 mi: 2.21451950 zkl:  160.697 cd:  -19.851 pos_prob:   58.836 prob_neg:   78.688 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    49 rec:    9.114 mi: 2.21451950 zkl:  160.697 cd:  -19.851 pos_prob:   58.836 prob_neg:   78.688 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:44:40,381 - 3:35:23 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    49 rec:   10.627 mi: 2.17761111 zkl:  165.557 cd:  -24.413 pos_prob:   54.596 prob_neg:   79.009 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    49 rec:   10.627 mi: 2.17761111 zkl:  165.557 cd:  -24.413 pos_prob:   54.596 prob_neg:   79.009 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:44:49,466 - 3:35:32 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    49 rec:   12.587 mi: 2.21207070 zkl:  168.638 cd:  -32.514 pos_prob:   50.209 prob_neg:   82.723 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    49 rec:   12.587 mi: 2.21207070 zkl:  168.638 cd:  -32.514 pos_prob:   50.209 prob_neg:   82.723 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:44:58,442 - 3:35:41 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/    49 rec:   14.202 mi: 2.11376047 zkl:  172.671 cd:  -30.626 pos_prob:   48.455 prob_neg:   79.081 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    49 rec:   14.202 mi: 2.11376047 zkl:  172.671 cd:  -30.626 pos_prob:   48.455 prob_neg:   79.081 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:44:58,454 - 3:35:41 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-307.711
Langevin prior   1/ 40: energy=-307.711
2022-12-28 20:44:58,459 - 3:35:41 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1248.492
Langevin prior   6/ 40: energy=-1248.492
2022-12-28 20:44:58,464 - 3:35:41 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1817.083
Langevin prior  11/ 40: energy=-1817.083
2022-12-28 20:44:58,470 - 3:35:41 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2135.296
Langevin prior  16/ 40: energy=-2135.296
2022-12-28 20:44:58,476 - 3:35:41 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2330.175
Langevin prior  21/ 40: energy=-2330.175
2022-12-28 20:44:58,482 - 3:35:41 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2361.310
Langevin prior  26/ 40: energy=-2361.310
2022-12-28 20:44:58,490 - 3:35:41 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2363.867
Langevin prior  31/ 40: energy=-2363.867
2022-12-28 20:44:58,498 - 3:35:41 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2401.755
Langevin prior  36/ 40: energy=-2401.755
2022-12-28 20:44:58,506 - 3:35:41 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2383.833
Langevin prior  40/ 40: energy=-2383.833
2022-12-28 20:45:01,173 - 3:35:44 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-049-test-greedy.txt
Generation: 188 batches
2022-12-28 20:45:06,836 - 3:35:50 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:45:08,322 - 3:35:51 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:45:08,322 - 3:35:51 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:45:10,719 - 3:35:53 - 2.4s - INFO - root - --- bleu: BLEU = 31.21, 55.1/33.7/27.6/24.4 (BP=0.933, ratio=0.935, hyp_len=158817, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.21, 55.1/33.7/27.6/24.4 (BP=0.933, ratio=0.935, hyp_len=158817, ref_len=169777)
--- bleu: BLEU = 31.21, 55.1/33.7/27.6/24.4 (BP=0.933, ratio=0.935, hyp_len=158817, ref_len=169777)

2022-12-28 20:45:10,720 - 3:35:53 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:45:10,728 - 3:35:53 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:45:19,806 - 3:36:03 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    50 rec:   10.431 mi: 2.08454180 zkl:  171.789 cd:  -34.635 pos_prob:   46.427 prob_neg:   81.061 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    50 rec:   10.431 mi: 2.08454180 zkl:  171.789 cd:  -34.635 pos_prob:   46.427 prob_neg:   81.061 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:45:28,936 - 3:36:12 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    50 rec:    9.402 mi: 2.16114759 zkl:  169.281 cd:  -30.676 pos_prob:   51.018 prob_neg:   81.695 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    50 rec:    9.402 mi: 2.16114759 zkl:  169.281 cd:  -30.676 pos_prob:   51.018 prob_neg:   81.695 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:45:37,912 - 3:36:21 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    50 rec:    9.643 mi: 2.19620538 zkl:  173.002 cd:  -32.137 pos_prob:   49.826 prob_neg:   81.963 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    50 rec:    9.643 mi: 2.19620538 zkl:  173.002 cd:  -32.137 pos_prob:   49.826 prob_neg:   81.963 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:45:47,022 - 3:36:30 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    50 rec:   11.389 mi: 2.10434175 zkl:  175.826 cd:  -31.740 pos_prob:   47.941 prob_neg:   79.681 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    50 rec:   11.389 mi: 2.10434175 zkl:  175.826 cd:  -31.740 pos_prob:   47.941 prob_neg:   79.681 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:45:56,171 - 3:36:39 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    50 rec:    9.512 mi: 1.94905114 zkl:  170.601 cd:  -32.874 pos_prob:   48.341 prob_neg:   81.215 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    50 rec:    9.512 mi: 1.94905114 zkl:  170.601 cd:  -32.874 pos_prob:   48.341 prob_neg:   81.215 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:45:56,183 - 3:36:39 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-260.408
Langevin prior   1/ 40: energy=-260.408
2022-12-28 20:45:56,189 - 3:36:39 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1122.645
Langevin prior   6/ 40: energy=-1122.645
2022-12-28 20:45:56,193 - 3:36:39 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1746.575
Langevin prior  11/ 40: energy=-1746.575
2022-12-28 20:45:56,199 - 3:36:39 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2096.038
Langevin prior  16/ 40: energy=-2096.038
2022-12-28 20:45:56,204 - 3:36:39 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2265.810
Langevin prior  21/ 40: energy=-2265.810
2022-12-28 20:45:56,210 - 3:36:39 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2368.206
Langevin prior  26/ 40: energy=-2368.206
2022-12-28 20:45:56,217 - 3:36:39 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2397.060
Langevin prior  31/ 40: energy=-2397.060
2022-12-28 20:45:56,225 - 3:36:39 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2407.865
Langevin prior  36/ 40: energy=-2407.865
2022-12-28 20:45:56,233 - 3:36:39 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2456.503
Langevin prior  40/ 40: energy=-2456.503
2022-12-28 20:45:58,870 - 3:36:42 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-050-test-greedy.txt
Generation: 188 batches
2022-12-28 20:46:04,510 - 3:36:47 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:46:05,998 - 3:36:49 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:46:05,999 - 3:36:49 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:46:08,408 - 3:36:51 - 2.4s - INFO - root - --- bleu: BLEU = 31.34, 55.8/34.3/28.1/24.9 (BP=0.922, ratio=0.925, hyp_len=156980, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.34, 55.8/34.3/28.1/24.9 (BP=0.922, ratio=0.925, hyp_len=156980, ref_len=169777)
--- bleu: BLEU = 31.34, 55.8/34.3/28.1/24.9 (BP=0.922, ratio=0.925, hyp_len=156980, ref_len=169777)

2022-12-28 20:46:08,408 - 3:36:51 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:46:08,416 - 3:36:51 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:46:17,475 - 3:37:00 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    51 rec:   11.893 mi: 2.20693851 zkl:  168.960 cd:  -39.443 pos_prob:   43.185 prob_neg:   82.628 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    51 rec:   11.893 mi: 2.20693851 zkl:  168.960 cd:  -39.443 pos_prob:   43.185 prob_neg:   82.628 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:46:26,680 - 3:37:09 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    51 rec:   10.985 mi: 2.06240964 zkl:  170.557 cd:  -37.653 pos_prob:   43.727 prob_neg:   81.380 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    51 rec:   10.985 mi: 2.06240964 zkl:  170.557 cd:  -37.653 pos_prob:   43.727 prob_neg:   81.380 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:46:35,865 - 3:37:19 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    51 rec:   10.997 mi: 2.15576744 zkl:  170.342 cd:  -37.674 pos_prob:   39.129 prob_neg:   76.803 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    51 rec:   10.997 mi: 2.15576744 zkl:  170.342 cd:  -37.674 pos_prob:   39.129 prob_neg:   76.803 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:46:45,052 - 3:37:28 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    51 rec:   13.820 mi: 1.89520979 zkl:  173.152 cd:  -35.795 pos_prob:   42.646 prob_neg:   78.442 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    51 rec:   13.820 mi: 1.89520979 zkl:  173.152 cd:  -35.795 pos_prob:   42.646 prob_neg:   78.442 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:46:54,163 - 3:37:37 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    51 rec:   14.236 mi: 2.12226129 zkl:  169.730 cd:  -44.006 pos_prob:   39.784 prob_neg:   83.790 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    51 rec:   14.236 mi: 2.12226129 zkl:  169.730 cd:  -44.006 pos_prob:   39.784 prob_neg:   83.790 kl_weight:    0.000 do_ae_train: True
2022-12-28 20:46:54,176 - 3:37:37 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-303.530
Langevin prior   1/ 40: energy=-303.530
2022-12-28 20:46:54,181 - 3:37:37 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1248.187
Langevin prior   6/ 40: energy=-1248.187
2022-12-28 20:46:54,187 - 3:37:37 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1880.280
Langevin prior  11/ 40: energy=-1880.280
2022-12-28 20:46:54,197 - 3:37:37 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2094.597
Langevin prior  16/ 40: energy=-2094.597
2022-12-28 20:46:54,208 - 3:37:37 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2204.104
Langevin prior  21/ 40: energy=-2204.104
2022-12-28 20:46:54,219 - 3:37:37 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2324.777
Langevin prior  26/ 40: energy=-2324.777
2022-12-28 20:46:54,230 - 3:37:37 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2333.631
Langevin prior  31/ 40: energy=-2333.631
2022-12-28 20:46:54,242 - 3:37:37 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2375.821
Langevin prior  36/ 40: energy=-2375.821
2022-12-28 20:46:54,250 - 3:37:37 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2331.242
Langevin prior  40/ 40: energy=-2331.242
2022-12-28 20:46:56,755 - 3:37:39 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-051-test-greedy.txt
Generation: 188 batches
2022-12-28 20:47:02,401 - 3:37:45 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:47:03,885 - 3:37:47 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:47:03,885 - 3:37:47 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:47:06,302 - 3:37:49 - 2.4s - INFO - root - --- bleu: BLEU = 31.30, 55.5/34.0/27.9/24.8 (BP=0.925, ratio=0.928, hyp_len=157508, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.30, 55.5/34.0/27.9/24.8 (BP=0.925, ratio=0.928, hyp_len=157508, ref_len=169777)
--- bleu: BLEU = 31.30, 55.5/34.0/27.9/24.8 (BP=0.925, ratio=0.928, hyp_len=157508, ref_len=169777)

2022-12-28 20:47:06,302 - 3:37:49 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:47:06,310 - 3:37:49 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:47:15,613 - 3:37:58 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    52 rec:   10.786 mi: 2.23230195 zkl:  101.720 cd:  -12.964 pos_prob:   69.396 prob_neg:   82.360 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    52 rec:   10.786 mi: 2.23230195 zkl:  101.720 cd:  -12.964 pos_prob:   69.396 prob_neg:   82.360 kl_weight:    0.062 do_ae_train: False
2022-12-28 20:47:25,061 - 3:38:08 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    52 rec:   12.992 mi: 2.25297332 zkl:   96.103 cd:   -8.402 pos_prob:   72.748 prob_neg:   81.149 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    52 rec:   12.992 mi: 2.25297332 zkl:   96.103 cd:   -8.402 pos_prob:   72.748 prob_neg:   81.149 kl_weight:    0.125 do_ae_train: False
2022-12-28 20:47:34,349 - 3:38:17 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    52 rec:   14.325 mi: 2.11849308 zkl:   88.011 cd:   -3.515 pos_prob:   81.184 prob_neg:   84.699 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    52 rec:   14.325 mi: 2.11849308 zkl:   88.011 cd:   -3.515 pos_prob:   81.184 prob_neg:   84.699 kl_weight:    0.188 do_ae_train: False
2022-12-28 20:47:43,674 - 3:38:26 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    52 rec:   10.575 mi: 2.19175267 zkl:   84.584 cd:   -4.292 pos_prob:   79.965 prob_neg:   84.257 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    52 rec:   10.575 mi: 2.19175267 zkl:   84.584 cd:   -4.292 pos_prob:   79.965 prob_neg:   84.257 kl_weight:    0.251 do_ae_train: False
2022-12-28 20:47:53,016 - 3:38:36 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    52 rec:   14.455 mi: 2.24740958 zkl:   78.506 cd:   -0.910 pos_prob:   79.528 prob_neg:   80.438 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    52 rec:   14.455 mi: 2.24740958 zkl:   78.506 cd:   -0.910 pos_prob:   79.528 prob_neg:   80.438 kl_weight:    0.314 do_ae_train: False
2022-12-28 20:47:53,029 - 3:38:36 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-344.614
Langevin prior   1/ 40: energy=-344.614
2022-12-28 20:47:53,034 - 3:38:36 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1397.340
Langevin prior   6/ 40: energy=-1397.340
2022-12-28 20:47:53,040 - 3:38:36 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1999.488
Langevin prior  11/ 40: energy=-1999.488
2022-12-28 20:47:53,045 - 3:38:36 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2291.956
Langevin prior  16/ 40: energy=-2291.956
2022-12-28 20:47:53,052 - 3:38:36 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2411.649
Langevin prior  21/ 40: energy=-2411.649
2022-12-28 20:47:53,059 - 3:38:36 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2404.298
Langevin prior  26/ 40: energy=-2404.298
2022-12-28 20:47:53,067 - 3:38:36 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2491.153
Langevin prior  31/ 40: energy=-2491.153
2022-12-28 20:47:53,076 - 3:38:36 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2554.554
Langevin prior  36/ 40: energy=-2554.554
2022-12-28 20:47:53,084 - 3:38:36 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2509.683
Langevin prior  40/ 40: energy=-2509.683
2022-12-28 20:47:55,738 - 3:38:38 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-052-test-greedy.txt
Generation: 188 batches
2022-12-28 20:48:01,398 - 3:38:44 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:48:02,904 - 3:38:46 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:48:02,904 - 3:38:46 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:48:05,320 - 3:38:48 - 2.4s - INFO - root - --- bleu: BLEU = 29.63, 53.0/31.7/25.8/22.9 (BP=0.939, ratio=0.941, hyp_len=159706, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.63, 53.0/31.7/25.8/22.9 (BP=0.939, ratio=0.941, hyp_len=159706, ref_len=169777)
--- bleu: BLEU = 29.63, 53.0/31.7/25.8/22.9 (BP=0.939, ratio=0.941, hyp_len=159706, ref_len=169777)

2022-12-28 20:48:05,320 - 3:38:48 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:48:05,328 - 3:38:48 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:48:14,686 - 3:38:57 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    53 rec:   19.736 mi: 2.15803242 zkl:   72.507 cd:   -5.976 pos_prob:   82.727 prob_neg:   88.703 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    53 rec:   19.736 mi: 2.15803242 zkl:   72.507 cd:   -5.976 pos_prob:   82.727 prob_neg:   88.703 kl_weight:    0.396 do_ae_train: False
2022-12-28 20:48:23,948 - 3:39:07 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    53 rec:   12.782 mi: 2.20261574 zkl:   72.507 cd:   -0.528 pos_prob:   82.444 prob_neg:   82.972 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    53 rec:   12.782 mi: 2.20261574 zkl:   72.507 cd:   -0.528 pos_prob:   82.444 prob_neg:   82.972 kl_weight:    0.459 do_ae_train: False
2022-12-28 20:48:33,305 - 3:39:16 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    53 rec:   16.865 mi: 2.30665135 zkl:   68.479 cd:   -6.599 pos_prob:   83.238 prob_neg:   89.837 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    53 rec:   16.865 mi: 2.30665135 zkl:   68.479 cd:   -6.599 pos_prob:   83.238 prob_neg:   89.837 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:48:42,597 - 3:39:25 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    53 rec:   16.644 mi: 2.06542754 zkl:   84.218 cd:    7.462 pos_prob:   94.831 prob_neg:   87.369 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    53 rec:   16.644 mi: 2.06542754 zkl:   84.218 cd:    7.462 pos_prob:   94.831 prob_neg:   87.369 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:48:52,039 - 3:39:35 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    53 rec:   16.726 mi: 2.29607105 zkl:   70.440 cd:   -0.815 pos_prob:   85.108 prob_neg:   85.923 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    53 rec:   16.726 mi: 2.29607105 zkl:   70.440 cd:   -0.815 pos_prob:   85.108 prob_neg:   85.923 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:48:52,051 - 3:39:35 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-321.813
Langevin prior   1/ 40: energy=-321.813
2022-12-28 20:48:52,057 - 3:39:35 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1313.555
Langevin prior   6/ 40: energy=-1313.555
2022-12-28 20:48:52,062 - 3:39:35 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1969.426
Langevin prior  11/ 40: energy=-1969.426
2022-12-28 20:48:52,068 - 3:39:35 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2330.473
Langevin prior  16/ 40: energy=-2330.473
2022-12-28 20:48:52,073 - 3:39:35 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2479.925
Langevin prior  21/ 40: energy=-2479.925
2022-12-28 20:48:52,079 - 3:39:35 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2558.043
Langevin prior  26/ 40: energy=-2558.043
2022-12-28 20:48:52,086 - 3:39:35 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2674.687
Langevin prior  31/ 40: energy=-2674.687
2022-12-28 20:48:52,093 - 3:39:35 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2594.520
Langevin prior  36/ 40: energy=-2594.520
2022-12-28 20:48:52,100 - 3:39:35 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2675.436
Langevin prior  40/ 40: energy=-2675.436
2022-12-28 20:58:37,875 - 3:49:21 - 585.8s - INFO - root - Negative Log-likehood -183.709757
200 done. -160.49779215345458
400 done. -154.45784134256817
600 done. -153.80848750495036
800 done. -154.492538639654
1000 done. -153.41503682242188
1200 done. -154.1362168063985
1400 done. -155.63296071733248
1600 done. -153.73345495846172
1800 done. -153.2208173456282
2000 done. -171.05980065105155
2200 done. -182.89560250332835
2400 done. -195.67508018529503
2600 done. -206.51808467790337
2800 done. -213.6870638538631
3000 done. -222.91600988501725
3200 done. -228.81545685477093
3400 done. -235.83206512843674
3600 done. -240.52749244786332
3800 done. -246.23696041869826
4000 done. -250.30900684779868
4200 done. -240.38801009757344
4400 done. -230.2166731767046
4600 done. -221.15996431271844
4800 done. -212.77688243630473
5000 done. -205.25121421122262
5200 done. -198.19494291490096
5400 done. -191.72229667637646
5600 done. -185.71616933639208
Negative Log-likehood -183.709757
2022-12-28 20:58:37,875 - 3:49:21 - 0.0s - INFO - root - log-likelihood:   -183.710
log-likelihood:   -183.710
2022-12-28 20:59:24,191 - 3:50:07 - 46.3s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-053-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 20:59:24,193 - 3:50:07 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-053-test-greedy.txt
Generation: 188 batches
2022-12-28 20:59:29,819 - 3:50:13 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:59:31,341 - 3:50:14 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 20:59:31,341 - 3:50:14 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 20:59:33,799 - 3:50:16 - 2.5s - INFO - root - --- bleu: BLEU = 29.64, 51.9/31.3/25.6/22.7 (BP=0.951, ratio=0.952, hyp_len=161615, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.64, 51.9/31.3/25.6/22.7 (BP=0.951, ratio=0.952, hyp_len=161615, ref_len=169777)
--- bleu: BLEU = 29.64, 51.9/31.3/25.6/22.7 (BP=0.951, ratio=0.952, hyp_len=161615, ref_len=169777)

2022-12-28 20:59:33,801 - 3:50:16 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 20:59:33,820 - 3:50:17 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 20:59:43,072 - 3:50:26 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    54 rec:   14.166 mi: 2.46332717 zkl:   74.862 cd:    1.111 pos_prob:   91.774 prob_neg:   90.663 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    54 rec:   14.166 mi: 2.46332717 zkl:   74.862 cd:    1.111 pos_prob:   91.774 prob_neg:   90.663 kl_weight:    0.500 do_ae_train: False
2022-12-28 20:59:52,278 - 3:50:35 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    54 rec:   17.083 mi: 2.25135159 zkl:   74.507 cd:    2.951 pos_prob:   91.779 prob_neg:   88.828 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    54 rec:   17.083 mi: 2.25135159 zkl:   74.507 cd:    2.951 pos_prob:   91.779 prob_neg:   88.828 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:00:01,893 - 3:50:45 - 9.6s - INFO - root - batch/max_batch/ep:   300/   529/    54 rec:   19.078 mi: 2.39445257 zkl:   74.542 cd:   -5.228 pos_prob:   90.915 prob_neg:   96.143 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    54 rec:   19.078 mi: 2.39445257 zkl:   74.542 cd:   -5.228 pos_prob:   90.915 prob_neg:   96.143 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:00:11,319 - 3:50:54 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    54 rec:   12.564 mi: 2.36866593 zkl:   77.009 cd:    7.392 pos_prob:   95.750 prob_neg:   88.358 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    54 rec:   12.564 mi: 2.36866593 zkl:   77.009 cd:    7.392 pos_prob:   95.750 prob_neg:   88.358 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:00:20,596 - 3:51:03 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    54 rec:   17.940 mi: 2.32694888 zkl:   72.363 cd:    1.845 pos_prob:   90.281 prob_neg:   88.436 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    54 rec:   17.940 mi: 2.32694888 zkl:   72.363 cd:    1.845 pos_prob:   90.281 prob_neg:   88.436 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:00:20,608 - 3:51:03 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-235.199
Langevin prior   1/ 40: energy=-235.199
2022-12-28 21:00:20,614 - 3:51:03 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1408.704
Langevin prior   6/ 40: energy=-1408.704
2022-12-28 21:00:20,619 - 3:51:03 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2088.314
Langevin prior  11/ 40: energy=-2088.314
2022-12-28 21:00:20,624 - 3:51:03 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2440.686
Langevin prior  16/ 40: energy=-2440.686
2022-12-28 21:00:20,629 - 3:51:03 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2525.667
Langevin prior  21/ 40: energy=-2525.667
2022-12-28 21:00:20,636 - 3:51:03 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2703.932
Langevin prior  26/ 40: energy=-2703.932
2022-12-28 21:00:20,643 - 3:51:03 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2799.828
Langevin prior  31/ 40: energy=-2799.828
2022-12-28 21:00:20,652 - 3:51:03 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2829.244
Langevin prior  36/ 40: energy=-2829.244
2022-12-28 21:00:20,659 - 3:51:03 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2799.862
Langevin prior  40/ 40: energy=-2799.862
2022-12-28 21:10:06,312 - 4:00:49 - 585.7s - INFO - root - Negative Log-likehood -183.067826
200 done. -159.80382229687945
400 done. -153.21576704188178
600 done. -152.55476451621755
800 done. -153.18571362974455
1000 done. -151.91994835523147
1200 done. -152.60714153218763
1400 done. -154.06496622185588
1600 done. -152.2177432792251
1800 done. -151.67883137878206
2000 done. -169.570416762342
2200 done. -181.5284434478006
2400 done. -194.317171649098
2600 done. -205.428859350983
2800 done. -212.97199161864532
3000 done. -222.27132867433238
3200 done. -228.30636603694015
3400 done. -235.36859565704142
3600 done. -240.16884904948174
3800 done. -245.91309536447267
4000 done. -249.96651549042025
4200 done. -240.0252576814222
4400 done. -229.82321420781304
4600 done. -220.71528593427593
4800 done. -212.27334107366084
5000 done. -204.7129639547654
5200 done. -197.61805064667527
5400 done. -191.1303780996622
5600 done. -185.09151345235222
Negative Log-likehood -183.067826
2022-12-28 21:10:06,312 - 4:00:49 - 0.0s - INFO - root - log-likelihood:   -183.068
log-likelihood:   -183.068
2022-12-28 21:10:52,812 - 4:01:36 - 46.5s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-054-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 21:10:52,814 - 4:01:36 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-054-test-greedy.txt
Generation: 188 batches
2022-12-28 21:10:58,484 - 4:01:41 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:11:00,018 - 4:01:43 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 21:11:00,018 - 4:01:43 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:11:02,800 - 4:01:45 - 2.8s - INFO - root - --- bleu: BLEU = 29.56, 51.4/30.8/25.1/22.2 (BP=0.963, ratio=0.964, hyp_len=163662, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.56, 51.4/30.8/25.1/22.2 (BP=0.963, ratio=0.964, hyp_len=163662, ref_len=169777)
--- bleu: BLEU = 29.56, 51.4/30.8/25.1/22.2 (BP=0.963, ratio=0.964, hyp_len=163662, ref_len=169777)

2022-12-28 21:11:02,800 - 4:01:45 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:11:02,808 - 4:01:46 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:11:11,868 - 4:01:55 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    55 rec:   10.179 mi: 2.29479957 zkl:  150.486 cd:  -18.228 pos_prob:   75.999 prob_neg:   94.227 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    55 rec:   10.179 mi: 2.29479957 zkl:  150.486 cd:  -18.228 pos_prob:   75.999 prob_neg:   94.227 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:11:21,122 - 4:02:04 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    55 rec:   10.504 mi: 2.30139017 zkl:  167.954 cd:  -29.156 pos_prob:   64.158 prob_neg:   93.314 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    55 rec:   10.504 mi: 2.30139017 zkl:  167.954 cd:  -29.156 pos_prob:   64.158 prob_neg:   93.314 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:11:30,340 - 4:02:13 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    55 rec:   12.339 mi: 2.22300696 zkl:  174.505 cd:  -29.023 pos_prob:   62.634 prob_neg:   91.657 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    55 rec:   12.339 mi: 2.22300696 zkl:  174.505 cd:  -29.023 pos_prob:   62.634 prob_neg:   91.657 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:11:39,477 - 4:02:22 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    55 rec:   12.778 mi: 2.31773138 zkl:  171.474 cd:  -42.401 pos_prob:   52.372 prob_neg:   94.773 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    55 rec:   12.778 mi: 2.31773138 zkl:  171.474 cd:  -42.401 pos_prob:   52.372 prob_neg:   94.773 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:11:48,492 - 4:02:31 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/    55 rec:    8.022 mi: 2.24908447 zkl:  173.640 cd:  -43.448 pos_prob:   53.760 prob_neg:   97.208 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    55 rec:    8.022 mi: 2.24908447 zkl:  173.640 cd:  -43.448 pos_prob:   53.760 prob_neg:   97.208 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:11:48,505 - 4:02:31 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-355.116
Langevin prior   1/ 40: energy=-355.116
2022-12-28 21:11:48,510 - 4:02:31 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1508.575
Langevin prior   6/ 40: energy=-1508.575
2022-12-28 21:11:48,516 - 4:02:31 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2166.682
Langevin prior  11/ 40: energy=-2166.682
2022-12-28 21:11:48,521 - 4:02:31 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2494.649
Langevin prior  16/ 40: energy=-2494.649
2022-12-28 21:11:48,527 - 4:02:31 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2623.227
Langevin prior  21/ 40: energy=-2623.227
2022-12-28 21:11:48,533 - 4:02:31 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2760.008
Langevin prior  26/ 40: energy=-2760.008
2022-12-28 21:11:48,541 - 4:02:31 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2731.591
Langevin prior  31/ 40: energy=-2731.591
2022-12-28 21:11:48,550 - 4:02:31 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2852.490
Langevin prior  36/ 40: energy=-2852.490
2022-12-28 21:11:48,558 - 4:02:31 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2863.618
Langevin prior  40/ 40: energy=-2863.618
2022-12-28 21:11:51,200 - 4:02:34 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-055-test-greedy.txt
Generation: 188 batches
2022-12-28 21:11:56,882 - 4:02:40 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:11:58,377 - 4:02:41 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 21:11:58,377 - 4:02:41 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:12:00,832 - 4:02:44 - 2.5s - INFO - root - --- bleu: BLEU = 30.59, 54.2/32.8/26.8/23.7 (BP=0.938, ratio=0.940, hyp_len=159628, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.59, 54.2/32.8/26.8/23.7 (BP=0.938, ratio=0.940, hyp_len=159628, ref_len=169777)
--- bleu: BLEU = 30.59, 54.2/32.8/26.8/23.7 (BP=0.938, ratio=0.940, hyp_len=159628, ref_len=169777)

2022-12-28 21:12:00,833 - 4:02:44 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:12:00,841 - 4:02:44 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:12:09,932 - 4:02:53 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    56 rec:   12.068 mi: 2.35409188 zkl:  172.927 cd:  -42.344 pos_prob:   51.729 prob_neg:   94.074 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    56 rec:   12.068 mi: 2.35409188 zkl:  172.927 cd:  -42.344 pos_prob:   51.729 prob_neg:   94.074 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:12:19,073 - 4:03:02 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    56 rec:   12.195 mi: 2.18184614 zkl:  171.774 cd:  -46.958 pos_prob:   46.482 prob_neg:   93.441 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    56 rec:   12.195 mi: 2.18184614 zkl:  171.774 cd:  -46.958 pos_prob:   46.482 prob_neg:   93.441 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:12:28,102 - 4:03:11 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    56 rec:   11.109 mi: 2.31599760 zkl:  172.714 cd:  -44.503 pos_prob:   46.502 prob_neg:   91.005 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    56 rec:   11.109 mi: 2.31599760 zkl:  172.714 cd:  -44.503 pos_prob:   46.502 prob_neg:   91.005 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:12:37,381 - 4:03:20 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    56 rec:   16.387 mi: 2.34183669 zkl:  177.232 cd:  -43.763 pos_prob:   47.303 prob_neg:   91.066 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    56 rec:   16.387 mi: 2.34183669 zkl:  177.232 cd:  -43.763 pos_prob:   47.303 prob_neg:   91.066 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:12:46,619 - 4:03:29 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    56 rec:    9.815 mi: 2.07296157 zkl:  172.160 cd:  -49.246 pos_prob:   47.026 prob_neg:   96.272 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    56 rec:    9.815 mi: 2.07296157 zkl:  172.160 cd:  -49.246 pos_prob:   47.026 prob_neg:   96.272 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:12:46,632 - 4:03:29 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-361.811
Langevin prior   1/ 40: energy=-361.811
2022-12-28 21:12:46,638 - 4:03:29 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1504.476
Langevin prior   6/ 40: energy=-1504.476
2022-12-28 21:12:46,643 - 4:03:29 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2094.689
Langevin prior  11/ 40: energy=-2094.689
2022-12-28 21:12:46,649 - 4:03:29 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2405.532
Langevin prior  16/ 40: energy=-2405.532
2022-12-28 21:12:46,655 - 4:03:29 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2545.613
Langevin prior  21/ 40: energy=-2545.613
2022-12-28 21:12:46,660 - 4:03:29 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2650.046
Langevin prior  26/ 40: energy=-2650.046
2022-12-28 21:12:46,667 - 4:03:29 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2795.014
Langevin prior  31/ 40: energy=-2795.014
2022-12-28 21:12:46,676 - 4:03:29 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2754.907
Langevin prior  36/ 40: energy=-2754.907
2022-12-28 21:12:46,683 - 4:03:29 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2752.791
Langevin prior  40/ 40: energy=-2752.791
2022-12-28 21:12:49,256 - 4:03:32 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-056-test-greedy.txt
Generation: 188 batches
2022-12-28 21:12:54,911 - 4:03:38 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:12:56,401 - 4:03:39 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 21:12:56,401 - 4:03:39 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:12:58,804 - 4:03:42 - 2.4s - INFO - root - --- bleu: BLEU = 31.23, 54.6/33.5/27.5/24.5 (BP=0.937, ratio=0.939, hyp_len=159433, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.23, 54.6/33.5/27.5/24.5 (BP=0.937, ratio=0.939, hyp_len=159433, ref_len=169777)
--- bleu: BLEU = 31.23, 54.6/33.5/27.5/24.5 (BP=0.937, ratio=0.939, hyp_len=159433, ref_len=169777)

2022-12-28 21:12:58,805 - 4:03:42 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:12:58,813 - 4:03:42 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:13:07,898 - 4:03:51 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    57 rec:    9.979 mi: 2.12551999 zkl:  181.036 cd:  -52.412 pos_prob:   48.239 prob_neg:  100.651 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    57 rec:    9.979 mi: 2.12551999 zkl:  181.036 cd:  -52.412 pos_prob:   48.239 prob_neg:  100.651 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:13:17,052 - 4:04:00 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    57 rec:   11.251 mi: 2.16096568 zkl:  174.669 cd:  -47.406 pos_prob:   44.199 prob_neg:   91.605 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    57 rec:   11.251 mi: 2.16096568 zkl:  174.669 cd:  -47.406 pos_prob:   44.199 prob_neg:   91.605 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:13:26,141 - 4:04:09 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    57 rec:   11.305 mi: 2.22925711 zkl:  176.103 cd:  -51.426 pos_prob:   40.647 prob_neg:   92.072 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    57 rec:   11.305 mi: 2.22925711 zkl:  176.103 cd:  -51.426 pos_prob:   40.647 prob_neg:   92.072 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:13:35,371 - 4:04:18 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    57 rec:    9.657 mi: 2.17852759 zkl:  178.462 cd:  -43.331 pos_prob:   46.980 prob_neg:   90.311 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    57 rec:    9.657 mi: 2.17852759 zkl:  178.462 cd:  -43.331 pos_prob:   46.980 prob_neg:   90.311 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:13:44,590 - 4:04:27 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    57 rec:    9.563 mi: 2.10248995 zkl:  176.014 cd:  -49.010 pos_prob:   46.190 prob_neg:   95.200 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    57 rec:    9.563 mi: 2.10248995 zkl:  176.014 cd:  -49.010 pos_prob:   46.190 prob_neg:   95.200 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:13:44,602 - 4:04:27 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-322.480
Langevin prior   1/ 40: energy=-322.480
2022-12-28 21:13:44,607 - 4:04:27 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1375.193
Langevin prior   6/ 40: energy=-1375.193
2022-12-28 21:13:44,612 - 4:04:27 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2114.673
Langevin prior  11/ 40: energy=-2114.673
2022-12-28 21:13:44,618 - 4:04:27 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2464.508
Langevin prior  16/ 40: energy=-2464.508
2022-12-28 21:13:44,623 - 4:04:27 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2587.263
Langevin prior  21/ 40: energy=-2587.263
2022-12-28 21:13:44,629 - 4:04:27 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2749.211
Langevin prior  26/ 40: energy=-2749.211
2022-12-28 21:13:44,636 - 4:04:27 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2838.409
Langevin prior  31/ 40: energy=-2838.409
2022-12-28 21:13:44,644 - 4:04:27 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2837.824
Langevin prior  36/ 40: energy=-2837.824
2022-12-28 21:13:44,651 - 4:04:27 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2792.799
Langevin prior  40/ 40: energy=-2792.799
2022-12-28 21:13:47,251 - 4:04:30 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-057-test-greedy.txt
Generation: 188 batches
2022-12-28 21:13:52,914 - 4:04:36 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:13:54,423 - 4:04:37 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 21:13:54,423 - 4:04:37 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:13:56,855 - 4:04:40 - 2.4s - INFO - root - --- bleu: BLEU = 31.59, 54.3/33.2/27.2/24.2 (BP=0.957, ratio=0.958, hyp_len=162632, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.59, 54.3/33.2/27.2/24.2 (BP=0.957, ratio=0.958, hyp_len=162632, ref_len=169777)
--- bleu: BLEU = 31.59, 54.3/33.2/27.2/24.2 (BP=0.957, ratio=0.958, hyp_len=162632, ref_len=169777)

2022-12-28 21:13:56,856 - 4:04:40 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:13:56,864 - 4:04:40 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:14:06,333 - 4:04:49 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    58 rec:    7.974 mi: 2.26944113 zkl:  100.808 cd:  -24.377 pos_prob:   80.193 prob_neg:  104.570 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    58 rec:    7.974 mi: 2.26944113 zkl:  100.808 cd:  -24.377 pos_prob:   80.193 prob_neg:  104.570 kl_weight:    0.062 do_ae_train: False
2022-12-28 21:14:15,751 - 4:04:58 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    58 rec:   10.064 mi: 2.07677650 zkl:   93.276 cd:   -8.799 pos_prob:   86.041 prob_neg:   94.840 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    58 rec:   10.064 mi: 2.07677650 zkl:   93.276 cd:   -8.799 pos_prob:   86.041 prob_neg:   94.840 kl_weight:    0.125 do_ae_train: False
2022-12-28 21:14:25,144 - 4:05:08 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    58 rec:   11.085 mi: 2.20845127 zkl:   91.952 cd:  -10.810 pos_prob:   90.226 prob_neg:  101.036 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    58 rec:   11.085 mi: 2.20845127 zkl:   91.952 cd:  -10.810 pos_prob:   90.226 prob_neg:  101.036 kl_weight:    0.188 do_ae_train: False
2022-12-28 21:14:34,489 - 4:05:17 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    58 rec:   14.077 mi: 2.42093968 zkl:   93.045 cd:    4.243 pos_prob:   96.612 prob_neg:   92.369 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    58 rec:   14.077 mi: 2.42093968 zkl:   93.045 cd:    4.243 pos_prob:   96.612 prob_neg:   92.369 kl_weight:    0.251 do_ae_train: False
2022-12-28 21:14:43,866 - 4:05:27 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    58 rec:   12.227 mi: 2.41279697 zkl:   76.295 cd:   -7.215 pos_prob:   86.620 prob_neg:   93.835 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    58 rec:   12.227 mi: 2.41279697 zkl:   76.295 cd:   -7.215 pos_prob:   86.620 prob_neg:   93.835 kl_weight:    0.314 do_ae_train: False
2022-12-28 21:14:43,878 - 4:05:27 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-381.163
Langevin prior   1/ 40: energy=-381.163
2022-12-28 21:14:43,884 - 4:05:27 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1658.409
Langevin prior   6/ 40: energy=-1658.409
2022-12-28 21:14:43,889 - 4:05:27 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2317.485
Langevin prior  11/ 40: energy=-2317.485
2022-12-28 21:14:43,894 - 4:05:27 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2687.879
Langevin prior  16/ 40: energy=-2687.879
2022-12-28 21:14:43,900 - 4:05:27 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2801.850
Langevin prior  21/ 40: energy=-2801.850
2022-12-28 21:14:43,906 - 4:05:27 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2879.154
Langevin prior  26/ 40: energy=-2879.154
2022-12-28 21:14:43,912 - 4:05:27 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2911.252
Langevin prior  31/ 40: energy=-2911.252
2022-12-28 21:14:43,921 - 4:05:27 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2866.734
Langevin prior  36/ 40: energy=-2866.734
2022-12-28 21:14:43,928 - 4:05:27 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2948.357
Langevin prior  40/ 40: energy=-2948.357
2022-12-28 21:14:46,617 - 4:05:29 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-058-test-greedy.txt
Generation: 188 batches
2022-12-28 21:14:52,262 - 4:05:35 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:14:53,748 - 4:05:36 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 21:14:53,748 - 4:05:36 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:14:56,131 - 4:05:39 - 2.4s - INFO - root - --- bleu: BLEU = 29.91, 54.0/32.8/26.9/23.9 (BP=0.916, ratio=0.919, hyp_len=156036, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.91, 54.0/32.8/26.9/23.9 (BP=0.916, ratio=0.919, hyp_len=156036, ref_len=169777)
--- bleu: BLEU = 29.91, 54.0/32.8/26.9/23.9 (BP=0.916, ratio=0.919, hyp_len=156036, ref_len=169777)

2022-12-28 21:14:56,131 - 4:05:39 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:14:56,139 - 4:05:39 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:15:05,536 - 4:05:48 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    59 rec:   16.813 mi: 2.40987802 zkl:   81.865 cd:   -1.498 pos_prob:   92.978 prob_neg:   94.475 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    59 rec:   16.813 mi: 2.40987802 zkl:   81.865 cd:   -1.498 pos_prob:   92.978 prob_neg:   94.475 kl_weight:    0.396 do_ae_train: False
2022-12-28 21:15:14,906 - 4:05:58 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    59 rec:   15.567 mi: 2.37285042 zkl:   86.239 cd:   -1.630 pos_prob:   97.448 prob_neg:   99.079 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    59 rec:   15.567 mi: 2.37285042 zkl:   86.239 cd:   -1.630 pos_prob:   97.448 prob_neg:   99.079 kl_weight:    0.459 do_ae_train: False
2022-12-28 21:15:24,223 - 4:06:07 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    59 rec:   15.550 mi: 2.35783005 zkl:   76.739 cd:   -6.181 pos_prob:   94.326 prob_neg:  100.507 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    59 rec:   15.550 mi: 2.35783005 zkl:   76.739 cd:   -6.181 pos_prob:   94.326 prob_neg:  100.507 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:15:33,598 - 4:06:16 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    59 rec:   13.723 mi: 2.33952093 zkl:   75.165 cd:   -4.559 pos_prob:   92.554 prob_neg:   97.113 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    59 rec:   13.723 mi: 2.33952093 zkl:   75.165 cd:   -4.559 pos_prob:   92.554 prob_neg:   97.113 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:15:43,030 - 4:06:26 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    59 rec:   15.429 mi: 2.30513048 zkl:   79.087 cd:   -2.499 pos_prob:   96.873 prob_neg:   99.372 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    59 rec:   15.429 mi: 2.30513048 zkl:   79.087 cd:   -2.499 pos_prob:   96.873 prob_neg:   99.372 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:15:43,042 - 4:06:26 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-361.111
Langevin prior   1/ 40: energy=-361.111
2022-12-28 21:15:43,048 - 4:06:26 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1696.456
Langevin prior   6/ 40: energy=-1696.456
2022-12-28 21:15:43,053 - 4:06:26 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2377.753
Langevin prior  11/ 40: energy=-2377.753
2022-12-28 21:15:43,058 - 4:06:26 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2764.994
Langevin prior  16/ 40: energy=-2764.994
2022-12-28 21:15:43,064 - 4:06:26 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2935.248
Langevin prior  21/ 40: energy=-2935.248
2022-12-28 21:15:43,072 - 4:06:26 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3048.522
Langevin prior  26/ 40: energy=-3048.522
2022-12-28 21:15:43,081 - 4:06:26 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3071.876
Langevin prior  31/ 40: energy=-3071.876
2022-12-28 21:15:43,090 - 4:06:26 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3041.786
Langevin prior  36/ 40: energy=-3041.786
2022-12-28 21:15:43,097 - 4:06:26 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2995.076
Langevin prior  40/ 40: energy=-2995.076
2022-12-28 21:25:28,854 - 4:16:12 - 585.8s - INFO - root - Negative Log-likehood -188.470617
200 done. -164.4720140268603
400 done. -157.85459061715443
600 done. -157.13087633538112
800 done. -157.88639554841984
1000 done. -156.71684171572946
1200 done. -157.42331662868304
1400 done. -158.8916511579482
1600 done. -157.01683360919552
1800 done. -156.4490337309177
2000 done. -174.9230747792062
2200 done. -187.22288864047206
2400 done. -200.38524447637982
2600 done. -211.70645579372086
2800 done. -219.34311736566988
3000 done. -228.801760415047
3200 done. -234.88403130643047
3400 done. -242.22041354473197
3600 done. -247.16472304607095
3800 done. -253.11311045613547
4000 done. -257.13273463486905
4200 done. -246.90613231181612
4400 done. -236.42737909191922
4600 done. -227.0884005182763
4800 done. -218.43085274086746
5000 done. -210.6582356196597
5200 done. -203.3786129469352
5400 done. -196.71885565965
5600 done. -190.53962650178283
Negative Log-likehood -188.470617
2022-12-28 21:25:28,854 - 4:16:12 - 0.0s - INFO - root - log-likelihood:   -188.471
log-likelihood:   -188.471
2022-12-28 21:26:15,885 - 4:16:59 - 47.0s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-059-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 21:26:15,887 - 4:16:59 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-059-test-greedy.txt
Generation: 188 batches
2022-12-28 21:26:21,519 - 4:17:04 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:26:23,025 - 4:17:06 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 21:26:23,025 - 4:17:06 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:26:25,489 - 4:17:08 - 2.5s - INFO - root - --- bleu: BLEU = 29.50, 52.7/31.8/26.1/23.2 (BP=0.930, ratio=0.933, hyp_len=158334, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.50, 52.7/31.8/26.1/23.2 (BP=0.930, ratio=0.933, hyp_len=158334, ref_len=169777)
--- bleu: BLEU = 29.50, 52.7/31.8/26.1/23.2 (BP=0.930, ratio=0.933, hyp_len=158334, ref_len=169777)

2022-12-28 21:26:25,491 - 4:17:08 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:26:25,512 - 4:17:08 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:26:35,069 - 4:17:18 - 9.6s - INFO - root - batch/max_batch/ep:   100/   529/    60 rec:   13.838 mi: 2.42308998 zkl:   80.944 cd:   -6.335 pos_prob:   95.587 prob_neg:  101.922 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    60 rec:   13.838 mi: 2.42308998 zkl:   80.944 cd:   -6.335 pos_prob:   95.587 prob_neg:  101.922 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:26:44,501 - 4:17:27 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    60 rec:   12.523 mi: 2.22989011 zkl:   75.745 cd:   -6.119 pos_prob:   95.673 prob_neg:  101.792 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    60 rec:   12.523 mi: 2.22989011 zkl:   75.745 cd:   -6.119 pos_prob:   95.673 prob_neg:  101.792 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:26:53,912 - 4:17:37 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    60 rec:   17.101 mi: 2.22687960 zkl:   73.450 cd:   -2.948 pos_prob:   97.264 prob_neg:  100.212 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    60 rec:   17.101 mi: 2.22687960 zkl:   73.450 cd:   -2.948 pos_prob:   97.264 prob_neg:  100.212 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:27:03,130 - 4:17:46 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    60 rec:   12.302 mi: 2.17870235 zkl:   77.974 cd:    3.332 pos_prob:  103.229 prob_neg:   99.897 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    60 rec:   12.302 mi: 2.17870235 zkl:   77.974 cd:    3.332 pos_prob:  103.229 prob_neg:   99.897 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:27:12,541 - 4:17:55 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    60 rec:   16.222 mi: 2.29932022 zkl:   75.917 cd:   -9.363 pos_prob:   95.754 prob_neg:  105.117 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    60 rec:   16.222 mi: 2.29932022 zkl:   75.917 cd:   -9.363 pos_prob:   95.754 prob_neg:  105.117 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:27:12,553 - 4:17:55 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-291.630
Langevin prior   1/ 40: energy=-291.630
2022-12-28 21:27:12,558 - 4:17:55 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1594.844
Langevin prior   6/ 40: energy=-1594.844
2022-12-28 21:27:12,564 - 4:17:55 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2445.765
Langevin prior  11/ 40: energy=-2445.765
2022-12-28 21:27:12,569 - 4:17:55 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2730.506
Langevin prior  16/ 40: energy=-2730.506
2022-12-28 21:27:12,575 - 4:17:55 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2876.042
Langevin prior  21/ 40: energy=-2876.042
2022-12-28 21:27:12,582 - 4:17:55 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2989.894
Langevin prior  26/ 40: energy=-2989.894
2022-12-28 21:27:12,590 - 4:17:55 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3010.422
Langevin prior  31/ 40: energy=-3010.422
2022-12-28 21:27:12,599 - 4:17:55 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3011.430
Langevin prior  36/ 40: energy=-3011.430
2022-12-28 21:27:12,607 - 4:17:55 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3052.306
Langevin prior  40/ 40: energy=-3052.306
2022-12-28 21:36:58,633 - 4:27:41 - 586.0s - INFO - root - Negative Log-likehood -187.929873
200 done. -163.48938234820616
400 done. -156.8081679768152
600 done. -156.2352755833065
800 done. -156.9542790192478
1000 done. -155.64532050989553
1200 done. -156.37318785194935
1400 done. -157.87115745376852
1600 done. -156.11260648693613
1800 done. -155.53716047366257
2000 done. -174.07088982347545
2200 done. -186.2791852708268
2400 done. -199.35347891593887
2600 done. -210.5995619538652
2800 done. -218.34830262592786
3000 done. -227.81090047849548
3200 done. -233.73246241795263
3400 done. -241.22724957600406
3600 done. -246.1887773139989
3800 done. -252.23675854646478
4000 done. -256.1294982772199
4200 done. -245.95561600015373
4400 done. -235.5399594420522
4600 done. -226.26992513386514
4800 done. -217.66479640102756
5000 done. -209.95984100984697
5200 done. -202.7334376068004
5400 done. -196.1253970065119
5600 done. -189.9847551414886
Negative Log-likehood -187.929873
2022-12-28 21:36:58,634 - 4:27:41 - 0.0s - INFO - root - log-likelihood:   -187.930
log-likelihood:   -187.930
2022-12-28 21:37:45,677 - 4:28:28 - 47.0s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-060-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 21:37:45,679 - 4:28:28 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-060-test-greedy.txt
Generation: 188 batches
2022-12-28 21:37:51,339 - 4:28:34 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:37:52,856 - 4:28:36 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 21:37:52,856 - 4:28:36 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:37:55,364 - 4:28:38 - 2.5s - INFO - root - --- bleu: BLEU = 28.90, 51.5/30.8/25.0/22.1 (BP=0.946, ratio=0.947, hyp_len=160775, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 28.90, 51.5/30.8/25.0/22.1 (BP=0.946, ratio=0.947, hyp_len=160775, ref_len=169777)
--- bleu: BLEU = 28.90, 51.5/30.8/25.0/22.1 (BP=0.946, ratio=0.947, hyp_len=160775, ref_len=169777)

2022-12-28 21:37:55,696 - 4:28:38 - 0.3s - INFO - root - task_id=1
task_id=1
Done loading corpus
Done loading corpus
/home/xiaodi/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
2022-12-28 21:38:08,653 - 4:28:51 - 13.0s - INFO - root - GMVAE4Lamol (
  (embedding): Embedding(40489, 512, padding_idx=40479), parameters=20730368
  (dec_embedding): Embedding(40489, 512, padding_idx=40479), parameters=20730368
  (x_encoder): EncoderRNN(
    (input_dropout): Dropout(p=0, inplace=False)
    (rnn): GRU(512, 512, batch_first=True, dropout=0.3, bidirectional=True)
  ), parameters=3151872
  (decoder): DecoderRNN(
    (input_dropout): Dropout(p=0.3, inplace=False)
    (rnn): GRU(552, 512, batch_first=True, dropout=0.3)
    (embedding): Embedding(40489, 512, padding_idx=40479)
    (output_dropout): Dropout(p=0.3, inplace=False)
  ), parameters=22367744
  (q_y_mean): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (q_y_logvar): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (dec_init_connector): LinearConnector(
    (linear): Linear(in_features=40, out_features=512, bias=False)
  ), parameters=20480
  (nll_loss): NLLEntropy(
    (nll_loss): NLLLoss()
  ), parameters=0
  (ebm): Sequential (
    (0): Linear(in_features=40, out_features=200, bias=True), weights=((200, 40), (200,)), parameters=8200
    (1): GELU(), weights=(), parameters=0
    (2): Linear(in_features=200, out_features=200, bias=True), weights=((200, 200), (200,)), parameters=40200
    (3): GELU(), weights=(), parameters=0
    (4): Linear(in_features=200, out_features=20, bias=True), weights=((20, 200), (20,)), parameters=4020
  ) Total Parameters=52420, parameters=52420
) Total Parameters=67135252
Done loading corpus
Max len 40 and min len 11 and avg len 31.746755
Max len 40 and min len 11 and avg len 31.068278
Max len 40 and min len 11 and avg len 31.068278
GMVAE4Lamol (
  (embedding): Embedding(40489, 512, padding_idx=40479), parameters=20730368
  (dec_embedding): Embedding(40489, 512, padding_idx=40479), parameters=20730368
  (x_encoder): EncoderRNN(
    (input_dropout): Dropout(p=0, inplace=False)
    (rnn): GRU(512, 512, batch_first=True, dropout=0.3, bidirectional=True)
  ), parameters=3151872
  (decoder): DecoderRNN(
    (input_dropout): Dropout(p=0.3, inplace=False)
    (rnn): GRU(552, 512, batch_first=True, dropout=0.3)
    (embedding): Embedding(40489, 512, padding_idx=40479)
    (output_dropout): Dropout(p=0.3, inplace=False)
  ), parameters=22367744
  (q_y_mean): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (q_y_logvar): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (dec_init_connector): LinearConnector(
    (linear): Linear(in_features=40, out_features=512, bias=False)
  ), parameters=20480
  (nll_loss): NLLEntropy(
    (nll_loss): NLLLoss()
  ), parameters=0
  (ebm): Sequential (
    (0): Linear(in_features=40, out_features=200, bias=True), weights=((200, 40), (200,)), parameters=8200
    (1): GELU(), weights=(), parameters=0
    (2): Linear(in_features=200, out_features=200, bias=True), weights=((200, 200), (200,)), parameters=40200
    (3): GELU(), weights=(), parameters=0
    (4): Linear(in_features=200, out_features=20, bias=True), weights=((20, 200), (20,)), parameters=4020
  ) Total Parameters=52420, parameters=52420
) Total Parameters=67135252
2022-12-28 21:38:08,653 - 4:28:51 - 0.0s - INFO - root - **** Training Begins ****
**** Training Begins ****
2022-12-28 21:38:08,653 - 4:28:51 - 0.0s - INFO - root - **** Epoch 0/60 ****
**** Epoch 0/60 ****
2022-12-28 21:38:08,663 - 4:28:51 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:38:08,664 - 4:28:51 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:38:17,825 - 4:29:01 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/     1 rec:  185.381 mi: 0.00052309 zkl:  104.061 cd:    0.007 pos_prob:    3.005 prob_neg:    2.998 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     1 rec:  185.381 mi: 0.00052309 zkl:  104.061 cd:    0.007 pos_prob:    3.005 prob_neg:    2.998 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:38:27,440 - 4:29:10 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/     1 rec:  186.822 mi: 0.00118613 zkl:  120.652 cd:    0.002 pos_prob:    3.003 prob_neg:    3.001 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     1 rec:  186.822 mi: 0.00118613 zkl:  120.652 cd:    0.002 pos_prob:    3.003 prob_neg:    3.001 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:38:36,709 - 4:29:19 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/     1 rec:  129.476 mi: 0.00169277 zkl:  147.225 cd:    0.008 pos_prob:    3.005 prob_neg:    2.997 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     1 rec:  129.476 mi: 0.00169277 zkl:  147.225 cd:    0.008 pos_prob:    3.005 prob_neg:    2.997 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:38:45,996 - 4:29:29 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/     1 rec:  146.061 mi: 0.00228119 zkl:  153.520 cd:    0.008 pos_prob:    3.004 prob_neg:    2.996 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     1 rec:  146.061 mi: 0.00228119 zkl:  153.520 cd:    0.008 pos_prob:    3.004 prob_neg:    2.996 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:38:55,625 - 4:29:38 - 9.6s - INFO - root - batch/max_batch/ep:   500/   529/     1 rec:  131.308 mi: 0.00307989 zkl:  162.215 cd:    0.008 pos_prob:    3.008 prob_neg:    3.000 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     1 rec:  131.308 mi: 0.00307989 zkl:  162.215 cd:    0.008 pos_prob:    3.008 prob_neg:    3.000 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:38:55,637 - 4:29:38 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -90.034
Langevin prior   1/ 40: energy= -90.034
2022-12-28 21:38:55,643 - 4:29:38 - 0.0s - INFO - root - Langevin prior   6/ 40: energy= -90.078
Langevin prior   6/ 40: energy= -90.078
2022-12-28 21:38:55,648 - 4:29:38 - 0.0s - INFO - root - Langevin prior  11/ 40: energy= -89.998
Langevin prior  11/ 40: energy= -89.998
2022-12-28 21:38:55,654 - 4:29:38 - 0.0s - INFO - root - Langevin prior  16/ 40: energy= -90.004
Langevin prior  16/ 40: energy= -90.004
2022-12-28 21:38:55,661 - 4:29:38 - 0.0s - INFO - root - Langevin prior  21/ 40: energy= -89.990
Langevin prior  21/ 40: energy= -89.990
2022-12-28 21:38:55,669 - 4:29:38 - 0.0s - INFO - root - Langevin prior  26/ 40: energy= -89.987
Langevin prior  26/ 40: energy= -89.987
2022-12-28 21:38:55,678 - 4:29:38 - 0.0s - INFO - root - Langevin prior  31/ 40: energy= -90.023
Langevin prior  31/ 40: energy= -90.023
2022-12-28 21:38:55,687 - 4:29:38 - 0.0s - INFO - root - Langevin prior  36/ 40: energy= -90.041
Langevin prior  36/ 40: energy= -90.041
2022-12-28 21:38:55,695 - 4:29:38 - 0.0s - INFO - root - Langevin prior  40/ 40: energy= -89.947
Langevin prior  40/ 40: energy= -89.947
2022-12-28 21:38:58,310 - 4:29:41 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-001-test-greedy.txt
Generation: 188 batches
2022-12-28 21:39:04,030 - 4:29:47 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:39:05,359 - 4:29:48 - 1.3s - INFO - root - Generation Done
Generation Done
2022-12-28 21:39:05,359 - 4:29:48 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:39:07,585 - 4:29:50 - 2.2s - INFO - root - --- bleu: BLEU = 25.52, 54.1/31.3/24.0/20.6 (BP=0.844, ratio=0.855, hyp_len=145138, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 25.52, 54.1/31.3/24.0/20.6 (BP=0.844, ratio=0.855, hyp_len=145138, ref_len=169777)
--- bleu: BLEU = 25.52, 54.1/31.3/24.0/20.6 (BP=0.844, ratio=0.855, hyp_len=145138, ref_len=169777)

2022-12-28 21:39:07,586 - 4:29:50 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:39:07,587 - 4:29:50 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:39:17,234 - 4:30:00 - 9.6s - INFO - root - batch/max_batch/ep:   100/   529/     2 rec:   81.169 mi: 0.00440311 zkl:  184.309 cd:    0.008 pos_prob:    3.007 prob_neg:    2.999 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     2 rec:   81.169 mi: 0.00440311 zkl:  184.309 cd:    0.008 pos_prob:    3.007 prob_neg:    2.999 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:39:26,345 - 4:30:09 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/     2 rec:   84.172 mi: 0.00539947 zkl:  194.979 cd:    0.016 pos_prob:    3.011 prob_neg:    2.995 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     2 rec:   84.172 mi: 0.00539947 zkl:  194.979 cd:    0.016 pos_prob:    3.011 prob_neg:    2.995 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:39:35,729 - 4:30:18 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/     2 rec:  104.471 mi: 0.00526381 zkl:  195.102 cd:    0.012 pos_prob:    3.015 prob_neg:    3.003 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     2 rec:  104.471 mi: 0.00526381 zkl:  195.102 cd:    0.012 pos_prob:    3.015 prob_neg:    3.003 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:39:45,064 - 4:30:28 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/     2 rec:   95.260 mi: 0.00615740 zkl:  200.283 cd:    0.022 pos_prob:    3.020 prob_neg:    2.998 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     2 rec:   95.260 mi: 0.00615740 zkl:  200.283 cd:    0.022 pos_prob:    3.020 prob_neg:    2.998 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:39:54,330 - 4:30:37 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/     2 rec:   84.778 mi: 0.00429940 zkl:  192.309 cd:    0.004 pos_prob:    3.001 prob_neg:    2.996 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     2 rec:   84.778 mi: 0.00429940 zkl:  192.309 cd:    0.004 pos_prob:    3.001 prob_neg:    2.996 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:39:54,343 - 4:30:37 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -90.080
Langevin prior   1/ 40: energy= -90.080
2022-12-28 21:39:54,349 - 4:30:37 - 0.0s - INFO - root - Langevin prior   6/ 40: energy= -90.075
Langevin prior   6/ 40: energy= -90.075
2022-12-28 21:39:54,354 - 4:30:37 - 0.0s - INFO - root - Langevin prior  11/ 40: energy= -90.068
Langevin prior  11/ 40: energy= -90.068
2022-12-28 21:39:54,360 - 4:30:37 - 0.0s - INFO - root - Langevin prior  16/ 40: energy= -89.939
Langevin prior  16/ 40: energy= -89.939
2022-12-28 21:39:54,367 - 4:30:37 - 0.0s - INFO - root - Langevin prior  21/ 40: energy= -89.929
Langevin prior  21/ 40: energy= -89.929
2022-12-28 21:39:54,374 - 4:30:37 - 0.0s - INFO - root - Langevin prior  26/ 40: energy= -89.970
Langevin prior  26/ 40: energy= -89.970
2022-12-28 21:39:54,384 - 4:30:37 - 0.0s - INFO - root - Langevin prior  31/ 40: energy= -90.050
Langevin prior  31/ 40: energy= -90.050
2022-12-28 21:39:54,393 - 4:30:37 - 0.0s - INFO - root - Langevin prior  36/ 40: energy= -89.968
Langevin prior  36/ 40: energy= -89.968
2022-12-28 21:39:54,402 - 4:30:37 - 0.0s - INFO - root - Langevin prior  40/ 40: energy= -89.887
Langevin prior  40/ 40: energy= -89.887
2022-12-28 21:39:57,003 - 4:30:40 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-002-test-greedy.txt
Generation: 188 batches
2022-12-28 21:40:02,778 - 4:30:45 - 5.8s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:40:04,211 - 4:30:47 - 1.4s - INFO - root - Generation Done
Generation Done
2022-12-28 21:40:04,211 - 4:30:47 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:40:06,553 - 4:30:49 - 2.3s - INFO - root - --- bleu: BLEU = 29.43, 55.5/32.6/25.5/22.3 (BP=0.925, ratio=0.927, hyp_len=157431, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.43, 55.5/32.6/25.5/22.3 (BP=0.925, ratio=0.927, hyp_len=157431, ref_len=169777)
--- bleu: BLEU = 29.43, 55.5/32.6/25.5/22.3 (BP=0.925, ratio=0.927, hyp_len=157431, ref_len=169777)

2022-12-28 21:40:06,553 - 4:30:49 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:40:06,555 - 4:30:49 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:40:15,972 - 4:30:59 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/     3 rec:   84.238 mi: 0.00616670 zkl:  218.355 cd:    0.032 pos_prob:    3.025 prob_neg:    2.993 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     3 rec:   84.238 mi: 0.00616670 zkl:  218.355 cd:    0.032 pos_prob:    3.025 prob_neg:    2.993 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:40:25,186 - 4:31:08 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/     3 rec:   58.733 mi: 0.00721931 zkl:  219.288 cd:    0.018 pos_prob:    3.016 prob_neg:    2.998 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     3 rec:   58.733 mi: 0.00721931 zkl:  219.288 cd:    0.018 pos_prob:    3.016 prob_neg:    2.998 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:40:34,725 - 4:31:17 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/     3 rec:   68.275 mi: 0.00799751 zkl:  232.043 cd:    0.010 pos_prob:    3.015 prob_neg:    3.005 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     3 rec:   68.275 mi: 0.00799751 zkl:  232.043 cd:    0.010 pos_prob:    3.015 prob_neg:    3.005 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:40:43,924 - 4:31:27 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/     3 rec:   77.615 mi: 0.00572991 zkl:  210.937 cd:    0.018 pos_prob:    3.014 prob_neg:    2.995 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     3 rec:   77.615 mi: 0.00572991 zkl:  210.937 cd:    0.018 pos_prob:    3.014 prob_neg:    2.995 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:40:53,385 - 4:31:36 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/     3 rec:   53.643 mi: 0.00754356 zkl:  223.352 cd:    0.014 pos_prob:    3.012 prob_neg:    2.999 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     3 rec:   53.643 mi: 0.00754356 zkl:  223.352 cd:    0.014 pos_prob:    3.012 prob_neg:    2.999 kl_weight:    0.000 do_ae_train: True
2022-12-28 21:40:53,400 - 4:31:36 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -89.837
Langevin prior   1/ 40: energy= -89.837
2022-12-28 21:40:53,409 - 4:31:36 - 0.0s - INFO - root - Langevin prior   6/ 40: energy= -90.014
Langevin prior   6/ 40: energy= -90.014
2022-12-28 21:40:53,418 - 4:31:36 - 0.0s - INFO - root - Langevin prior  11/ 40: energy= -90.040
Langevin prior  11/ 40: energy= -90.040
2022-12-28 21:40:53,428 - 4:31:36 - 0.0s - INFO - root - Langevin prior  16/ 40: energy= -89.978
Langevin prior  16/ 40: energy= -89.978
2022-12-28 21:40:53,438 - 4:31:36 - 0.0s - INFO - root - Langevin prior  21/ 40: energy= -89.965
Langevin prior  21/ 40: energy= -89.965
2022-12-28 21:40:53,448 - 4:31:36 - 0.0s - INFO - root - Langevin prior  26/ 40: energy= -90.050
Langevin prior  26/ 40: energy= -90.050
2022-12-28 21:40:53,460 - 4:31:36 - 0.0s - INFO - root - Langevin prior  31/ 40: energy= -89.941
Langevin prior  31/ 40: energy= -89.941
2022-12-28 21:40:53,471 - 4:31:36 - 0.0s - INFO - root - Langevin prior  36/ 40: energy= -89.937
Langevin prior  36/ 40: energy= -89.937
2022-12-28 21:40:53,481 - 4:31:36 - 0.0s - INFO - root - Langevin prior  40/ 40: energy= -90.101
Langevin prior  40/ 40: energy= -90.101
2022-12-28 21:40:56,162 - 4:31:39 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-003-test-greedy.txt
Generation: 188 batches
2022-12-28 21:41:01,910 - 4:31:45 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:41:03,350 - 4:31:46 - 1.4s - INFO - root - Generation Done
Generation Done
2022-12-28 21:41:03,350 - 4:31:46 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:41:05,962 - 4:31:49 - 2.6s - INFO - root - --- bleu: BLEU = 30.03, 56.5/33.5/26.6/23.4 (BP=0.912, ratio=0.915, hyp_len=155426, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.03, 56.5/33.5/26.6/23.4 (BP=0.912, ratio=0.915, hyp_len=155426, ref_len=169777)
--- bleu: BLEU = 30.03, 56.5/33.5/26.6/23.4 (BP=0.912, ratio=0.915, hyp_len=155426, ref_len=169777)

2022-12-28 21:41:05,963 - 4:31:49 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:41:05,964 - 4:31:49 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:41:15,481 - 4:31:58 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/     4 rec:   55.245 mi: 0.79319525 zkl:  210.250 cd:    2.884 pos_prob:    5.891 prob_neg:    3.007 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/     4 rec:   55.245 mi: 0.79319525 zkl:  210.250 cd:    2.884 pos_prob:    5.891 prob_neg:    3.007 kl_weight:    0.062 do_ae_train: False
2022-12-28 21:41:24,787 - 4:32:07 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/     4 rec:   54.699 mi: 1.31698954 zkl:  126.318 cd:    5.021 pos_prob:    8.047 prob_neg:    3.026 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/     4 rec:   54.699 mi: 1.31698954 zkl:  126.318 cd:    5.021 pos_prob:    8.047 prob_neg:    3.026 kl_weight:    0.125 do_ae_train: False
2022-12-28 21:41:34,367 - 4:32:17 - 9.6s - INFO - root - batch/max_batch/ep:   300/   529/     4 rec:   44.326 mi: 1.34542775 zkl:   72.087 cd:    6.635 pos_prob:    9.762 prob_neg:    3.127 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/     4 rec:   44.326 mi: 1.34542775 zkl:   72.087 cd:    6.635 pos_prob:    9.762 prob_neg:    3.127 kl_weight:    0.188 do_ae_train: False
2022-12-28 21:41:43,855 - 4:32:27 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/     4 rec:   64.987 mi: 1.40782475 zkl:   56.344 cd:    7.087 pos_prob:   10.635 prob_neg:    3.548 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/     4 rec:   64.987 mi: 1.40782475 zkl:   56.344 cd:    7.087 pos_prob:   10.635 prob_neg:    3.548 kl_weight:    0.251 do_ae_train: False
2022-12-28 21:41:53,203 - 4:32:36 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/     4 rec:   44.885 mi: 1.59807515 zkl:   49.972 cd:    7.709 pos_prob:   13.221 prob_neg:    5.511 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/     4 rec:   44.885 mi: 1.59807515 zkl:   49.972 cd:    7.709 pos_prob:   13.221 prob_neg:    5.511 kl_weight:    0.314 do_ae_train: False
2022-12-28 21:41:53,215 - 4:32:36 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-100.678
Langevin prior   1/ 40: energy=-100.678
2022-12-28 21:41:53,221 - 4:32:36 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-116.018
Langevin prior   6/ 40: energy=-116.018
2022-12-28 21:41:53,226 - 4:32:36 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-121.620
Langevin prior  11/ 40: energy=-121.620
2022-12-28 21:41:53,231 - 4:32:36 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-137.772
Langevin prior  16/ 40: energy=-137.772
2022-12-28 21:41:53,236 - 4:32:36 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-138.253
Langevin prior  21/ 40: energy=-138.253
2022-12-28 21:41:53,243 - 4:32:36 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-157.554
Langevin prior  26/ 40: energy=-157.554
2022-12-28 21:41:53,251 - 4:32:36 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-154.530
Langevin prior  31/ 40: energy=-154.530
2022-12-28 21:41:53,259 - 4:32:36 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-154.315
Langevin prior  36/ 40: energy=-154.315
2022-12-28 21:41:53,266 - 4:32:36 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-151.703
Langevin prior  40/ 40: energy=-151.703
2022-12-28 21:41:55,903 - 4:32:39 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-004-test-greedy.txt
Generation: 188 batches
2022-12-28 21:42:01,661 - 4:32:44 - 5.8s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:42:03,154 - 4:32:46 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 21:42:03,154 - 4:32:46 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:42:05,568 - 4:32:48 - 2.4s - INFO - root - --- bleu: BLEU = 29.13, 53.4/31.4/24.8/21.8 (BP=0.944, ratio=0.945, hyp_len=160495, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.13, 53.4/31.4/24.8/21.8 (BP=0.944, ratio=0.945, hyp_len=160495, ref_len=169777)
--- bleu: BLEU = 29.13, 53.4/31.4/24.8/21.8 (BP=0.944, ratio=0.945, hyp_len=160495, ref_len=169777)

2022-12-28 21:42:05,569 - 4:32:48 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:42:05,570 - 4:32:48 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:42:15,461 - 4:32:58 - 9.9s - INFO - root - batch/max_batch/ep:   100/   529/     5 rec:   45.267 mi: 1.52683938 zkl:   51.298 cd:    8.588 pos_prob:   14.503 prob_neg:    5.915 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/     5 rec:   45.267 mi: 1.52683938 zkl:   51.298 cd:    8.588 pos_prob:   14.503 prob_neg:    5.915 kl_weight:    0.396 do_ae_train: False
2022-12-28 21:42:24,813 - 4:33:08 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/     5 rec:   66.185 mi: 1.60798883 zkl:   40.762 cd:    6.913 pos_prob:   14.867 prob_neg:    7.954 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/     5 rec:   66.185 mi: 1.60798883 zkl:   40.762 cd:    6.913 pos_prob:   14.867 prob_neg:    7.954 kl_weight:    0.459 do_ae_train: False
2022-12-28 21:42:34,184 - 4:33:17 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/     5 rec:   57.107 mi: 1.58321726 zkl:   39.926 cd:    6.092 pos_prob:   18.112 prob_neg:   12.020 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/     5 rec:   57.107 mi: 1.58321726 zkl:   39.926 cd:    6.092 pos_prob:   18.112 prob_neg:   12.020 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:42:43,610 - 4:33:26 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/     5 rec:   48.804 mi: 1.65151954 zkl:   40.517 cd:    8.157 pos_prob:   20.295 prob_neg:   12.139 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/     5 rec:   48.804 mi: 1.65151954 zkl:   40.517 cd:    8.157 pos_prob:   20.295 prob_neg:   12.139 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:42:53,809 - 4:33:37 - 10.2s - INFO - root - batch/max_batch/ep:   500/   529/     5 rec:   45.615 mi: 1.86108172 zkl:   43.367 cd:    7.201 pos_prob:   23.683 prob_neg:   16.482 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/     5 rec:   45.615 mi: 1.86108172 zkl:   43.367 cd:    7.201 pos_prob:   23.683 prob_neg:   16.482 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:42:53,821 - 4:33:37 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-137.805
Langevin prior   1/ 40: energy=-137.805
2022-12-28 21:42:53,827 - 4:33:37 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-237.159
Langevin prior   6/ 40: energy=-237.159
2022-12-28 21:42:53,832 - 4:33:37 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-313.061
Langevin prior  11/ 40: energy=-313.061
2022-12-28 21:42:53,837 - 4:33:37 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-373.735
Langevin prior  16/ 40: energy=-373.735
2022-12-28 21:42:53,843 - 4:33:37 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-413.627
Langevin prior  21/ 40: energy=-413.627
2022-12-28 21:42:53,849 - 4:33:37 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-432.723
Langevin prior  26/ 40: energy=-432.723
2022-12-28 21:42:53,857 - 4:33:37 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-461.177
Langevin prior  31/ 40: energy=-461.177
2022-12-28 21:42:53,865 - 4:33:37 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-487.610
Langevin prior  36/ 40: energy=-487.610
2022-12-28 21:42:53,872 - 4:33:37 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-455.399
Langevin prior  40/ 40: energy=-455.399
2022-12-28 21:52:39,594 - 4:43:22 - 585.7s - INFO - root - Negative Log-likehood -133.530799
200 done. -120.98073456191013
400 done. -116.97995207361744
600 done. -116.67980104870482
800 done. -117.42281985310076
1000 done. -116.4865569264335
1200 done. -117.17157686638468
1400 done. -118.09430790694157
1600 done. -116.7687777706436
1800 done. -116.51895327142218
2000 done. -128.02195887781969
2200 done. -135.59032678869747
2400 done. -143.9068090542783
2600 done. -150.96661044894296
2800 done. -155.94740818174708
3000 done. -162.09216085337354
3200 done. -165.93365004175072
3400 done. -170.7010928600057
3600 done. -173.62228629581315
3800 done. -177.27623130005335
4000 done. -179.75820830039638
4200 done. -172.73840302644294
4400 done. -165.63071591193795
4600 done. -159.34428245219016
4800 done. -153.54216090333344
5000 done. -148.36832869086496
5200 done. -143.47372337204277
5400 done. -139.04356186000075
5600 done. -134.90754910813618
Negative Log-likehood -133.530799
2022-12-28 21:52:39,594 - 4:43:22 - 0.0s - INFO - root - log-likelihood:   -133.531
log-likelihood:   -133.531
2022-12-28 21:53:27,152 - 4:44:10 - 47.6s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-005-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 21:53:27,173 - 4:44:10 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-005-test-greedy.txt
Generation: 188 batches
2022-12-28 21:53:32,916 - 4:44:16 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:53:34,426 - 4:44:17 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 21:53:34,426 - 4:44:17 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 21:53:36,866 - 4:44:20 - 2.4s - INFO - root - --- bleu: BLEU = 28.34, 51.8/30.1/23.6/20.7 (BP=0.959, ratio=0.960, hyp_len=162981, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 28.34, 51.8/30.1/23.6/20.7 (BP=0.959, ratio=0.960, hyp_len=162981, ref_len=169777)
--- bleu: BLEU = 28.34, 51.8/30.1/23.6/20.7 (BP=0.959, ratio=0.960, hyp_len=162981, ref_len=169777)

2022-12-28 21:53:36,867 - 4:44:20 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 21:53:36,868 - 4:44:20 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 21:53:46,625 - 4:44:29 - 9.8s - INFO - root - batch/max_batch/ep:   100/   529/     6 rec:   51.859 mi: 1.66052663 zkl:   45.589 cd:    7.117 pos_prob:   26.131 prob_neg:   19.014 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/     6 rec:   51.859 mi: 1.66052663 zkl:   45.589 cd:    7.117 pos_prob:   26.131 prob_neg:   19.014 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:53:56,181 - 4:44:39 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/     6 rec:   33.618 mi: 1.77798426 zkl:   42.823 cd:    6.113 pos_prob:   28.640 prob_neg:   22.527 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/     6 rec:   33.618 mi: 1.77798426 zkl:   42.823 cd:    6.113 pos_prob:   28.640 prob_neg:   22.527 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:54:05,962 - 4:44:49 - 9.8s - INFO - root - batch/max_batch/ep:   300/   529/     6 rec:   48.334 mi: 1.68462873 zkl:   45.547 cd:    5.506 pos_prob:   31.774 prob_neg:   26.268 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/     6 rec:   48.334 mi: 1.68462873 zkl:   45.547 cd:    5.506 pos_prob:   31.774 prob_neg:   26.268 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:54:16,223 - 4:44:59 - 10.3s - INFO - root - batch/max_batch/ep:   400/   529/     6 rec:   73.351 mi: 1.95867920 zkl:   47.429 cd:    5.793 pos_prob:   34.667 prob_neg:   28.874 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/     6 rec:   73.351 mi: 1.95867920 zkl:   47.429 cd:    5.793 pos_prob:   34.667 prob_neg:   28.874 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:54:25,627 - 4:45:08 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/     6 rec:   56.090 mi: 1.75876653 zkl:   45.324 cd:    4.152 pos_prob:   34.767 prob_neg:   30.615 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/     6 rec:   56.090 mi: 1.75876653 zkl:   45.324 cd:    4.152 pos_prob:   34.767 prob_neg:   30.615 kl_weight:    0.500 do_ae_train: False
2022-12-28 21:54:25,640 - 4:45:08 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-175.332
Langevin prior   1/ 40: energy=-175.332
2022-12-28 21:54:25,645 - 4:45:08 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-458.012
Langevin prior   6/ 40: energy=-458.012
2022-12-28 21:54:25,651 - 4:45:08 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-669.015
Langevin prior  11/ 40: energy=-669.015
2022-12-28 21:54:25,656 - 4:45:08 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-757.814
Langevin prior  16/ 40: energy=-757.814
2022-12-28 21:54:25,661 - 4:45:08 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-842.347
Langevin prior  21/ 40: energy=-842.347
2022-12-28 21:54:25,667 - 4:45:08 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-905.036
Langevin prior  26/ 40: energy=-905.036
2022-12-28 21:54:25,675 - 4:45:08 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-943.657
Langevin prior  31/ 40: energy=-943.657
2022-12-28 21:54:25,683 - 4:45:08 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-893.933
Langevin prior  36/ 40: energy=-893.933
2022-12-28 21:54:25,690 - 4:45:08 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-918.889
Langevin prior  40/ 40: energy=-918.889
2022-12-28 22:04:11,382 - 4:54:54 - 585.7s - INFO - root - Negative Log-likehood -135.246906
200 done. -121.64117668929555
400 done. -117.54769168456066
600 done. -117.09452870385334
800 done. -117.91811335026699
1000 done. -117.03598619335465
1200 done. -117.70445396543792
1400 done. -118.57741456093538
1600 done. -117.26830497725932
1800 done. -117.0083278015666
2000 done. -128.94670589650423
2200 done. -136.81374608805461
2400 done. -145.36796027155557
2600 done. -152.73107147532565
2800 done. -157.76904383366755
3000 done. -164.15027840581203
3200 done. -168.16011284479094
3400 done. -173.0333385288828
3600 done. -175.9671356262376
3800 done. -179.76078183294314
4000 done. -182.36769043991072
4200 done. -175.22982124247966
4400 done. -167.9936701395512
4600 done. -161.58416331733522
4800 done. -155.6625327454766
5000 done. -150.3856554087546
5200 done. -145.3950067053505
5400 done. -140.87855269245722
5600 done. -136.65594236859897
Negative Log-likehood -135.246906
2022-12-28 22:04:11,382 - 4:54:54 - 0.0s - INFO - root - log-likelihood:   -135.247
log-likelihood:   -135.247
2022-12-28 22:04:59,611 - 4:55:42 - 48.2s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-006-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 22:04:59,632 - 4:55:42 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-006-test-greedy.txt
Generation: 188 batches
2022-12-28 22:05:05,388 - 4:55:48 - 5.8s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:05:06,895 - 4:55:50 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:05:06,895 - 4:55:50 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:05:09,360 - 4:55:52 - 2.5s - INFO - root - --- bleu: BLEU = 28.44, 51.3/29.9/23.6/20.8 (BP=0.966, ratio=0.966, hyp_len=164019, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 28.44, 51.3/29.9/23.6/20.8 (BP=0.966, ratio=0.966, hyp_len=164019, ref_len=169777)
--- bleu: BLEU = 28.44, 51.3/29.9/23.6/20.8 (BP=0.966, ratio=0.966, hyp_len=164019, ref_len=169777)

2022-12-28 22:05:09,361 - 4:55:52 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:05:09,362 - 4:55:52 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:05:18,670 - 4:56:01 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/     7 rec:   33.492 mi: 1.77183986 zkl:  128.326 cd:   -1.127 pos_prob:   33.148 prob_neg:   34.275 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     7 rec:   33.492 mi: 1.77183986 zkl:  128.326 cd:   -1.127 pos_prob:   33.148 prob_neg:   34.275 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:05:28,618 - 4:56:11 - 9.9s - INFO - root - batch/max_batch/ep:   200/   529/     7 rec:   54.411 mi: 1.83656311 zkl:  138.359 cd:   -5.427 pos_prob:   28.129 prob_neg:   33.556 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     7 rec:   54.411 mi: 1.83656311 zkl:  138.359 cd:   -5.427 pos_prob:   28.129 prob_neg:   33.556 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:05:37,698 - 4:56:20 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/     7 rec:   49.709 mi: 1.76870883 zkl:  141.301 cd:   -6.674 pos_prob:   26.050 prob_neg:   32.724 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     7 rec:   49.709 mi: 1.76870883 zkl:  141.301 cd:   -6.674 pos_prob:   26.050 prob_neg:   32.724 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:05:46,817 - 4:56:30 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/     7 rec:   31.354 mi: 1.73616946 zkl:  147.561 cd:   -7.919 pos_prob:   25.841 prob_neg:   33.760 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     7 rec:   31.354 mi: 1.73616946 zkl:  147.561 cd:   -7.919 pos_prob:   25.841 prob_neg:   33.760 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:05:55,860 - 4:56:39 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/     7 rec:   44.203 mi: 1.71777678 zkl:  149.874 cd:   -9.393 pos_prob:   25.235 prob_neg:   34.628 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     7 rec:   44.203 mi: 1.71777678 zkl:  149.874 cd:   -9.393 pos_prob:   25.235 prob_neg:   34.628 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:05:55,872 - 4:56:39 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-174.515
Langevin prior   1/ 40: energy=-174.515
2022-12-28 22:05:55,878 - 4:56:39 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-454.274
Langevin prior   6/ 40: energy=-454.274
2022-12-28 22:05:55,883 - 4:56:39 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-677.393
Langevin prior  11/ 40: energy=-677.393
2022-12-28 22:05:55,888 - 4:56:39 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-854.129
Langevin prior  16/ 40: energy=-854.129
2022-12-28 22:05:55,894 - 4:56:39 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-943.403
Langevin prior  21/ 40: energy=-943.403
2022-12-28 22:05:55,900 - 4:56:39 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-950.815
Langevin prior  26/ 40: energy=-950.815
2022-12-28 22:05:55,906 - 4:56:39 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-965.969
Langevin prior  31/ 40: energy=-965.969
2022-12-28 22:05:55,914 - 4:56:39 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-975.229
Langevin prior  36/ 40: energy=-975.229
2022-12-28 22:05:55,921 - 4:56:39 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-959.434
Langevin prior  40/ 40: energy=-959.434
2022-12-28 22:05:58,528 - 4:56:41 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-007-test-greedy.txt
Generation: 188 batches
2022-12-28 22:06:04,267 - 4:56:47 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:06:05,741 - 4:56:48 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:06:05,741 - 4:56:48 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:06:08,201 - 4:56:51 - 2.5s - INFO - root - --- bleu: BLEU = 30.74, 55.3/33.0/26.4/23.3 (BP=0.944, ratio=0.946, hyp_len=160526, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.74, 55.3/33.0/26.4/23.3 (BP=0.944, ratio=0.946, hyp_len=160526, ref_len=169777)
--- bleu: BLEU = 30.74, 55.3/33.0/26.4/23.3 (BP=0.944, ratio=0.946, hyp_len=160526, ref_len=169777)

2022-12-28 22:06:08,202 - 4:56:51 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:06:08,203 - 4:56:51 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:06:17,378 - 4:57:00 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/     8 rec:   36.761 mi: 1.82041943 zkl:  150.777 cd:   -5.377 pos_prob:   27.020 prob_neg:   32.397 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     8 rec:   36.761 mi: 1.82041943 zkl:  150.777 cd:   -5.377 pos_prob:   27.020 prob_neg:   32.397 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:06:26,593 - 4:57:09 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/     8 rec:   27.691 mi: 1.77320397 zkl:  160.428 cd:   -6.917 pos_prob:   25.208 prob_neg:   32.124 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     8 rec:   27.691 mi: 1.77320397 zkl:  160.428 cd:   -6.917 pos_prob:   25.208 prob_neg:   32.124 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:06:35,698 - 4:57:18 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/     8 rec:   28.190 mi: 1.74811006 zkl:  161.704 cd:   -8.881 pos_prob:   24.363 prob_neg:   33.244 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     8 rec:   28.190 mi: 1.74811006 zkl:  161.704 cd:   -8.881 pos_prob:   24.363 prob_neg:   33.244 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:06:44,851 - 4:57:28 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/     8 rec:   33.487 mi: 1.72469926 zkl:  161.112 cd:   -7.714 pos_prob:   25.690 prob_neg:   33.404 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     8 rec:   33.487 mi: 1.72469926 zkl:  161.112 cd:   -7.714 pos_prob:   25.690 prob_neg:   33.404 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:06:54,017 - 4:57:37 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/     8 rec:   29.223 mi: 1.69303989 zkl:  167.442 cd:  -10.006 pos_prob:   22.761 prob_neg:   32.767 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     8 rec:   29.223 mi: 1.69303989 zkl:  167.442 cd:  -10.006 pos_prob:   22.761 prob_neg:   32.767 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:06:54,030 - 4:57:37 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-169.719
Langevin prior   1/ 40: energy=-169.719
2022-12-28 22:06:54,036 - 4:57:37 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-405.636
Langevin prior   6/ 40: energy=-405.636
2022-12-28 22:06:54,041 - 4:57:37 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-661.440
Langevin prior  11/ 40: energy=-661.440
2022-12-28 22:06:54,047 - 4:57:37 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-855.842
Langevin prior  16/ 40: energy=-855.842
2022-12-28 22:06:54,053 - 4:57:37 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-937.885
Langevin prior  21/ 40: energy=-937.885
2022-12-28 22:06:54,062 - 4:57:37 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-982.592
Langevin prior  26/ 40: energy=-982.592
2022-12-28 22:06:54,070 - 4:57:37 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1009.616
Langevin prior  31/ 40: energy=-1009.616
2022-12-28 22:06:54,080 - 4:57:37 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1084.980
Langevin prior  36/ 40: energy=-1084.980
2022-12-28 22:06:54,089 - 4:57:37 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1049.249
Langevin prior  40/ 40: energy=-1049.249
2022-12-28 22:06:56,602 - 4:57:39 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-008-test-greedy.txt
Generation: 188 batches
2022-12-28 22:07:02,292 - 4:57:45 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:07:03,768 - 4:57:46 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:07:03,769 - 4:57:46 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:07:06,196 - 4:57:49 - 2.4s - INFO - root - --- bleu: BLEU = 31.33, 56.6/34.2/27.7/24.6 (BP=0.925, ratio=0.928, hyp_len=157510, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.33, 56.6/34.2/27.7/24.6 (BP=0.925, ratio=0.928, hyp_len=157510, ref_len=169777)
--- bleu: BLEU = 31.33, 56.6/34.2/27.7/24.6 (BP=0.925, ratio=0.928, hyp_len=157510, ref_len=169777)

2022-12-28 22:07:06,196 - 4:57:49 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:07:06,198 - 4:57:49 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:07:15,523 - 4:57:58 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/     9 rec:   19.345 mi: 1.62483931 zkl:  168.131 cd:  -10.496 pos_prob:   22.776 prob_neg:   33.272 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     9 rec:   19.345 mi: 1.62483931 zkl:  168.131 cd:  -10.496 pos_prob:   22.776 prob_neg:   33.272 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:07:24,713 - 4:58:07 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/     9 rec:   33.207 mi: 1.72234511 zkl:  164.990 cd:   -9.042 pos_prob:   25.767 prob_neg:   34.809 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     9 rec:   33.207 mi: 1.72234511 zkl:  164.990 cd:   -9.042 pos_prob:   25.767 prob_neg:   34.809 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:07:33,826 - 4:58:17 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/     9 rec:   29.977 mi: 1.69196367 zkl:  168.287 cd:  -11.697 pos_prob:   22.025 prob_neg:   33.722 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     9 rec:   29.977 mi: 1.69196367 zkl:  168.287 cd:  -11.697 pos_prob:   22.025 prob_neg:   33.722 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:07:43,051 - 4:58:26 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/     9 rec:   25.336 mi: 1.71407664 zkl:  171.377 cd:  -11.704 pos_prob:   22.371 prob_neg:   34.075 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     9 rec:   25.336 mi: 1.71407664 zkl:  171.377 cd:  -11.704 pos_prob:   22.371 prob_neg:   34.075 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:07:52,546 - 4:58:35 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/     9 rec:   19.811 mi: 1.58768880 zkl:  173.854 cd:  -10.247 pos_prob:   23.516 prob_neg:   33.763 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     9 rec:   19.811 mi: 1.58768880 zkl:  173.854 cd:  -10.247 pos_prob:   23.516 prob_neg:   33.763 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:07:52,559 - 4:58:35 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-175.063
Langevin prior   1/ 40: energy=-175.063
2022-12-28 22:07:52,564 - 4:58:35 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-461.508
Langevin prior   6/ 40: energy=-461.508
2022-12-28 22:07:52,569 - 4:58:35 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-691.698
Langevin prior  11/ 40: energy=-691.698
2022-12-28 22:07:52,575 - 4:58:35 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-803.988
Langevin prior  16/ 40: energy=-803.988
2022-12-28 22:07:52,581 - 4:58:35 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-900.123
Langevin prior  21/ 40: energy=-900.123
2022-12-28 22:07:52,587 - 4:58:35 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-991.082
Langevin prior  26/ 40: energy=-991.082
2022-12-28 22:07:52,595 - 4:58:35 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1003.917
Langevin prior  31/ 40: energy=-1003.917
2022-12-28 22:07:52,604 - 4:58:35 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-977.139
Langevin prior  36/ 40: energy=-977.139
2022-12-28 22:07:52,612 - 4:58:35 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-973.888
Langevin prior  40/ 40: energy=-973.888
2022-12-28 22:07:55,244 - 4:58:38 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-009-test-greedy.txt
Generation: 188 batches
2022-12-28 22:08:01,009 - 4:58:44 - 5.8s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:08:02,501 - 4:58:45 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:08:02,501 - 4:58:45 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:08:04,915 - 4:58:48 - 2.4s - INFO - root - --- bleu: BLEU = 31.62, 55.8/33.8/27.2/24.1 (BP=0.948, ratio=0.949, hyp_len=161174, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.62, 55.8/33.8/27.2/24.1 (BP=0.948, ratio=0.949, hyp_len=161174, ref_len=169777)
--- bleu: BLEU = 31.62, 55.8/33.8/27.2/24.1 (BP=0.948, ratio=0.949, hyp_len=161174, ref_len=169777)

2022-12-28 22:08:04,916 - 4:58:48 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:08:04,917 - 4:58:48 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:08:14,247 - 4:58:57 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    10 rec:   32.527 mi: 1.97901392 zkl:  125.260 cd:    3.767 pos_prob:   39.842 prob_neg:   36.076 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    10 rec:   32.527 mi: 1.97901392 zkl:  125.260 cd:    3.767 pos_prob:   39.842 prob_neg:   36.076 kl_weight:    0.062 do_ae_train: False
2022-12-28 22:08:23,850 - 4:59:07 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/    10 rec:   27.390 mi: 2.12915564 zkl:   92.882 cd:    9.798 pos_prob:   47.958 prob_neg:   38.161 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    10 rec:   27.390 mi: 2.12915564 zkl:   92.882 cd:    9.798 pos_prob:   47.958 prob_neg:   38.161 kl_weight:    0.125 do_ae_train: False
2022-12-28 22:08:33,559 - 4:59:16 - 9.7s - INFO - root - batch/max_batch/ep:   300/   529/    10 rec:   27.700 mi: 2.12414551 zkl:   85.105 cd:    6.894 pos_prob:   45.469 prob_neg:   38.576 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    10 rec:   27.700 mi: 2.12414551 zkl:   85.105 cd:    6.894 pos_prob:   45.469 prob_neg:   38.576 kl_weight:    0.188 do_ae_train: False
2022-12-28 22:08:43,004 - 4:59:26 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    10 rec:   39.649 mi: 1.95626318 zkl:   80.064 cd:    7.605 pos_prob:   47.768 prob_neg:   40.163 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    10 rec:   39.649 mi: 1.95626318 zkl:   80.064 cd:    7.605 pos_prob:   47.768 prob_neg:   40.163 kl_weight:    0.251 do_ae_train: False
2022-12-28 22:08:52,431 - 4:59:35 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    10 rec:   28.049 mi: 2.03196883 zkl:   68.539 cd:    7.041 pos_prob:   51.367 prob_neg:   44.326 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    10 rec:   28.049 mi: 2.03196883 zkl:   68.539 cd:    7.041 pos_prob:   51.367 prob_neg:   44.326 kl_weight:    0.314 do_ae_train: False
2022-12-28 22:08:52,443 - 4:59:35 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-220.753
Langevin prior   1/ 40: energy=-220.753
2022-12-28 22:08:52,449 - 4:59:35 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-634.192
Langevin prior   6/ 40: energy=-634.192
2022-12-28 22:08:52,454 - 4:59:35 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-944.524
Langevin prior  11/ 40: energy=-944.524
2022-12-28 22:08:52,459 - 4:59:35 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1166.381
Langevin prior  16/ 40: energy=-1166.381
2022-12-28 22:08:52,465 - 4:59:35 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1189.588
Langevin prior  21/ 40: energy=-1189.588
2022-12-28 22:08:52,473 - 4:59:35 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1300.136
Langevin prior  26/ 40: energy=-1300.136
2022-12-28 22:08:52,484 - 4:59:35 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1336.710
Langevin prior  31/ 40: energy=-1336.710
2022-12-28 22:08:52,495 - 4:59:35 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1383.816
Langevin prior  36/ 40: energy=-1383.816
2022-12-28 22:08:52,503 - 4:59:35 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1364.686
Langevin prior  40/ 40: energy=-1364.686
2022-12-28 22:08:55,332 - 4:59:38 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-010-test-greedy.txt
Generation: 188 batches
2022-12-28 22:09:01,011 - 4:59:44 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:09:02,477 - 4:59:45 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:09:02,477 - 4:59:45 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:09:04,847 - 4:59:48 - 2.4s - INFO - root - --- bleu: BLEU = 29.96, 55.2/33.4/26.9/23.9 (BP=0.908, ratio=0.912, hyp_len=154889, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.96, 55.2/33.4/26.9/23.9 (BP=0.908, ratio=0.912, hyp_len=154889, ref_len=169777)
--- bleu: BLEU = 29.96, 55.2/33.4/26.9/23.9 (BP=0.908, ratio=0.912, hyp_len=154889, ref_len=169777)

2022-12-28 22:09:04,847 - 4:59:48 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:09:04,849 - 4:59:48 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:09:14,306 - 4:59:57 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    11 rec:   29.484 mi: 2.11419916 zkl:   63.322 cd:    0.300 pos_prob:   47.684 prob_neg:   47.384 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    11 rec:   29.484 mi: 2.11419916 zkl:   63.322 cd:    0.300 pos_prob:   47.684 prob_neg:   47.384 kl_weight:    0.396 do_ae_train: False
2022-12-28 22:09:24,233 - 5:00:07 - 9.9s - INFO - root - batch/max_batch/ep:   200/   529/    11 rec:   29.402 mi: 2.12425327 zkl:   65.921 cd:    4.229 pos_prob:   54.008 prob_neg:   49.779 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    11 rec:   29.402 mi: 2.12425327 zkl:   65.921 cd:    4.229 pos_prob:   54.008 prob_neg:   49.779 kl_weight:    0.459 do_ae_train: False
2022-12-28 22:09:33,532 - 5:00:16 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    11 rec:   35.894 mi: 1.93527746 zkl:   60.512 cd:    2.394 pos_prob:   54.376 prob_neg:   51.982 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    11 rec:   35.894 mi: 1.93527746 zkl:   60.512 cd:    2.394 pos_prob:   54.376 prob_neg:   51.982 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:09:42,769 - 5:00:25 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    11 rec:   28.940 mi: 2.03580213 zkl:   60.236 cd:    2.147 pos_prob:   56.980 prob_neg:   54.833 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    11 rec:   28.940 mi: 2.03580213 zkl:   60.236 cd:    2.147 pos_prob:   56.980 prob_neg:   54.833 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:09:52,183 - 5:00:35 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    11 rec:   29.365 mi: 2.18156815 zkl:   63.467 cd:    3.251 pos_prob:   60.620 prob_neg:   57.369 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    11 rec:   29.365 mi: 2.18156815 zkl:   63.467 cd:    3.251 pos_prob:   60.620 prob_neg:   57.369 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:09:52,196 - 5:00:35 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-247.774
Langevin prior   1/ 40: energy=-247.774
2022-12-28 22:09:52,201 - 5:00:35 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-798.911
Langevin prior   6/ 40: energy=-798.911
2022-12-28 22:09:52,206 - 5:00:35 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1237.872
Langevin prior  11/ 40: energy=-1237.872
2022-12-28 22:09:52,211 - 5:00:35 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1382.250
Langevin prior  16/ 40: energy=-1382.250
2022-12-28 22:09:52,216 - 5:00:35 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1512.826
Langevin prior  21/ 40: energy=-1512.826
2022-12-28 22:09:52,223 - 5:00:35 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1607.534
Langevin prior  26/ 40: energy=-1607.534
2022-12-28 22:09:52,230 - 5:00:35 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1705.567
Langevin prior  31/ 40: energy=-1705.567
2022-12-28 22:09:52,238 - 5:00:35 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1711.852
Langevin prior  36/ 40: energy=-1711.852
2022-12-28 22:09:52,246 - 5:00:35 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1720.315
Langevin prior  40/ 40: energy=-1720.315
2022-12-28 22:19:37,384 - 5:10:20 - 585.1s - INFO - root - Negative Log-likehood -142.745322
200 done. -124.04729033151547
400 done. -120.17660625206996
600 done. -119.78279567982759
800 done. -120.60757932129292
1000 done. -119.75823558756713
1200 done. -120.33009235492042
1400 done. -121.17316428364308
1600 done. -119.79069401204345
1800 done. -119.45463968391422
2000 done. -132.98076644135367
2200 done. -141.87347821978005
2400 done. -151.59740236288866
2600 done. -159.73921743963209
2800 done. -165.28023456781648
3000 done. -172.29551219876205
3200 done. -176.71909978732512
3400 done. -182.16967936030437
3600 done. -185.52732310387452
3800 done. -189.62034215634336
4000 done. -192.69013753788752
4200 done. -185.19658756187098
4400 done. -177.53995014328703
4600 done. -170.7387548973578
4800 done. -164.4540408943068
5000 done. -158.83993503975228
5200 done. -153.53942358868602
5400 done. -148.71661029215738
5600 done. -144.23567109164716
Negative Log-likehood -142.745322
2022-12-28 22:19:37,385 - 5:10:20 - 0.0s - INFO - root - log-likelihood:   -142.745
log-likelihood:   -142.745
2022-12-28 22:20:25,058 - 5:11:08 - 47.7s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-011-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 22:20:25,079 - 5:11:08 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-011-test-greedy.txt
Generation: 188 batches
2022-12-28 22:20:30,842 - 5:11:14 - 5.8s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:20:32,623 - 5:11:15 - 1.8s - INFO - root - Generation Done
Generation Done
2022-12-28 22:20:32,623 - 5:11:15 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:20:35,063 - 5:11:18 - 2.4s - INFO - root - --- bleu: BLEU = 29.10, 52.1/30.8/24.6/21.6 (BP=0.958, ratio=0.959, hyp_len=162790, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.10, 52.1/30.8/24.6/21.6 (BP=0.958, ratio=0.959, hyp_len=162790, ref_len=169777)
--- bleu: BLEU = 29.10, 52.1/30.8/24.6/21.6 (BP=0.958, ratio=0.959, hyp_len=162790, ref_len=169777)

2022-12-28 22:20:35,063 - 5:11:18 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:20:35,065 - 5:11:18 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:20:44,626 - 5:11:27 - 9.6s - INFO - root - batch/max_batch/ep:   100/   529/    12 rec:   27.528 mi: 2.07591319 zkl:   61.489 cd:    2.654 pos_prob:   62.085 prob_neg:   59.431 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    12 rec:   27.528 mi: 2.07591319 zkl:   61.489 cd:    2.654 pos_prob:   62.085 prob_neg:   59.431 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:20:53,936 - 5:11:37 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    12 rec:   28.911 mi: 2.09470296 zkl:   59.883 cd:   -2.275 pos_prob:   59.717 prob_neg:   61.992 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    12 rec:   28.911 mi: 2.09470296 zkl:   59.883 cd:   -2.275 pos_prob:   59.717 prob_neg:   61.992 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:21:03,375 - 5:11:46 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    12 rec:   31.125 mi: 2.03385758 zkl:   61.220 cd:    4.720 pos_prob:   65.321 prob_neg:   60.601 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    12 rec:   31.125 mi: 2.03385758 zkl:   61.220 cd:    4.720 pos_prob:   65.321 prob_neg:   60.601 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:21:12,920 - 5:11:56 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    12 rec:   30.512 mi: 1.81911290 zkl:   54.079 cd:    1.596 pos_prob:   65.349 prob_neg:   63.753 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    12 rec:   30.512 mi: 1.81911290 zkl:   54.079 cd:    1.596 pos_prob:   65.349 prob_neg:   63.753 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:21:22,478 - 5:12:05 - 9.6s - INFO - root - batch/max_batch/ep:   500/   529/    12 rec:   42.109 mi: 1.94254375 zkl:   63.273 cd:    0.167 pos_prob:   68.571 prob_neg:   68.403 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    12 rec:   42.109 mi: 1.94254375 zkl:   63.273 cd:    0.167 pos_prob:   68.571 prob_neg:   68.403 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:21:22,490 - 5:12:05 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-210.805
Langevin prior   1/ 40: energy=-210.805
2022-12-28 22:21:22,496 - 5:12:05 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-937.358
Langevin prior   6/ 40: energy=-937.358
2022-12-28 22:21:22,502 - 5:12:05 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1555.432
Langevin prior  11/ 40: energy=-1555.432
2022-12-28 22:21:22,507 - 5:12:05 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1854.673
Langevin prior  16/ 40: energy=-1854.673
2022-12-28 22:21:22,515 - 5:12:05 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1978.900
Langevin prior  21/ 40: energy=-1978.900
2022-12-28 22:21:22,523 - 5:12:05 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1987.624
Langevin prior  26/ 40: energy=-1987.624
2022-12-28 22:21:22,532 - 5:12:05 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2047.467
Langevin prior  31/ 40: energy=-2047.467
2022-12-28 22:21:22,542 - 5:12:05 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2041.942
Langevin prior  36/ 40: energy=-2041.942
2022-12-28 22:21:22,550 - 5:12:05 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2053.386
Langevin prior  40/ 40: energy=-2053.386
2022-12-28 22:31:07,902 - 5:21:51 - 585.4s - INFO - root - Negative Log-likehood -144.629132
200 done. -126.44681602415528
400 done. -122.22355703477926
600 done. -121.82881784950348
800 done. -122.6524537167147
1000 done. -121.78634411174035
1200 done. -122.39670507699199
1400 done. -123.26884000233017
1600 done. -121.90265085493643
1800 done. -121.5268004235769
2000 done. -135.10918044856356
2200 done. -144.14620765811938
2400 done. -153.94175672174282
2600 done. -162.0761945587743
2800 done. -167.73046812336395
3000 done. -174.9591771194256
3200 done. -179.44867141920207
3400 done. -184.93580719845156
3600 done. -188.3433638639412
3800 done. -192.4846719349343
4000 done. -195.5317016648305
4200 done. -187.88796015627972
4400 done. -180.08132090991379
4600 done. -173.15490928677414
4800 done. -166.75188674801785
5000 done. -161.02244966663088
5200 done. -155.6311839506291
5400 done. -150.72793530438912
5600 done. -146.15281134269904
Negative Log-likehood -144.629132
2022-12-28 22:31:07,902 - 5:21:51 - 0.0s - INFO - root - log-likelihood:   -144.629
log-likelihood:   -144.629
2022-12-28 22:31:55,327 - 5:22:38 - 47.4s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-012-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 22:31:55,348 - 5:22:38 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-012-test-greedy.txt
Generation: 188 batches
2022-12-28 22:32:01,092 - 5:22:44 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:32:02,621 - 5:22:45 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:32:02,621 - 5:22:45 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:32:05,081 - 5:22:48 - 2.5s - INFO - root - --- bleu: BLEU = 29.13, 51.6/30.5/24.5/21.6 (BP=0.964, ratio=0.964, hyp_len=163737, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.13, 51.6/30.5/24.5/21.6 (BP=0.964, ratio=0.964, hyp_len=163737, ref_len=169777)
--- bleu: BLEU = 29.13, 51.6/30.5/24.5/21.6 (BP=0.964, ratio=0.964, hyp_len=163737, ref_len=169777)

2022-12-28 22:32:05,082 - 5:22:48 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:32:05,083 - 5:22:48 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:32:14,306 - 5:22:57 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    13 rec:   31.762 mi: 2.02318501 zkl:  138.432 cd:  -13.960 pos_prob:   56.486 prob_neg:   70.446 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    13 rec:   31.762 mi: 2.02318501 zkl:  138.432 cd:  -13.960 pos_prob:   56.486 prob_neg:   70.446 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:32:24,081 - 5:23:07 - 9.8s - INFO - root - batch/max_batch/ep:   200/   529/    13 rec:   23.639 mi: 1.77197695 zkl:  143.395 cd:  -20.294 pos_prob:   49.675 prob_neg:   69.969 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    13 rec:   23.639 mi: 1.77197695 zkl:  143.395 cd:  -20.294 pos_prob:   49.675 prob_neg:   69.969 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:32:33,307 - 5:23:16 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    13 rec:   17.464 mi: 2.03055882 zkl:  146.501 cd:  -19.281 pos_prob:   48.517 prob_neg:   67.798 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    13 rec:   17.464 mi: 2.03055882 zkl:  146.501 cd:  -19.281 pos_prob:   48.517 prob_neg:   67.798 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:32:42,992 - 5:23:26 - 9.7s - INFO - root - batch/max_batch/ep:   400/   529/    13 rec:   29.443 mi: 2.09541368 zkl:  147.408 cd:  -23.815 pos_prob:   42.200 prob_neg:   66.015 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    13 rec:   29.443 mi: 2.09541368 zkl:  147.408 cd:  -23.815 pos_prob:   42.200 prob_neg:   66.015 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:32:52,176 - 5:23:35 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    13 rec:   23.132 mi: 2.08738828 zkl:  152.303 cd:  -21.879 pos_prob:   44.510 prob_neg:   66.389 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    13 rec:   23.132 mi: 2.08738828 zkl:  152.303 cd:  -21.879 pos_prob:   44.510 prob_neg:   66.389 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:32:52,189 - 5:23:35 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-235.916
Langevin prior   1/ 40: energy=-235.916
2022-12-28 22:32:52,194 - 5:23:35 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-866.330
Langevin prior   6/ 40: energy=-866.330
2022-12-28 22:32:52,200 - 5:23:35 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1404.836
Langevin prior  11/ 40: energy=-1404.836
2022-12-28 22:32:52,205 - 5:23:35 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1741.157
Langevin prior  16/ 40: energy=-1741.157
2022-12-28 22:32:52,211 - 5:23:35 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1857.801
Langevin prior  21/ 40: energy=-1857.801
2022-12-28 22:32:52,218 - 5:23:35 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1949.851
Langevin prior  26/ 40: energy=-1949.851
2022-12-28 22:32:52,226 - 5:23:35 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2000.574
Langevin prior  31/ 40: energy=-2000.574
2022-12-28 22:32:52,236 - 5:23:35 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2035.145
Langevin prior  36/ 40: energy=-2035.145
2022-12-28 22:32:52,244 - 5:23:35 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1963.729
Langevin prior  40/ 40: energy=-1963.729
2022-12-28 22:32:54,802 - 5:23:37 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-013-test-greedy.txt
Generation: 188 batches
2022-12-28 22:33:00,529 - 5:23:43 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:33:02,047 - 5:23:45 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:33:02,047 - 5:23:45 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:33:04,525 - 5:23:47 - 2.5s - INFO - root - --- bleu: BLEU = 31.19, 53.8/32.5/26.3/23.2 (BP=0.971, ratio=0.971, hyp_len=164852, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.19, 53.8/32.5/26.3/23.2 (BP=0.971, ratio=0.971, hyp_len=164852, ref_len=169777)
--- bleu: BLEU = 31.19, 53.8/32.5/26.3/23.2 (BP=0.971, ratio=0.971, hyp_len=164852, ref_len=169777)

2022-12-28 22:33:04,526 - 5:23:47 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:33:04,527 - 5:23:47 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:33:13,580 - 5:23:56 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    14 rec:   11.091 mi: 1.89298511 zkl:  156.670 cd:  -24.434 pos_prob:   43.920 prob_neg:   68.354 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    14 rec:   11.091 mi: 1.89298511 zkl:  156.670 cd:  -24.434 pos_prob:   43.920 prob_neg:   68.354 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:33:22,715 - 5:24:05 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    14 rec:   17.648 mi: 1.94253564 zkl:  156.498 cd:  -30.910 pos_prob:   38.785 prob_neg:   69.696 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    14 rec:   17.648 mi: 1.94253564 zkl:  156.498 cd:  -30.910 pos_prob:   38.785 prob_neg:   69.696 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:33:31,776 - 5:24:14 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    14 rec:   15.434 mi: 1.95563853 zkl:  158.859 cd:  -26.769 pos_prob:   42.094 prob_neg:   68.863 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    14 rec:   15.434 mi: 1.95563853 zkl:  158.859 cd:  -26.769 pos_prob:   42.094 prob_neg:   68.863 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:33:40,882 - 5:24:24 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    14 rec:   12.361 mi: 1.95798659 zkl:  159.343 cd:  -28.566 pos_prob:   40.135 prob_neg:   68.701 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    14 rec:   12.361 mi: 1.95798659 zkl:  159.343 cd:  -28.566 pos_prob:   40.135 prob_neg:   68.701 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:33:50,301 - 5:24:33 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    14 rec:   20.189 mi: 2.02576756 zkl:  164.812 cd:  -29.166 pos_prob:   37.309 prob_neg:   66.475 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    14 rec:   20.189 mi: 2.02576756 zkl:  164.812 cd:  -29.166 pos_prob:   37.309 prob_neg:   66.475 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:33:50,314 - 5:24:33 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-261.144
Langevin prior   1/ 40: energy=-261.144
2022-12-28 22:33:50,320 - 5:24:33 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-936.964
Langevin prior   6/ 40: energy=-936.964
2022-12-28 22:33:50,325 - 5:24:33 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1559.813
Langevin prior  11/ 40: energy=-1559.813
2022-12-28 22:33:50,330 - 5:24:33 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1877.296
Langevin prior  16/ 40: energy=-1877.296
2022-12-28 22:33:50,336 - 5:24:33 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1939.187
Langevin prior  21/ 40: energy=-1939.187
2022-12-28 22:33:50,343 - 5:24:33 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1984.355
Langevin prior  26/ 40: energy=-1984.355
2022-12-28 22:33:50,351 - 5:24:33 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2072.141
Langevin prior  31/ 40: energy=-2072.141
2022-12-28 22:33:50,360 - 5:24:33 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2047.574
Langevin prior  36/ 40: energy=-2047.574
2022-12-28 22:33:50,368 - 5:24:33 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2047.297
Langevin prior  40/ 40: energy=-2047.297
2022-12-28 22:33:52,987 - 5:24:36 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-014-test-greedy.txt
Generation: 188 batches
2022-12-28 22:33:58,699 - 5:24:41 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:34:00,196 - 5:24:43 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:34:00,196 - 5:24:43 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:34:02,634 - 5:24:45 - 2.4s - INFO - root - --- bleu: BLEU = 31.56, 55.6/33.9/27.6/24.4 (BP=0.940, ratio=0.942, hyp_len=159890, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.56, 55.6/33.9/27.6/24.4 (BP=0.940, ratio=0.942, hyp_len=159890, ref_len=169777)
--- bleu: BLEU = 31.56, 55.6/33.9/27.6/24.4 (BP=0.940, ratio=0.942, hyp_len=159890, ref_len=169777)

2022-12-28 22:34:02,634 - 5:24:45 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:34:02,636 - 5:24:45 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:34:11,988 - 5:24:55 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    15 rec:   19.050 mi: 1.95757997 zkl:  162.868 cd:  -30.328 pos_prob:   35.208 prob_neg:   65.535 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    15 rec:   19.050 mi: 1.95757997 zkl:  162.868 cd:  -30.328 pos_prob:   35.208 prob_neg:   65.535 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:34:21,307 - 5:25:04 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    15 rec:   16.281 mi: 2.04078603 zkl:  168.462 cd:  -27.187 pos_prob:   39.088 prob_neg:   66.274 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    15 rec:   16.281 mi: 2.04078603 zkl:  168.462 cd:  -27.187 pos_prob:   39.088 prob_neg:   66.274 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:34:30,966 - 5:25:14 - 9.7s - INFO - root - batch/max_batch/ep:   300/   529/    15 rec:   21.086 mi: 1.88763201 zkl:  161.142 cd:  -28.885 pos_prob:   34.363 prob_neg:   63.248 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    15 rec:   21.086 mi: 1.88763201 zkl:  161.142 cd:  -28.885 pos_prob:   34.363 prob_neg:   63.248 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:34:39,991 - 5:25:23 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    15 rec:   15.437 mi: 1.89741373 zkl:  163.000 cd:  -27.006 pos_prob:   38.924 prob_neg:   65.931 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    15 rec:   15.437 mi: 1.89741373 zkl:  163.000 cd:  -27.006 pos_prob:   38.924 prob_neg:   65.931 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:34:49,035 - 5:25:32 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/    15 rec:   19.957 mi: 1.92617548 zkl:  164.313 cd:  -34.703 pos_prob:   35.690 prob_neg:   70.393 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    15 rec:   19.957 mi: 1.92617548 zkl:  164.313 cd:  -34.703 pos_prob:   35.690 prob_neg:   70.393 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:34:49,047 - 5:25:32 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-265.993
Langevin prior   1/ 40: energy=-265.993
2022-12-28 22:34:49,053 - 5:25:32 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-873.071
Langevin prior   6/ 40: energy=-873.071
2022-12-28 22:34:49,059 - 5:25:32 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1403.091
Langevin prior  11/ 40: energy=-1403.091
2022-12-28 22:34:49,064 - 5:25:32 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1701.206
Langevin prior  16/ 40: energy=-1701.206
2022-12-28 22:34:49,071 - 5:25:32 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1853.146
Langevin prior  21/ 40: energy=-1853.146
2022-12-28 22:34:49,079 - 5:25:32 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2008.793
Langevin prior  26/ 40: energy=-2008.793
2022-12-28 22:34:49,087 - 5:25:32 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1922.521
Langevin prior  31/ 40: energy=-1922.521
2022-12-28 22:34:49,097 - 5:25:32 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1946.942
Langevin prior  36/ 40: energy=-1946.942
2022-12-28 22:34:49,106 - 5:25:32 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1975.261
Langevin prior  40/ 40: energy=-1975.261
2022-12-28 22:34:51,653 - 5:25:34 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-015-test-greedy.txt
Generation: 188 batches
2022-12-28 22:34:57,349 - 5:25:40 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:34:58,844 - 5:25:42 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:34:58,844 - 5:25:42 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:35:01,263 - 5:25:44 - 2.4s - INFO - root - --- bleu: BLEU = 31.60, 56.1/34.0/27.7/24.6 (BP=0.936, ratio=0.938, hyp_len=159237, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.60, 56.1/34.0/27.7/24.6 (BP=0.936, ratio=0.938, hyp_len=159237, ref_len=169777)
--- bleu: BLEU = 31.60, 56.1/34.0/27.7/24.6 (BP=0.936, ratio=0.938, hyp_len=159237, ref_len=169777)

2022-12-28 22:35:01,263 - 5:25:44 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:35:01,265 - 5:25:44 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:35:10,701 - 5:25:53 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    16 rec:   25.024 mi: 1.91495693 zkl:  118.095 cd:  -12.201 pos_prob:   58.555 prob_neg:   70.756 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    16 rec:   25.024 mi: 1.91495693 zkl:  118.095 cd:  -12.201 pos_prob:   58.555 prob_neg:   70.756 kl_weight:    0.062 do_ae_train: False
2022-12-28 22:35:20,732 - 5:26:03 - 10.0s - INFO - root - batch/max_batch/ep:   200/   529/    16 rec:   16.244 mi: 2.10908198 zkl:   90.153 cd:   -3.387 pos_prob:   67.784 prob_neg:   71.171 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    16 rec:   16.244 mi: 2.10908198 zkl:   90.153 cd:   -3.387 pos_prob:   67.784 prob_neg:   71.171 kl_weight:    0.125 do_ae_train: False
2022-12-28 22:35:30,572 - 5:26:13 - 9.8s - INFO - root - batch/max_batch/ep:   300/   529/    16 rec:   22.755 mi: 1.98710704 zkl:   81.171 cd:   -7.513 pos_prob:   64.957 prob_neg:   72.470 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    16 rec:   22.755 mi: 1.98710704 zkl:   81.171 cd:   -7.513 pos_prob:   64.957 prob_neg:   72.470 kl_weight:    0.188 do_ae_train: False
2022-12-28 22:35:40,338 - 5:26:23 - 9.8s - INFO - root - batch/max_batch/ep:   400/   529/    16 rec:   18.665 mi: 2.04983735 zkl:   70.955 cd:   -9.058 pos_prob:   63.519 prob_neg:   72.577 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    16 rec:   18.665 mi: 2.04983735 zkl:   70.955 cd:   -9.058 pos_prob:   63.519 prob_neg:   72.577 kl_weight:    0.251 do_ae_train: False
2022-12-28 22:35:49,983 - 5:26:33 - 9.6s - INFO - root - batch/max_batch/ep:   500/   529/    16 rec:   22.950 mi: 2.21796179 zkl:   73.329 cd:   -4.794 pos_prob:   65.788 prob_neg:   70.583 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    16 rec:   22.950 mi: 2.21796179 zkl:   73.329 cd:   -4.794 pos_prob:   65.788 prob_neg:   70.583 kl_weight:    0.314 do_ae_train: False
2022-12-28 22:35:49,995 - 5:26:33 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-296.326
Langevin prior   1/ 40: energy=-296.326
2022-12-28 22:35:50,001 - 5:26:33 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1127.641
Langevin prior   6/ 40: energy=-1127.641
2022-12-28 22:35:50,006 - 5:26:33 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1626.345
Langevin prior  11/ 40: energy=-1626.345
2022-12-28 22:35:50,011 - 5:26:33 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1900.879
Langevin prior  16/ 40: energy=-1900.879
2022-12-28 22:35:50,017 - 5:26:33 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2029.534
Langevin prior  21/ 40: energy=-2029.534
2022-12-28 22:35:50,023 - 5:26:33 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2088.663
Langevin prior  26/ 40: energy=-2088.663
2022-12-28 22:35:50,031 - 5:26:33 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2090.476
Langevin prior  31/ 40: energy=-2090.476
2022-12-28 22:35:50,040 - 5:26:33 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2152.186
Langevin prior  36/ 40: energy=-2152.186
2022-12-28 22:35:50,048 - 5:26:33 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2181.460
Langevin prior  40/ 40: energy=-2181.460
2022-12-28 22:35:52,852 - 5:26:36 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-016-test-greedy.txt
Generation: 188 batches
2022-12-28 22:35:58,538 - 5:26:41 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:36:00,037 - 5:26:43 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:36:00,037 - 5:26:43 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:36:02,712 - 5:26:45 - 2.7s - INFO - root - --- bleu: BLEU = 30.19, 54.3/32.8/26.6/23.5 (BP=0.930, ratio=0.932, hyp_len=158223, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.19, 54.3/32.8/26.6/23.5 (BP=0.930, ratio=0.932, hyp_len=158223, ref_len=169777)
--- bleu: BLEU = 30.19, 54.3/32.8/26.6/23.5 (BP=0.930, ratio=0.932, hyp_len=158223, ref_len=169777)

2022-12-28 22:36:02,712 - 5:26:45 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:36:02,714 - 5:26:45 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:36:12,369 - 5:26:55 - 9.7s - INFO - root - batch/max_batch/ep:   100/   529/    17 rec:   23.190 mi: 2.14502358 zkl:   69.612 cd:   -3.288 pos_prob:   69.070 prob_neg:   72.358 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    17 rec:   23.190 mi: 2.14502358 zkl:   69.612 cd:   -3.288 pos_prob:   69.070 prob_neg:   72.358 kl_weight:    0.396 do_ae_train: False
2022-12-28 22:36:22,025 - 5:27:05 - 9.7s - INFO - root - batch/max_batch/ep:   200/   529/    17 rec:   26.511 mi: 2.14212966 zkl:   68.073 cd:   -1.896 pos_prob:   69.123 prob_neg:   71.019 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    17 rec:   26.511 mi: 2.14212966 zkl:   68.073 cd:   -1.896 pos_prob:   69.123 prob_neg:   71.019 kl_weight:    0.459 do_ae_train: False
2022-12-28 22:36:31,772 - 5:27:14 - 9.7s - INFO - root - batch/max_batch/ep:   300/   529/    17 rec:   25.028 mi: 2.06334376 zkl:   65.112 cd:   -1.369 pos_prob:   70.510 prob_neg:   71.879 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    17 rec:   25.028 mi: 2.06334376 zkl:   65.112 cd:   -1.369 pos_prob:   70.510 prob_neg:   71.879 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:36:41,361 - 5:27:24 - 9.6s - INFO - root - batch/max_batch/ep:   400/   529/    17 rec:   22.906 mi: 2.11907911 zkl:   66.021 cd:    1.208 pos_prob:   73.222 prob_neg:   72.014 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    17 rec:   22.906 mi: 2.11907911 zkl:   66.021 cd:    1.208 pos_prob:   73.222 prob_neg:   72.014 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:36:50,735 - 5:27:33 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    17 rec:   27.374 mi: 2.20752740 zkl:   65.883 cd:   -0.469 pos_prob:   74.146 prob_neg:   74.614 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    17 rec:   27.374 mi: 2.20752740 zkl:   65.883 cd:   -0.469 pos_prob:   74.146 prob_neg:   74.614 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:36:50,747 - 5:27:33 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-281.808
Langevin prior   1/ 40: energy=-281.808
2022-12-28 22:36:50,753 - 5:27:33 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1086.480
Langevin prior   6/ 40: energy=-1086.480
2022-12-28 22:36:50,758 - 5:27:33 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1681.160
Langevin prior  11/ 40: energy=-1681.160
2022-12-28 22:36:50,764 - 5:27:33 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2021.306
Langevin prior  16/ 40: energy=-2021.306
2022-12-28 22:36:50,771 - 5:27:33 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2135.410
Langevin prior  21/ 40: energy=-2135.410
2022-12-28 22:36:50,779 - 5:27:33 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2259.413
Langevin prior  26/ 40: energy=-2259.413
2022-12-28 22:36:50,787 - 5:27:33 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2281.580
Langevin prior  31/ 40: energy=-2281.580
2022-12-28 22:36:50,796 - 5:27:33 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2298.138
Langevin prior  36/ 40: energy=-2298.138
2022-12-28 22:36:50,804 - 5:27:33 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2332.373
Langevin prior  40/ 40: energy=-2332.373
2022-12-28 22:46:36,230 - 5:37:19 - 585.4s - INFO - root - Negative Log-likehood -151.974555
200 done. -132.12204076395284
400 done. -127.98905658172959
600 done. -127.4994532886141
800 done. -128.23824878738802
1000 done. -127.43247206576798
1200 done. -128.03221318638847
1400 done. -128.99209660523593
1600 done. -127.47899559246567
1800 done. -127.08436396317038
2000 done. -141.7456311393962
2200 done. -151.41906332610262
2400 done. -161.83649874660236
2600 done. -170.40434676780896
2800 done. -176.62997259406927
3000 done. -184.26571850739418
3200 done. -189.01601936898362
3400 done. -194.84765818079504
3600 done. -198.4980446185204
3800 done. -202.89296873946415
4000 done. -206.08004624175055
4200 done. -198.02568111341535
4400 done. -189.75854078669758
4600 done. -182.3788755261327
4800 done. -175.56397259015245
5000 done. -169.45469209514965
5200 done. -163.70303040674224
5400 done. -158.46870285605488
5600 done. -153.59026875230302
Negative Log-likehood -151.974555
2022-12-28 22:46:36,231 - 5:37:19 - 0.0s - INFO - root - log-likelihood:   -151.975
log-likelihood:   -151.975
2022-12-28 22:47:23,767 - 5:38:06 - 47.5s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-017-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 22:47:23,788 - 5:38:06 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-017-test-greedy.txt
Generation: 188 batches
2022-12-28 22:47:29,518 - 5:38:12 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:47:31,061 - 5:38:14 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:47:31,061 - 5:38:14 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:47:33,591 - 5:38:16 - 2.5s - INFO - root - --- bleu: BLEU = 30.01, 52.1/31.4/25.5/22.5 (BP=0.964, ratio=0.964, hyp_len=163743, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.01, 52.1/31.4/25.5/22.5 (BP=0.964, ratio=0.964, hyp_len=163743, ref_len=169777)
--- bleu: BLEU = 30.01, 52.1/31.4/25.5/22.5 (BP=0.964, ratio=0.964, hyp_len=163743, ref_len=169777)

2022-12-28 22:47:33,592 - 5:38:16 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:47:33,593 - 5:38:16 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:47:42,966 - 5:38:26 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    18 rec:   17.369 mi: 2.15045190 zkl:   60.734 cd:   -4.333 pos_prob:   73.486 prob_neg:   77.819 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    18 rec:   17.369 mi: 2.15045190 zkl:   60.734 cd:   -4.333 pos_prob:   73.486 prob_neg:   77.819 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:47:52,219 - 5:38:35 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    18 rec:   22.236 mi: 2.18199992 zkl:   64.043 cd:   -2.138 pos_prob:   75.151 prob_neg:   77.289 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    18 rec:   22.236 mi: 2.18199992 zkl:   64.043 cd:   -2.138 pos_prob:   75.151 prob_neg:   77.289 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:48:01,455 - 5:38:44 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    18 rec:   21.915 mi: 2.08101821 zkl:   67.147 cd:    2.374 pos_prob:   78.705 prob_neg:   76.331 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    18 rec:   21.915 mi: 2.08101821 zkl:   67.147 cd:    2.374 pos_prob:   78.705 prob_neg:   76.331 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:48:10,857 - 5:38:54 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    18 rec:   28.861 mi: 2.15917611 zkl:   67.419 cd:    2.848 pos_prob:   77.571 prob_neg:   74.722 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    18 rec:   28.861 mi: 2.15917611 zkl:   67.419 cd:    2.848 pos_prob:   77.571 prob_neg:   74.722 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:48:20,470 - 5:39:03 - 9.6s - INFO - root - batch/max_batch/ep:   500/   529/    18 rec:   20.985 mi: 1.92441165 zkl:   65.540 cd:    4.538 pos_prob:   81.219 prob_neg:   76.680 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    18 rec:   20.985 mi: 1.92441165 zkl:   65.540 cd:    4.538 pos_prob:   81.219 prob_neg:   76.680 kl_weight:    0.500 do_ae_train: False
2022-12-28 22:48:20,483 - 5:39:03 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-329.761
Langevin prior   1/ 40: energy=-329.761
2022-12-28 22:48:20,489 - 5:39:03 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1094.701
Langevin prior   6/ 40: energy=-1094.701
2022-12-28 22:48:20,494 - 5:39:03 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1678.276
Langevin prior  11/ 40: energy=-1678.276
2022-12-28 22:48:20,500 - 5:39:03 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1923.149
Langevin prior  16/ 40: energy=-1923.149
2022-12-28 22:48:20,507 - 5:39:03 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2100.526
Langevin prior  21/ 40: energy=-2100.526
2022-12-28 22:48:20,515 - 5:39:03 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2198.782
Langevin prior  26/ 40: energy=-2198.782
2022-12-28 22:48:20,524 - 5:39:03 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2333.650
Langevin prior  31/ 40: energy=-2333.650
2022-12-28 22:48:20,534 - 5:39:03 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2263.472
Langevin prior  36/ 40: energy=-2263.472
2022-12-28 22:48:20,541 - 5:39:03 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2185.727
Langevin prior  40/ 40: energy=-2185.727
2022-12-28 22:58:06,594 - 5:48:49 - 586.1s - INFO - root - Negative Log-likehood -153.132468
200 done. -132.00537836643338
400 done. -127.87031495431465
600 done. -127.28383041054975
800 done. -128.08976557261877
1000 done. -127.31138755613975
1200 done. -127.85799734014984
1400 done. -128.80558275362958
1600 done. -127.35438776272905
1800 done. -126.92809298064005
2000 done. -141.74341831688145
2200 done. -151.66341934229752
2400 done. -162.38678661424348
2600 done. -171.25754315557467
2800 done. -177.57989987919203
3000 done. -185.53275912079363
3200 done. -190.37039695953777
3400 done. -196.47511257876533
3600 done. -200.29647476419765
3800 done. -204.79289789869335
4000 done. -208.00045348607773
4200 done. -199.80798314410725
4400 done. -191.42127312486917
4600 done. -183.95490373969002
4800 done. -177.0505387188911
5000 done. -170.85385507324816
5200 done. -165.02512914261254
5400 done. -159.71584499339727
5600 done. -154.76708156682673
Negative Log-likehood -153.132468
2022-12-28 22:58:06,595 - 5:48:49 - 0.0s - INFO - root - log-likelihood:   -153.132
log-likelihood:   -153.132
2022-12-28 22:58:54,689 - 5:49:37 - 48.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-018-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 22:58:54,710 - 5:49:37 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-018-test-greedy.txt
Generation: 188 batches
2022-12-28 22:59:00,434 - 5:49:43 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:59:01,942 - 5:49:45 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 22:59:01,942 - 5:49:45 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 22:59:04,373 - 5:49:47 - 2.4s - INFO - root - --- bleu: BLEU = 29.56, 52.5/31.6/25.7/22.8 (BP=0.941, ratio=0.943, hyp_len=160044, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.56, 52.5/31.6/25.7/22.8 (BP=0.941, ratio=0.943, hyp_len=160044, ref_len=169777)
--- bleu: BLEU = 29.56, 52.5/31.6/25.7/22.8 (BP=0.941, ratio=0.943, hyp_len=160044, ref_len=169777)

2022-12-28 22:59:04,374 - 5:49:47 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 22:59:04,375 - 5:49:47 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 22:59:13,588 - 5:49:56 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    19 rec:   13.182 mi: 2.02208424 zkl:  142.976 cd:  -12.712 pos_prob:   69.501 prob_neg:   82.212 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    19 rec:   13.182 mi: 2.02208424 zkl:  142.976 cd:  -12.712 pos_prob:   69.501 prob_neg:   82.212 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:59:22,726 - 5:50:05 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    19 rec:   16.440 mi: 1.93960285 zkl:  147.680 cd:  -16.880 pos_prob:   61.003 prob_neg:   77.883 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    19 rec:   16.440 mi: 1.93960285 zkl:  147.680 cd:  -16.880 pos_prob:   61.003 prob_neg:   77.883 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:59:32,082 - 5:50:15 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    19 rec:   13.769 mi: 2.01045775 zkl:  155.146 cd:  -20.189 pos_prob:   60.769 prob_neg:   80.958 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    19 rec:   13.769 mi: 2.01045775 zkl:  155.146 cd:  -20.189 pos_prob:   60.769 prob_neg:   80.958 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:59:41,131 - 5:50:24 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    19 rec:   18.587 mi: 2.03706217 zkl:  154.348 cd:  -22.229 pos_prob:   54.973 prob_neg:   77.201 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    19 rec:   18.587 mi: 2.03706217 zkl:  154.348 cd:  -22.229 pos_prob:   54.973 prob_neg:   77.201 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:59:50,188 - 5:50:33 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    19 rec:   13.275 mi: 1.94416475 zkl:  155.349 cd:  -34.446 pos_prob:   51.959 prob_neg:   86.405 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    19 rec:   13.275 mi: 1.94416475 zkl:  155.349 cd:  -34.446 pos_prob:   51.959 prob_neg:   86.405 kl_weight:    0.000 do_ae_train: True
2022-12-28 22:59:50,201 - 5:50:33 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-244.153
Langevin prior   1/ 40: energy=-244.153
2022-12-28 22:59:50,206 - 5:50:33 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1105.773
Langevin prior   6/ 40: energy=-1105.773
2022-12-28 22:59:50,211 - 5:50:33 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1737.118
Langevin prior  11/ 40: energy=-1737.118
2022-12-28 22:59:50,217 - 5:50:33 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2065.815
Langevin prior  16/ 40: energy=-2065.815
2022-12-28 22:59:50,224 - 5:50:33 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2212.551
Langevin prior  21/ 40: energy=-2212.551
2022-12-28 22:59:50,233 - 5:50:33 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2285.262
Langevin prior  26/ 40: energy=-2285.262
2022-12-28 22:59:50,242 - 5:50:33 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2309.053
Langevin prior  31/ 40: energy=-2309.053
2022-12-28 22:59:50,252 - 5:50:33 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2354.181
Langevin prior  36/ 40: energy=-2354.181
2022-12-28 22:59:50,261 - 5:50:33 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2314.297
Langevin prior  40/ 40: energy=-2314.297
2022-12-28 22:59:52,908 - 5:50:36 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-019-test-greedy.txt
Generation: 188 batches
2022-12-28 22:59:58,589 - 5:50:41 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:00:00,082 - 5:50:43 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:00:00,082 - 5:50:43 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:00:02,534 - 5:50:45 - 2.5s - INFO - root - --- bleu: BLEU = 31.25, 55.2/33.6/27.4/24.4 (BP=0.937, ratio=0.938, hyp_len=159329, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.25, 55.2/33.6/27.4/24.4 (BP=0.937, ratio=0.938, hyp_len=159329, ref_len=169777)
--- bleu: BLEU = 31.25, 55.2/33.6/27.4/24.4 (BP=0.937, ratio=0.938, hyp_len=159329, ref_len=169777)

2022-12-28 23:00:02,535 - 5:50:45 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:00:02,536 - 5:50:45 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:00:11,727 - 5:50:54 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    20 rec:   12.960 mi: 2.12368464 zkl:  157.498 cd:  -34.109 pos_prob:   47.602 prob_neg:   81.711 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    20 rec:   12.960 mi: 2.12368464 zkl:  157.498 cd:  -34.109 pos_prob:   47.602 prob_neg:   81.711 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:00:20,802 - 5:51:03 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    20 rec:   20.655 mi: 2.06106162 zkl:  159.963 cd:  -34.409 pos_prob:   45.570 prob_neg:   79.979 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    20 rec:   20.655 mi: 2.06106162 zkl:  159.963 cd:  -34.409 pos_prob:   45.570 prob_neg:   79.979 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:00:29,868 - 5:51:13 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    20 rec:   12.973 mi: 2.09393835 zkl:  162.554 cd:  -29.914 pos_prob:   47.453 prob_neg:   77.367 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    20 rec:   12.973 mi: 2.09393835 zkl:  162.554 cd:  -29.914 pos_prob:   47.453 prob_neg:   77.367 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:00:39,068 - 5:51:22 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    20 rec:   18.757 mi: 2.11357594 zkl:  164.277 cd:  -38.346 pos_prob:   42.521 prob_neg:   80.868 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    20 rec:   18.757 mi: 2.11357594 zkl:  164.277 cd:  -38.346 pos_prob:   42.521 prob_neg:   80.868 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:00:48,259 - 5:51:31 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    20 rec:   15.640 mi: 1.84378946 zkl:  161.104 cd:  -41.997 pos_prob:   41.247 prob_neg:   83.243 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    20 rec:   15.640 mi: 1.84378946 zkl:  161.104 cd:  -41.997 pos_prob:   41.247 prob_neg:   83.243 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:00:48,272 - 5:51:31 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-277.998
Langevin prior   1/ 40: energy=-277.998
2022-12-28 23:00:48,278 - 5:51:31 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1210.217
Langevin prior   6/ 40: energy=-1210.217
2022-12-28 23:00:48,283 - 5:51:31 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1789.670
Langevin prior  11/ 40: energy=-1789.670
2022-12-28 23:00:48,288 - 5:51:31 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2066.182
Langevin prior  16/ 40: energy=-2066.182
2022-12-28 23:00:48,294 - 5:51:31 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2243.449
Langevin prior  21/ 40: energy=-2243.449
2022-12-28 23:00:48,300 - 5:51:31 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2291.052
Langevin prior  26/ 40: energy=-2291.052
2022-12-28 23:00:48,306 - 5:51:31 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2348.860
Langevin prior  31/ 40: energy=-2348.860
2022-12-28 23:00:48,314 - 5:51:31 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2356.949
Langevin prior  36/ 40: energy=-2356.949
2022-12-28 23:00:48,321 - 5:51:31 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2373.025
Langevin prior  40/ 40: energy=-2373.025
2022-12-28 23:00:51,016 - 5:51:34 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-020-test-greedy.txt
Generation: 188 batches
2022-12-28 23:00:56,701 - 5:51:39 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:00:58,207 - 5:51:41 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:00:58,207 - 5:51:41 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:01:00,640 - 5:51:43 - 2.4s - INFO - root - --- bleu: BLEU = 31.51, 54.9/33.6/27.5/24.4 (BP=0.945, ratio=0.946, hyp_len=160689, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.51, 54.9/33.6/27.5/24.4 (BP=0.945, ratio=0.946, hyp_len=160689, ref_len=169777)
--- bleu: BLEU = 31.51, 54.9/33.6/27.5/24.4 (BP=0.945, ratio=0.946, hyp_len=160689, ref_len=169777)

2022-12-28 23:01:00,640 - 5:51:43 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:01:00,642 - 5:51:43 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:01:09,644 - 5:51:52 - 9.0s - INFO - root - batch/max_batch/ep:   100/   529/    21 rec:   10.469 mi: 2.07546663 zkl:  160.656 cd:  -30.650 pos_prob:   45.854 prob_neg:   76.504 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    21 rec:   10.469 mi: 2.07546663 zkl:  160.656 cd:  -30.650 pos_prob:   45.854 prob_neg:   76.504 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:01:18,755 - 5:52:01 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    21 rec:   16.623 mi: 2.09601068 zkl:  165.890 cd:  -35.902 pos_prob:   43.606 prob_neg:   79.508 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    21 rec:   16.623 mi: 2.09601068 zkl:  165.890 cd:  -35.902 pos_prob:   43.606 prob_neg:   79.508 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:01:27,846 - 5:52:11 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    21 rec:   15.592 mi: 1.99172390 zkl:  163.874 cd:  -36.084 pos_prob:   39.970 prob_neg:   76.054 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    21 rec:   15.592 mi: 1.99172390 zkl:  163.874 cd:  -36.084 pos_prob:   39.970 prob_neg:   76.054 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:01:37,702 - 5:52:20 - 9.9s - INFO - root - batch/max_batch/ep:   400/   529/    21 rec:   13.588 mi: 2.03879333 zkl:  164.297 cd:  -36.723 pos_prob:   44.445 prob_neg:   81.167 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    21 rec:   13.588 mi: 2.03879333 zkl:  164.297 cd:  -36.723 pos_prob:   44.445 prob_neg:   81.167 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:01:47,195 - 5:52:30 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    21 rec:   18.460 mi: 2.00365949 zkl:  167.371 cd:  -38.939 pos_prob:   39.954 prob_neg:   78.893 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    21 rec:   18.460 mi: 2.00365949 zkl:  167.371 cd:  -38.939 pos_prob:   39.954 prob_neg:   78.893 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:01:47,207 - 5:52:30 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-272.944
Langevin prior   1/ 40: energy=-272.944
2022-12-28 23:01:47,213 - 5:52:30 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1086.476
Langevin prior   6/ 40: energy=-1086.476
2022-12-28 23:01:47,218 - 5:52:30 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1726.291
Langevin prior  11/ 40: energy=-1726.291
2022-12-28 23:01:47,224 - 5:52:30 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2093.395
Langevin prior  16/ 40: energy=-2093.395
2022-12-28 23:01:47,229 - 5:52:30 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2255.569
Langevin prior  21/ 40: energy=-2255.569
2022-12-28 23:01:47,235 - 5:52:30 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2326.644
Langevin prior  26/ 40: energy=-2326.644
2022-12-28 23:01:47,242 - 5:52:30 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2287.842
Langevin prior  31/ 40: energy=-2287.842
2022-12-28 23:01:47,250 - 5:52:30 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2400.466
Langevin prior  36/ 40: energy=-2400.466
2022-12-28 23:01:47,257 - 5:52:30 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2393.048
Langevin prior  40/ 40: energy=-2393.048
2022-12-28 23:01:49,869 - 5:52:33 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-021-test-greedy.txt
Generation: 188 batches
2022-12-28 23:01:55,578 - 5:52:38 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:01:57,086 - 5:52:40 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:01:57,086 - 5:52:40 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:01:59,565 - 5:52:42 - 2.5s - INFO - root - --- bleu: BLEU = 31.38, 54.6/33.1/26.9/23.8 (BP=0.956, ratio=0.957, hyp_len=162454, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.38, 54.6/33.1/26.9/23.8 (BP=0.956, ratio=0.957, hyp_len=162454, ref_len=169777)
--- bleu: BLEU = 31.38, 54.6/33.1/26.9/23.8 (BP=0.956, ratio=0.957, hyp_len=162454, ref_len=169777)

2022-12-28 23:01:59,566 - 5:52:42 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:01:59,567 - 5:52:42 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:02:08,745 - 5:52:51 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    22 rec:   13.916 mi: 2.04046082 zkl:   89.022 cd:  -15.639 pos_prob:   61.562 prob_neg:   77.201 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    22 rec:   13.916 mi: 2.04046082 zkl:   89.022 cd:  -15.639 pos_prob:   61.562 prob_neg:   77.201 kl_weight:    0.062 do_ae_train: False
2022-12-28 23:02:18,011 - 5:53:01 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    22 rec:   15.168 mi: 2.15014791 zkl:   79.447 cd:   -6.049 pos_prob:   64.094 prob_neg:   70.143 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    22 rec:   15.168 mi: 2.15014791 zkl:   79.447 cd:   -6.049 pos_prob:   64.094 prob_neg:   70.143 kl_weight:    0.125 do_ae_train: False
2022-12-28 23:02:27,489 - 5:53:10 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    22 rec:   18.496 mi: 2.11617017 zkl:   79.593 cd:   -4.982 pos_prob:   69.909 prob_neg:   74.891 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    22 rec:   18.496 mi: 2.11617017 zkl:   79.593 cd:   -4.982 pos_prob:   69.909 prob_neg:   74.891 kl_weight:    0.188 do_ae_train: False
2022-12-28 23:02:36,804 - 5:53:20 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    22 rec:   17.764 mi: 2.02184796 zkl:   69.874 cd:   -2.186 pos_prob:   64.307 prob_neg:   66.492 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    22 rec:   17.764 mi: 2.02184796 zkl:   69.874 cd:   -2.186 pos_prob:   64.307 prob_neg:   66.492 kl_weight:    0.251 do_ae_train: False
2022-12-28 23:02:46,128 - 5:53:29 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    22 rec:   21.197 mi: 1.96458650 zkl:   71.906 cd:   -3.834 pos_prob:   64.019 prob_neg:   67.853 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    22 rec:   21.197 mi: 1.96458650 zkl:   71.906 cd:   -3.834 pos_prob:   64.019 prob_neg:   67.853 kl_weight:    0.314 do_ae_train: False
2022-12-28 23:02:46,140 - 5:53:29 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-222.570
Langevin prior   1/ 40: energy=-222.570
2022-12-28 23:02:46,146 - 5:53:29 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1025.667
Langevin prior   6/ 40: energy=-1025.667
2022-12-28 23:02:46,151 - 5:53:29 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1614.105
Langevin prior  11/ 40: energy=-1614.105
2022-12-28 23:02:46,157 - 5:53:29 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1820.194
Langevin prior  16/ 40: energy=-1820.194
2022-12-28 23:02:46,163 - 5:53:29 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1927.964
Langevin prior  21/ 40: energy=-1927.964
2022-12-28 23:02:46,172 - 5:53:29 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1993.994
Langevin prior  26/ 40: energy=-1993.994
2022-12-28 23:02:46,183 - 5:53:29 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1990.960
Langevin prior  31/ 40: energy=-1990.960
2022-12-28 23:02:46,193 - 5:53:29 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2070.193
Langevin prior  36/ 40: energy=-2070.193
2022-12-28 23:02:46,202 - 5:53:29 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2076.192
Langevin prior  40/ 40: energy=-2076.192
2022-12-28 23:02:48,982 - 5:53:32 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-022-test-greedy.txt
Generation: 188 batches
2022-12-28 23:02:54,672 - 5:53:37 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:02:56,185 - 5:53:39 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:02:56,185 - 5:53:39 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:02:58,632 - 5:53:41 - 2.4s - INFO - root - --- bleu: BLEU = 30.28, 52.9/31.9/25.8/22.9 (BP=0.958, ratio=0.959, hyp_len=162746, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.28, 52.9/31.9/25.8/22.9 (BP=0.958, ratio=0.959, hyp_len=162746, ref_len=169777)
--- bleu: BLEU = 30.28, 52.9/31.9/25.8/22.9 (BP=0.958, ratio=0.959, hyp_len=162746, ref_len=169777)

2022-12-28 23:02:58,633 - 5:53:41 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:02:58,634 - 5:53:41 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:03:07,947 - 5:53:51 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    23 rec:   19.392 mi: 1.94238091 zkl:   71.728 cd:    2.997 pos_prob:   71.757 prob_neg:   68.760 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    23 rec:   19.392 mi: 1.94238091 zkl:   71.728 cd:    2.997 pos_prob:   71.757 prob_neg:   68.760 kl_weight:    0.396 do_ae_train: False
2022-12-28 23:03:17,482 - 5:54:00 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    23 rec:   16.303 mi: 1.84989750 zkl:   71.101 cd:    3.148 pos_prob:   74.896 prob_neg:   71.748 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    23 rec:   16.303 mi: 1.84989750 zkl:   71.101 cd:    3.148 pos_prob:   74.896 prob_neg:   71.748 kl_weight:    0.459 do_ae_train: False
2022-12-28 23:03:27,254 - 5:54:10 - 9.8s - INFO - root - batch/max_batch/ep:   300/   529/    23 rec:   20.578 mi: 2.10884786 zkl:   65.618 cd:    1.311 pos_prob:   73.585 prob_neg:   72.274 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    23 rec:   20.578 mi: 2.10884786 zkl:   65.618 cd:    1.311 pos_prob:   73.585 prob_neg:   72.274 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:03:36,482 - 5:54:19 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    23 rec:   21.040 mi: 1.86647153 zkl:   68.401 cd:   -1.422 pos_prob:   76.022 prob_neg:   77.443 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    23 rec:   21.040 mi: 1.86647153 zkl:   68.401 cd:   -1.422 pos_prob:   76.022 prob_neg:   77.443 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:03:45,915 - 5:54:29 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    23 rec:   18.836 mi: 2.23742199 zkl:   61.588 cd:    0.932 pos_prob:   71.632 prob_neg:   70.701 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    23 rec:   18.836 mi: 2.23742199 zkl:   61.588 cd:    0.932 pos_prob:   71.632 prob_neg:   70.701 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:03:45,928 - 5:54:29 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-269.994
Langevin prior   1/ 40: energy=-269.994
2022-12-28 23:03:45,934 - 5:54:29 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1112.806
Langevin prior   6/ 40: energy=-1112.806
2022-12-28 23:03:45,939 - 5:54:29 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1598.195
Langevin prior  11/ 40: energy=-1598.195
2022-12-28 23:03:45,944 - 5:54:29 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1891.906
Langevin prior  16/ 40: energy=-1891.906
2022-12-28 23:03:45,950 - 5:54:29 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2051.316
Langevin prior  21/ 40: energy=-2051.316
2022-12-28 23:03:45,956 - 5:54:29 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2122.212
Langevin prior  26/ 40: energy=-2122.212
2022-12-28 23:03:45,964 - 5:54:29 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2050.328
Langevin prior  31/ 40: energy=-2050.328
2022-12-28 23:03:45,972 - 5:54:29 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2146.835
Langevin prior  36/ 40: energy=-2146.835
2022-12-28 23:03:45,981 - 5:54:29 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2155.417
Langevin prior  40/ 40: energy=-2155.417
2022-12-28 23:13:31,753 - 6:04:14 - 585.8s - INFO - root - Negative Log-likehood -159.191202
200 done. -138.2608287261713
400 done. -133.71768758485175
600 done. -133.20279932976712
800 done. -133.93938219679794
1000 done. -133.03937674742468
1200 done. -133.64469963579265
1400 done. -134.6105961987762
1600 done. -133.0982964957993
1800 done. -132.62999897497272
2000 done. -147.82243331150417
2200 done. -158.02418406466805
2400 done. -169.06855846886666
2600 done. -178.18970221095782
2800 done. -184.5773185083287
3000 done. -192.6644071889014
3200 done. -197.6356670000801
3400 done. -204.00556454919737
3600 done. -207.90244785618117
3800 done. -212.44651244999426
4000 done. -215.81608772885536
4200 done. -207.39677861329747
4400 done. -198.7477592877213
4600 done. -191.02932632223403
4800 done. -183.88181694107405
5000 done. -177.4928426732037
5200 done. -171.4656125591806
5400 done. -165.99336394815015
5600 done. -160.8826999442518
Negative Log-likehood -159.191202
2022-12-28 23:13:31,753 - 6:04:14 - 0.0s - INFO - root - log-likelihood:   -159.191
log-likelihood:   -159.191
2022-12-28 23:14:18,677 - 6:05:01 - 46.9s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-023-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 23:14:18,679 - 6:05:01 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-023-test-greedy.txt
Generation: 188 batches
2022-12-28 23:14:24,378 - 6:05:07 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:14:25,894 - 6:05:09 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:14:25,894 - 6:05:09 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:14:28,579 - 6:05:11 - 2.7s - INFO - root - --- bleu: BLEU = 29.89, 52.7/31.7/25.8/22.8 (BP=0.949, ratio=0.950, hyp_len=161313, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.89, 52.7/31.7/25.8/22.8 (BP=0.949, ratio=0.950, hyp_len=161313, ref_len=169777)
--- bleu: BLEU = 29.89, 52.7/31.7/25.8/22.8 (BP=0.949, ratio=0.950, hyp_len=161313, ref_len=169777)

2022-12-28 23:14:28,580 - 6:05:11 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:14:28,581 - 6:05:11 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:14:38,322 - 6:05:21 - 9.7s - INFO - root - batch/max_batch/ep:   100/   529/    24 rec:   17.025 mi: 1.92490423 zkl:   64.139 cd:    0.677 pos_prob:   71.364 prob_neg:   70.687 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    24 rec:   17.025 mi: 1.92490423 zkl:   64.139 cd:    0.677 pos_prob:   71.364 prob_neg:   70.687 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:14:47,793 - 6:05:30 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    24 rec:   16.742 mi: 1.99188292 zkl:   66.676 cd:   -0.475 pos_prob:   74.233 prob_neg:   74.708 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    24 rec:   16.742 mi: 1.99188292 zkl:   66.676 cd:   -0.475 pos_prob:   74.233 prob_neg:   74.708 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:14:57,153 - 6:05:40 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    24 rec:   17.447 mi: 2.12568188 zkl:   61.349 cd:   -0.016 pos_prob:   74.568 prob_neg:   74.584 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    24 rec:   17.447 mi: 2.12568188 zkl:   61.349 cd:   -0.016 pos_prob:   74.568 prob_neg:   74.584 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:15:06,469 - 6:05:49 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    24 rec:   23.653 mi: 2.08863640 zkl:   63.789 cd:   -2.318 pos_prob:   75.200 prob_neg:   77.518 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    24 rec:   23.653 mi: 2.08863640 zkl:   63.789 cd:   -2.318 pos_prob:   75.200 prob_neg:   77.518 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:15:16,008 - 6:05:59 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    24 rec:   23.268 mi: 2.03366303 zkl:   64.648 cd:    2.376 pos_prob:   77.000 prob_neg:   74.624 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    24 rec:   23.268 mi: 2.03366303 zkl:   64.648 cd:    2.376 pos_prob:   77.000 prob_neg:   74.624 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:15:16,020 - 6:05:59 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-275.801
Langevin prior   1/ 40: energy=-275.801
2022-12-28 23:15:16,026 - 6:05:59 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1123.749
Langevin prior   6/ 40: energy=-1123.749
2022-12-28 23:15:16,031 - 6:05:59 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1788.340
Langevin prior  11/ 40: energy=-1788.340
2022-12-28 23:15:16,037 - 6:05:59 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2073.956
Langevin prior  16/ 40: energy=-2073.956
2022-12-28 23:15:16,043 - 6:05:59 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2256.106
Langevin prior  21/ 40: energy=-2256.106
2022-12-28 23:15:16,051 - 6:05:59 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2353.599
Langevin prior  26/ 40: energy=-2353.599
2022-12-28 23:15:16,059 - 6:05:59 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2351.147
Langevin prior  31/ 40: energy=-2351.147
2022-12-28 23:15:16,068 - 6:05:59 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2344.282
Langevin prior  36/ 40: energy=-2344.282
2022-12-28 23:15:16,077 - 6:05:59 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2362.584
Langevin prior  40/ 40: energy=-2362.584
2022-12-28 23:25:01,507 - 6:15:44 - 585.4s - INFO - root - Negative Log-likehood -160.439566
200 done. -138.90175309625235
400 done. -134.37812000789773
600 done. -133.70122790677476
800 done. -134.2922439966921
1000 done. -133.38947711023718
1200 done. -134.00831266026702
1400 done. -135.0044422576989
1600 done. -133.4692192549232
1800 done. -133.03744708801148
2000 done. -148.63419874153408
2200 done. -158.87274300240063
2400 done. -169.99539998768037
2600 done. -179.27160732077368
2800 done. -185.70474024276996
3000 done. -193.91699682003835
3200 done. -198.9330916066063
3400 done. -205.34764097085744
3600 done. -209.24005336977254
3800 done. -213.8797938254866
4000 done. -217.2657165692879
4200 done. -208.84862820586977
4400 done. -200.1790925555468
4600 done. -192.4220746587694
4800 done. -185.2423172919541
5000 done. -178.82834666853358
5200 done. -172.76072292744695
5400 done. -167.26136473266237
5600 done. -162.13712578712511
Negative Log-likehood -160.439566
2022-12-28 23:25:01,507 - 6:15:44 - 0.0s - INFO - root - log-likelihood:   -160.440
log-likelihood:   -160.440
2022-12-28 23:25:48,562 - 6:16:31 - 47.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-024-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 23:25:48,565 - 6:16:31 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-024-test-greedy.txt
Generation: 188 batches
2022-12-28 23:25:54,276 - 6:16:37 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:25:55,800 - 6:16:38 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:25:55,800 - 6:16:38 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:25:58,258 - 6:16:41 - 2.5s - INFO - root - --- bleu: BLEU = 29.34, 52.0/31.3/25.4/22.5 (BP=0.944, ratio=0.946, hyp_len=160594, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.34, 52.0/31.3/25.4/22.5 (BP=0.944, ratio=0.946, hyp_len=160594, ref_len=169777)
--- bleu: BLEU = 29.34, 52.0/31.3/25.4/22.5 (BP=0.944, ratio=0.946, hyp_len=160594, ref_len=169777)

2022-12-28 23:25:58,258 - 6:16:41 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:25:58,260 - 6:16:41 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:26:07,456 - 6:16:50 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    25 rec:   15.157 mi: 2.08980322 zkl:  143.942 cd:  -11.345 pos_prob:   63.272 prob_neg:   74.618 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    25 rec:   15.157 mi: 2.08980322 zkl:  143.942 cd:  -11.345 pos_prob:   63.272 prob_neg:   74.618 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:26:16,984 - 6:17:00 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    25 rec:   13.861 mi: 2.07490706 zkl:  148.856 cd:  -21.111 pos_prob:   58.336 prob_neg:   79.448 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    25 rec:   13.861 mi: 2.07490706 zkl:  148.856 cd:  -21.111 pos_prob:   58.336 prob_neg:   79.448 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:26:26,249 - 6:17:09 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    25 rec:    9.240 mi: 2.01055217 zkl:  157.300 cd:  -18.405 pos_prob:   58.277 prob_neg:   76.681 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    25 rec:    9.240 mi: 2.01055217 zkl:  157.300 cd:  -18.405 pos_prob:   58.277 prob_neg:   76.681 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:26:35,321 - 6:17:18 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    25 rec:   14.962 mi: 2.12716508 zkl:  157.075 cd:  -21.468 pos_prob:   54.923 prob_neg:   76.391 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    25 rec:   14.962 mi: 2.12716508 zkl:  157.075 cd:  -21.468 pos_prob:   54.923 prob_neg:   76.391 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:26:44,985 - 6:17:28 - 9.7s - INFO - root - batch/max_batch/ep:   500/   529/    25 rec:   16.317 mi: 1.96601343 zkl:  157.566 cd:  -25.103 pos_prob:   51.687 prob_neg:   76.790 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    25 rec:   16.317 mi: 1.96601343 zkl:  157.566 cd:  -25.103 pos_prob:   51.687 prob_neg:   76.790 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:26:44,998 - 6:17:28 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-330.220
Langevin prior   1/ 40: energy=-330.220
2022-12-28 23:26:45,004 - 6:17:28 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1184.420
Langevin prior   6/ 40: energy=-1184.420
2022-12-28 23:26:45,009 - 6:17:28 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1799.119
Langevin prior  11/ 40: energy=-1799.119
2022-12-28 23:26:45,014 - 6:17:28 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2074.178
Langevin prior  16/ 40: energy=-2074.178
2022-12-28 23:26:45,020 - 6:17:28 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2182.414
Langevin prior  21/ 40: energy=-2182.414
2022-12-28 23:26:45,026 - 6:17:28 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2238.301
Langevin prior  26/ 40: energy=-2238.301
2022-12-28 23:26:45,033 - 6:17:28 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2200.586
Langevin prior  31/ 40: energy=-2200.586
2022-12-28 23:26:45,041 - 6:17:28 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2255.210
Langevin prior  36/ 40: energy=-2255.210
2022-12-28 23:26:45,048 - 6:17:28 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2278.250
Langevin prior  40/ 40: energy=-2278.250
2022-12-28 23:26:47,605 - 6:17:30 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-025-test-greedy.txt
Generation: 188 batches
2022-12-28 23:26:53,305 - 6:17:36 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:26:54,816 - 6:17:38 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:26:54,817 - 6:17:38 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:26:57,297 - 6:17:40 - 2.5s - INFO - root - --- bleu: BLEU = 31.14, 54.0/32.8/26.7/23.6 (BP=0.958, ratio=0.959, hyp_len=162764, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.14, 54.0/32.8/26.7/23.6 (BP=0.958, ratio=0.959, hyp_len=162764, ref_len=169777)
--- bleu: BLEU = 31.14, 54.0/32.8/26.7/23.6 (BP=0.958, ratio=0.959, hyp_len=162764, ref_len=169777)

2022-12-28 23:26:57,298 - 6:17:40 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:26:57,299 - 6:17:40 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:27:06,860 - 6:17:50 - 9.6s - INFO - root - batch/max_batch/ep:   100/   529/    26 rec:   12.364 mi: 2.13873625 zkl:  159.577 cd:  -28.940 pos_prob:   48.627 prob_neg:   77.567 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    26 rec:   12.364 mi: 2.13873625 zkl:  159.577 cd:  -28.940 pos_prob:   48.627 prob_neg:   77.567 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:27:15,966 - 6:17:59 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    26 rec:   14.765 mi: 1.98956239 zkl:  160.255 cd:  -32.986 pos_prob:   40.186 prob_neg:   73.173 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    26 rec:   14.765 mi: 1.98956239 zkl:  160.255 cd:  -32.986 pos_prob:   40.186 prob_neg:   73.173 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:27:25,407 - 6:18:08 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    26 rec:   18.343 mi: 2.13161039 zkl:  160.232 cd:  -34.747 pos_prob:   42.177 prob_neg:   76.924 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    26 rec:   18.343 mi: 2.13161039 zkl:  160.232 cd:  -34.747 pos_prob:   42.177 prob_neg:   76.924 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:27:34,593 - 6:18:17 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    26 rec:   17.189 mi: 2.01153970 zkl:  164.619 cd:  -37.392 pos_prob:   42.279 prob_neg:   79.671 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    26 rec:   17.189 mi: 2.01153970 zkl:  164.619 cd:  -37.392 pos_prob:   42.279 prob_neg:   79.671 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:27:43,960 - 6:18:27 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    26 rec:   13.739 mi: 2.07642031 zkl:  165.288 cd:  -32.505 pos_prob:   45.223 prob_neg:   77.728 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    26 rec:   13.739 mi: 2.07642031 zkl:  165.288 cd:  -32.505 pos_prob:   45.223 prob_neg:   77.728 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:27:43,973 - 6:18:27 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-282.510
Langevin prior   1/ 40: energy=-282.510
2022-12-28 23:27:43,978 - 6:18:27 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1143.897
Langevin prior   6/ 40: energy=-1143.897
2022-12-28 23:27:43,984 - 6:18:27 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1632.189
Langevin prior  11/ 40: energy=-1632.189
2022-12-28 23:27:43,989 - 6:18:27 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1946.393
Langevin prior  16/ 40: energy=-1946.393
2022-12-28 23:27:43,996 - 6:18:27 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2135.182
Langevin prior  21/ 40: energy=-2135.182
2022-12-28 23:27:44,004 - 6:18:27 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2185.734
Langevin prior  26/ 40: energy=-2185.734
2022-12-28 23:27:44,012 - 6:18:27 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2234.788
Langevin prior  31/ 40: energy=-2234.788
2022-12-28 23:27:44,021 - 6:18:27 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2285.969
Langevin prior  36/ 40: energy=-2285.969
2022-12-28 23:27:44,029 - 6:18:27 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2360.621
Langevin prior  40/ 40: energy=-2360.621
2022-12-28 23:27:46,579 - 6:18:29 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-026-test-greedy.txt
Generation: 188 batches
2022-12-28 23:27:52,289 - 6:18:35 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:27:53,802 - 6:18:36 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:27:53,802 - 6:18:36 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:27:56,247 - 6:18:39 - 2.4s - INFO - root - --- bleu: BLEU = 31.41, 54.6/33.3/27.3/24.2 (BP=0.948, ratio=0.950, hyp_len=161232, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.41, 54.6/33.3/27.3/24.2 (BP=0.948, ratio=0.950, hyp_len=161232, ref_len=169777)
--- bleu: BLEU = 31.41, 54.6/33.3/27.3/24.2 (BP=0.948, ratio=0.950, hyp_len=161232, ref_len=169777)

2022-12-28 23:27:56,247 - 6:18:39 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:27:56,248 - 6:18:39 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:28:05,501 - 6:18:48 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    27 rec:   12.744 mi: 2.10435939 zkl:  161.944 cd:  -35.160 pos_prob:   40.791 prob_neg:   75.951 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    27 rec:   12.744 mi: 2.10435939 zkl:  161.944 cd:  -35.160 pos_prob:   40.791 prob_neg:   75.951 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:28:14,628 - 6:18:57 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    27 rec:   13.978 mi: 2.02908158 zkl:  162.761 cd:  -35.770 pos_prob:   43.254 prob_neg:   79.023 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    27 rec:   13.978 mi: 2.02908158 zkl:  162.761 cd:  -35.770 pos_prob:   43.254 prob_neg:   79.023 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:28:23,959 - 6:19:07 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    27 rec:   16.918 mi: 2.14069486 zkl:  164.987 cd:  -37.051 pos_prob:   41.151 prob_neg:   78.202 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    27 rec:   16.918 mi: 2.14069486 zkl:  164.987 cd:  -37.051 pos_prob:   41.151 prob_neg:   78.202 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:28:33,231 - 6:19:16 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    27 rec:   10.300 mi: 2.08271527 zkl:  159.826 cd:  -33.597 pos_prob:   41.809 prob_neg:   75.406 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    27 rec:   10.300 mi: 2.08271527 zkl:  159.826 cd:  -33.597 pos_prob:   41.809 prob_neg:   75.406 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:28:42,667 - 6:19:25 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    27 rec:   10.487 mi: 1.95583594 zkl:  168.058 cd:  -34.949 pos_prob:   44.869 prob_neg:   79.818 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    27 rec:   10.487 mi: 1.95583594 zkl:  168.058 cd:  -34.949 pos_prob:   44.869 prob_neg:   79.818 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:28:42,680 - 6:19:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-290.124
Langevin prior   1/ 40: energy=-290.124
2022-12-28 23:28:42,685 - 6:19:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1179.166
Langevin prior   6/ 40: energy=-1179.166
2022-12-28 23:28:42,690 - 6:19:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1768.810
Langevin prior  11/ 40: energy=-1768.810
2022-12-28 23:28:42,695 - 6:19:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2006.800
Langevin prior  16/ 40: energy=-2006.800
2022-12-28 23:28:42,701 - 6:19:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2301.153
Langevin prior  21/ 40: energy=-2301.153
2022-12-28 23:28:42,707 - 6:19:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2335.102
Langevin prior  26/ 40: energy=-2335.102
2022-12-28 23:28:42,713 - 6:19:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2345.445
Langevin prior  31/ 40: energy=-2345.445
2022-12-28 23:28:42,721 - 6:19:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2382.293
Langevin prior  36/ 40: energy=-2382.293
2022-12-28 23:28:42,729 - 6:19:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2377.326
Langevin prior  40/ 40: energy=-2377.326
2022-12-28 23:28:45,351 - 6:19:28 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-027-test-greedy.txt
Generation: 188 batches
2022-12-28 23:28:51,071 - 6:19:34 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:28:52,580 - 6:19:35 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:28:52,581 - 6:19:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:28:55,065 - 6:19:38 - 2.5s - INFO - root - --- bleu: BLEU = 31.36, 54.5/33.2/27.1/24.1 (BP=0.951, ratio=0.952, hyp_len=161686, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.36, 54.5/33.2/27.1/24.1 (BP=0.951, ratio=0.952, hyp_len=161686, ref_len=169777)
--- bleu: BLEU = 31.36, 54.5/33.2/27.1/24.1 (BP=0.951, ratio=0.952, hyp_len=161686, ref_len=169777)

2022-12-28 23:28:55,065 - 6:19:38 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:28:55,067 - 6:19:38 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:29:04,423 - 6:19:47 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    28 rec:   12.466 mi: 2.04772973 zkl:   97.326 cd:  -17.278 pos_prob:   58.535 prob_neg:   75.813 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    28 rec:   12.466 mi: 2.04772973 zkl:   97.326 cd:  -17.278 pos_prob:   58.535 prob_neg:   75.813 kl_weight:    0.062 do_ae_train: False
2022-12-28 23:29:14,035 - 6:19:57 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/    28 rec:   11.508 mi: 1.96462905 zkl:   88.687 cd:  -10.199 pos_prob:   65.678 prob_neg:   75.876 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    28 rec:   11.508 mi: 1.96462905 zkl:   88.687 cd:  -10.199 pos_prob:   65.678 prob_neg:   75.876 kl_weight:    0.125 do_ae_train: False
2022-12-28 23:29:23,483 - 6:20:06 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    28 rec:   15.975 mi: 2.15318537 zkl:   76.819 cd:  -11.621 pos_prob:   62.355 prob_neg:   73.976 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    28 rec:   15.975 mi: 2.15318537 zkl:   76.819 cd:  -11.621 pos_prob:   62.355 prob_neg:   73.976 kl_weight:    0.188 do_ae_train: False
2022-12-28 23:29:32,899 - 6:20:16 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    28 rec:   15.490 mi: 2.14845562 zkl:   71.331 cd:   -4.721 pos_prob:   67.478 prob_neg:   72.199 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    28 rec:   15.490 mi: 2.14845562 zkl:   71.331 cd:   -4.721 pos_prob:   67.478 prob_neg:   72.199 kl_weight:    0.251 do_ae_train: False
2022-12-28 23:29:42,298 - 6:20:25 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    28 rec:   20.485 mi: 2.03383160 zkl:   71.925 cd:   -6.875 pos_prob:   61.414 prob_neg:   68.289 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    28 rec:   20.485 mi: 2.03383160 zkl:   71.925 cd:   -6.875 pos_prob:   61.414 prob_neg:   68.289 kl_weight:    0.314 do_ae_train: False
2022-12-28 23:29:42,310 - 6:20:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-306.066
Langevin prior   1/ 40: energy=-306.066
2022-12-28 23:29:42,316 - 6:20:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1130.779
Langevin prior   6/ 40: energy=-1130.779
2022-12-28 23:29:42,321 - 6:20:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1650.320
Langevin prior  11/ 40: energy=-1650.320
2022-12-28 23:29:42,326 - 6:20:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1876.080
Langevin prior  16/ 40: energy=-1876.080
2022-12-28 23:29:42,332 - 6:20:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1926.588
Langevin prior  21/ 40: energy=-1926.588
2022-12-28 23:29:42,337 - 6:20:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2016.686
Langevin prior  26/ 40: energy=-2016.686
2022-12-28 23:29:42,343 - 6:20:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2087.974
Langevin prior  31/ 40: energy=-2087.974
2022-12-28 23:29:42,351 - 6:20:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2076.863
Langevin prior  36/ 40: energy=-2076.863
2022-12-28 23:29:42,358 - 6:20:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2143.528
Langevin prior  40/ 40: energy=-2143.528
2022-12-28 23:29:44,961 - 6:20:28 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-028-test-greedy.txt
Generation: 188 batches
2022-12-28 23:29:50,686 - 6:20:33 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:29:52,215 - 6:20:35 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:29:52,216 - 6:20:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:29:54,706 - 6:20:37 - 2.5s - INFO - root - --- bleu: BLEU = 30.23, 53.2/32.0/26.0/23.1 (BP=0.951, ratio=0.952, hyp_len=161593, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.23, 53.2/32.0/26.0/23.1 (BP=0.951, ratio=0.952, hyp_len=161593, ref_len=169777)
--- bleu: BLEU = 30.23, 53.2/32.0/26.0/23.1 (BP=0.951, ratio=0.952, hyp_len=161593, ref_len=169777)

2022-12-28 23:29:54,706 - 6:20:37 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:29:54,708 - 6:20:37 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:30:04,294 - 6:20:47 - 9.6s - INFO - root - batch/max_batch/ep:   100/   529/    29 rec:   19.033 mi: 2.11590266 zkl:   72.053 cd:   -0.259 pos_prob:   69.771 prob_neg:   70.031 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    29 rec:   19.033 mi: 2.11590266 zkl:   72.053 cd:   -0.259 pos_prob:   69.771 prob_neg:   70.031 kl_weight:    0.396 do_ae_train: False
2022-12-28 23:30:14,058 - 6:20:57 - 9.8s - INFO - root - batch/max_batch/ep:   200/   529/    29 rec:   18.501 mi: 2.20722365 zkl:   61.145 cd:   -3.169 pos_prob:   65.205 prob_neg:   68.373 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    29 rec:   18.501 mi: 2.20722365 zkl:   61.145 cd:   -3.169 pos_prob:   65.205 prob_neg:   68.373 kl_weight:    0.459 do_ae_train: False
2022-12-28 23:30:23,941 - 6:21:07 - 9.9s - INFO - root - batch/max_batch/ep:   300/   529/    29 rec:   23.194 mi: 2.07622194 zkl:   66.324 cd:    0.584 pos_prob:   68.940 prob_neg:   68.355 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    29 rec:   23.194 mi: 2.07622194 zkl:   66.324 cd:    0.584 pos_prob:   68.940 prob_neg:   68.355 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:30:33,765 - 6:21:16 - 9.8s - INFO - root - batch/max_batch/ep:   400/   529/    29 rec:   21.598 mi: 2.01602268 zkl:   62.475 cd:    2.495 pos_prob:   70.675 prob_neg:   68.181 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    29 rec:   21.598 mi: 2.01602268 zkl:   62.475 cd:    2.495 pos_prob:   70.675 prob_neg:   68.181 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:30:43,280 - 6:21:26 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    29 rec:   19.398 mi: 2.02280736 zkl:   62.767 cd:   -0.228 pos_prob:   68.231 prob_neg:   68.460 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    29 rec:   19.398 mi: 2.02280736 zkl:   62.767 cd:   -0.228 pos_prob:   68.231 prob_neg:   68.460 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:30:43,293 - 6:21:26 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-326.759
Langevin prior   1/ 40: energy=-326.759
2022-12-28 23:30:43,298 - 6:21:26 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1105.928
Langevin prior   6/ 40: energy=-1105.928
2022-12-28 23:30:43,304 - 6:21:26 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1634.949
Langevin prior  11/ 40: energy=-1634.949
2022-12-28 23:30:43,309 - 6:21:26 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1886.852
Langevin prior  16/ 40: energy=-1886.852
2022-12-28 23:30:43,314 - 6:21:26 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1967.119
Langevin prior  21/ 40: energy=-1967.119
2022-12-28 23:30:43,320 - 6:21:26 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2043.982
Langevin prior  26/ 40: energy=-2043.982
2022-12-28 23:30:43,328 - 6:21:26 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2071.532
Langevin prior  31/ 40: energy=-2071.532
2022-12-28 23:30:43,336 - 6:21:26 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2128.188
Langevin prior  36/ 40: energy=-2128.188
2022-12-28 23:30:43,343 - 6:21:26 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2107.749
Langevin prior  40/ 40: energy=-2107.749
2022-12-28 23:40:28,739 - 6:31:11 - 585.4s - INFO - root - Negative Log-likehood -164.872926
200 done. -142.6043041120473
400 done. -138.0987391370674
600 done. -137.4277645050905
800 done. -138.24420864925557
1000 done. -137.35414278763994
1200 done. -137.93332196279857
1400 done. -138.94899028058046
1600 done. -137.36887869002499
1800 done. -136.8946695545619
2000 done. -152.9384480566708
2200 done. -163.67306588682584
2400 done. -175.0256833968671
2600 done. -184.58258214861337
2800 done. -191.07130367283534
3000 done. -199.70685019932554
3200 done. -204.90085058970462
3400 done. -211.53089961216796
3600 done. -215.57343566587343
3800 done. -220.35490306339267
4000 done. -223.80496053871977
4200 done. -215.00922249399147
4400 done. -206.02796198488883
4600 done. -197.99995014033559
4800 done. -190.57832690397123
5000 done. -183.91046621795235
5200 done. -177.641743511276
5400 done. -171.9325619412758
5600 done. -166.62442888187582
Negative Log-likehood -164.872926
2022-12-28 23:40:28,739 - 6:31:11 - 0.0s - INFO - root - log-likelihood:   -164.873
log-likelihood:   -164.873
2022-12-28 23:41:17,008 - 6:32:00 - 48.3s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-029-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 23:41:17,010 - 6:32:00 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-029-test-greedy.txt
Generation: 188 batches
2022-12-28 23:41:22,717 - 6:32:05 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:41:24,240 - 6:32:07 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:41:24,240 - 6:32:07 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:41:26,767 - 6:32:09 - 2.5s - INFO - root - --- bleu: BLEU = 29.71, 52.4/31.9/26.1/23.1 (BP=0.938, ratio=0.940, hyp_len=159572, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.71, 52.4/31.9/26.1/23.1 (BP=0.938, ratio=0.940, hyp_len=159572, ref_len=169777)
--- bleu: BLEU = 29.71, 52.4/31.9/26.1/23.1 (BP=0.938, ratio=0.940, hyp_len=159572, ref_len=169777)

2022-12-28 23:41:26,767 - 6:32:09 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:41:26,769 - 6:32:09 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:41:36,432 - 6:32:19 - 9.7s - INFO - root - batch/max_batch/ep:   100/   529/    30 rec:   19.021 mi: 1.99843442 zkl:   65.563 cd:   -0.510 pos_prob:   72.939 prob_neg:   73.449 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    30 rec:   19.021 mi: 1.99843442 zkl:   65.563 cd:   -0.510 pos_prob:   72.939 prob_neg:   73.449 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:41:45,805 - 6:32:29 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    30 rec:   12.612 mi: 2.19269419 zkl:   61.576 cd:    0.020 pos_prob:   71.398 prob_neg:   71.378 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    30 rec:   12.612 mi: 2.19269419 zkl:   61.576 cd:    0.020 pos_prob:   71.398 prob_neg:   71.378 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:41:55,306 - 6:32:38 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    30 rec:   19.675 mi: 2.08365464 zkl:   62.876 cd:    1.920 pos_prob:   74.799 prob_neg:   72.879 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    30 rec:   19.675 mi: 2.08365464 zkl:   62.876 cd:    1.920 pos_prob:   74.799 prob_neg:   72.879 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:42:04,826 - 6:32:48 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    30 rec:   19.179 mi: 2.01779270 zkl:   66.880 cd:    0.150 pos_prob:   76.916 prob_neg:   76.766 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    30 rec:   19.179 mi: 2.01779270 zkl:   66.880 cd:    0.150 pos_prob:   76.916 prob_neg:   76.766 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:42:14,728 - 6:32:57 - 9.9s - INFO - root - batch/max_batch/ep:   500/   529/    30 rec:   19.544 mi: 2.09090257 zkl:   64.180 cd:    0.986 pos_prob:   73.490 prob_neg:   72.504 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    30 rec:   19.544 mi: 2.09090257 zkl:   64.180 cd:    0.986 pos_prob:   73.490 prob_neg:   72.504 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:42:14,741 - 6:32:57 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-280.761
Langevin prior   1/ 40: energy=-280.761
2022-12-28 23:42:14,746 - 6:32:57 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1107.774
Langevin prior   6/ 40: energy=-1107.774
2022-12-28 23:42:14,751 - 6:32:57 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1734.553
Langevin prior  11/ 40: energy=-1734.553
2022-12-28 23:42:14,757 - 6:32:57 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2074.329
Langevin prior  16/ 40: energy=-2074.329
2022-12-28 23:42:14,762 - 6:32:57 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2144.406
Langevin prior  21/ 40: energy=-2144.406
2022-12-28 23:42:14,768 - 6:32:57 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2169.594
Langevin prior  26/ 40: energy=-2169.594
2022-12-28 23:42:14,776 - 6:32:57 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2217.787
Langevin prior  31/ 40: energy=-2217.787
2022-12-28 23:42:14,784 - 6:32:57 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2278.555
Langevin prior  36/ 40: energy=-2278.555
2022-12-28 23:42:14,792 - 6:32:57 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2227.108
Langevin prior  40/ 40: energy=-2227.108
2022-12-28 23:52:00,451 - 6:42:43 - 585.7s - INFO - root - Negative Log-likehood -167.555817
200 done. -144.82832684934885
400 done. -140.16810347288344
600 done. -139.42220503817214
800 done. -140.2504243880869
1000 done. -139.27273281795325
1200 done. -139.74460738491533
1400 done. -140.72698328713085
1600 done. -139.08056328793432
1800 done. -138.62446120086878
2000 done. -155.07682688356306
2200 done. -165.68442599819727
2400 done. -177.22860923616327
2600 done. -187.00478584084
2800 done. -193.5167818986546
3000 done. -202.15133913987606
3200 done. -207.2361980949572
3400 done. -213.97243075433136
3600 done. -218.0417798536395
3800 done. -222.98828448169624
4000 done. -226.4403937623343
4200 done. -217.70826357461806
4400 done. -208.75667975005123
4600 done. -200.72923450736778
4800 done. -193.2913054334425
5000 done. -186.63599408060733
5200 done. -180.346494460146
5400 done. -174.63821734197114
5600 done. -169.3138986503267
Negative Log-likehood -167.555817
2022-12-28 23:52:00,451 - 6:42:43 - 0.0s - INFO - root - log-likelihood:   -167.556
log-likelihood:   -167.556
2022-12-28 23:52:48,737 - 6:43:31 - 48.3s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-030-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-28 23:52:48,739 - 6:43:31 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-030-test-greedy.txt
Generation: 188 batches
2022-12-28 23:52:54,446 - 6:43:37 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:52:55,969 - 6:43:39 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:52:55,969 - 6:43:39 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:52:58,683 - 6:43:41 - 2.7s - INFO - root - --- bleu: BLEU = 29.78, 52.4/31.8/25.9/22.9 (BP=0.944, ratio=0.945, hyp_len=160499, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.78, 52.4/31.8/25.9/22.9 (BP=0.944, ratio=0.945, hyp_len=160499, ref_len=169777)
--- bleu: BLEU = 29.78, 52.4/31.8/25.9/22.9 (BP=0.944, ratio=0.945, hyp_len=160499, ref_len=169777)

2022-12-28 23:52:58,683 - 6:43:41 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:52:58,685 - 6:43:41 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:53:07,872 - 6:43:51 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    31 rec:   12.247 mi: 2.00440049 zkl:  143.872 cd:  -11.185 pos_prob:   63.064 prob_neg:   74.249 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    31 rec:   12.247 mi: 2.00440049 zkl:  143.872 cd:  -11.185 pos_prob:   63.064 prob_neg:   74.249 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:53:17,430 - 6:44:00 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/    31 rec:   13.369 mi: 2.01852441 zkl:  152.483 cd:  -15.268 pos_prob:   57.609 prob_neg:   72.877 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    31 rec:   13.369 mi: 2.01852441 zkl:  152.483 cd:  -15.268 pos_prob:   57.609 prob_neg:   72.877 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:53:26,902 - 6:44:10 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    31 rec:    9.416 mi: 2.14744568 zkl:  155.227 cd:  -23.090 pos_prob:   54.235 prob_neg:   77.324 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    31 rec:    9.416 mi: 2.14744568 zkl:  155.227 cd:  -23.090 pos_prob:   54.235 prob_neg:   77.324 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:53:36,067 - 6:44:19 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    31 rec:   11.529 mi: 2.08231306 zkl:  157.304 cd:  -25.131 pos_prob:   51.001 prob_neg:   76.132 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    31 rec:   11.529 mi: 2.08231306 zkl:  157.304 cd:  -25.131 pos_prob:   51.001 prob_neg:   76.132 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:53:45,487 - 6:44:28 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    31 rec:   14.830 mi: 2.14384055 zkl:  159.421 cd:  -29.385 pos_prob:   46.468 prob_neg:   75.853 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    31 rec:   14.830 mi: 2.14384055 zkl:  159.421 cd:  -29.385 pos_prob:   46.468 prob_neg:   75.853 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:53:45,500 - 6:44:28 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-313.605
Langevin prior   1/ 40: energy=-313.605
2022-12-28 23:53:45,505 - 6:44:28 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1174.463
Langevin prior   6/ 40: energy=-1174.463
2022-12-28 23:53:45,513 - 6:44:28 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1721.650
Langevin prior  11/ 40: energy=-1721.650
2022-12-28 23:53:45,522 - 6:44:28 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2084.527
Langevin prior  16/ 40: energy=-2084.527
2022-12-28 23:53:45,530 - 6:44:28 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2237.283
Langevin prior  21/ 40: energy=-2237.283
2022-12-28 23:53:45,538 - 6:44:28 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2264.328
Langevin prior  26/ 40: energy=-2264.328
2022-12-28 23:53:45,547 - 6:44:28 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2228.533
Langevin prior  31/ 40: energy=-2228.533
2022-12-28 23:53:45,558 - 6:44:28 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2266.040
Langevin prior  36/ 40: energy=-2266.040
2022-12-28 23:53:45,568 - 6:44:28 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2199.651
Langevin prior  40/ 40: energy=-2199.651
2022-12-28 23:53:48,323 - 6:44:31 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-031-test-greedy.txt
Generation: 188 batches
2022-12-28 23:53:54,016 - 6:44:37 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:53:55,530 - 6:44:38 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:53:55,530 - 6:44:38 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:53:58,032 - 6:44:41 - 2.5s - INFO - root - --- bleu: BLEU = 31.39, 54.9/33.8/27.6/24.5 (BP=0.938, ratio=0.940, hyp_len=159636, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.39, 54.9/33.8/27.6/24.5 (BP=0.938, ratio=0.940, hyp_len=159636, ref_len=169777)
--- bleu: BLEU = 31.39, 54.9/33.8/27.6/24.5 (BP=0.938, ratio=0.940, hyp_len=159636, ref_len=169777)

2022-12-28 23:53:58,033 - 6:44:41 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:53:58,034 - 6:44:41 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:54:07,576 - 6:44:50 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    32 rec:   12.033 mi: 2.14028740 zkl:  164.052 cd:  -29.244 pos_prob:   46.868 prob_neg:   76.112 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    32 rec:   12.033 mi: 2.14028740 zkl:  164.052 cd:  -29.244 pos_prob:   46.868 prob_neg:   76.112 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:54:16,782 - 6:44:59 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    32 rec:    7.370 mi: 1.94731867 zkl:  164.935 cd:  -25.614 pos_prob:   49.635 prob_neg:   75.249 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    32 rec:    7.370 mi: 1.94731867 zkl:  164.935 cd:  -25.614 pos_prob:   49.635 prob_neg:   75.249 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:54:25,841 - 6:45:09 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    32 rec:   11.730 mi: 1.95027983 zkl:  159.712 cd:  -35.068 pos_prob:   43.693 prob_neg:   78.761 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    32 rec:   11.730 mi: 1.95027983 zkl:  159.712 cd:  -35.068 pos_prob:   43.693 prob_neg:   78.761 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:54:35,122 - 6:45:18 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    32 rec:   11.407 mi: 2.00706244 zkl:  163.410 cd:  -31.434 pos_prob:   44.874 prob_neg:   76.308 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    32 rec:   11.407 mi: 2.00706244 zkl:  163.410 cd:  -31.434 pos_prob:   44.874 prob_neg:   76.308 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:54:44,281 - 6:45:27 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    32 rec:    9.816 mi: 1.99388623 zkl:  166.658 cd:  -22.795 pos_prob:   48.144 prob_neg:   70.939 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    32 rec:    9.816 mi: 1.99388623 zkl:  166.658 cd:  -22.795 pos_prob:   48.144 prob_neg:   70.939 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:54:44,294 - 6:45:27 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-321.584
Langevin prior   1/ 40: energy=-321.584
2022-12-28 23:54:44,299 - 6:45:27 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1247.053
Langevin prior   6/ 40: energy=-1247.053
2022-12-28 23:54:44,304 - 6:45:27 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1759.600
Langevin prior  11/ 40: energy=-1759.600
2022-12-28 23:54:44,309 - 6:45:27 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1962.800
Langevin prior  16/ 40: energy=-1962.800
2022-12-28 23:54:44,314 - 6:45:27 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2072.813
Langevin prior  21/ 40: energy=-2072.813
2022-12-28 23:54:44,320 - 6:45:27 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2110.031
Langevin prior  26/ 40: energy=-2110.031
2022-12-28 23:54:44,327 - 6:45:27 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2085.170
Langevin prior  31/ 40: energy=-2085.170
2022-12-28 23:54:44,336 - 6:45:27 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2130.327
Langevin prior  36/ 40: energy=-2130.327
2022-12-28 23:54:44,343 - 6:45:27 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2073.816
Langevin prior  40/ 40: energy=-2073.816
2022-12-28 23:54:46,982 - 6:45:30 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-032-test-greedy.txt
Generation: 188 batches
2022-12-28 23:54:52,668 - 6:45:35 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:54:54,162 - 6:45:37 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:54:54,163 - 6:45:37 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:54:56,591 - 6:45:39 - 2.4s - INFO - root - --- bleu: BLEU = 31.43, 55.5/34.1/27.9/24.7 (BP=0.930, ratio=0.932, hyp_len=158306, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.43, 55.5/34.1/27.9/24.7 (BP=0.930, ratio=0.932, hyp_len=158306, ref_len=169777)
--- bleu: BLEU = 31.43, 55.5/34.1/27.9/24.7 (BP=0.930, ratio=0.932, hyp_len=158306, ref_len=169777)

2022-12-28 23:54:56,591 - 6:45:39 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:54:56,593 - 6:45:39 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:55:05,937 - 6:45:49 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    33 rec:    9.040 mi: 1.91673493 zkl:  168.792 cd:  -30.421 pos_prob:   45.815 prob_neg:   76.235 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    33 rec:    9.040 mi: 1.91673493 zkl:  168.792 cd:  -30.421 pos_prob:   45.815 prob_neg:   76.235 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:55:15,217 - 6:45:58 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    33 rec:   11.880 mi: 2.03619671 zkl:  166.860 cd:  -31.986 pos_prob:   42.131 prob_neg:   74.118 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    33 rec:   11.880 mi: 2.03619671 zkl:  166.860 cd:  -31.986 pos_prob:   42.131 prob_neg:   74.118 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:55:24,171 - 6:46:07 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    33 rec:    7.381 mi: 2.00822377 zkl:  170.691 cd:  -28.849 pos_prob:   44.143 prob_neg:   72.992 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    33 rec:    7.381 mi: 2.00822377 zkl:  170.691 cd:  -28.849 pos_prob:   44.143 prob_neg:   72.992 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:55:33,315 - 6:46:16 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    33 rec:   12.402 mi: 2.05874562 zkl:  169.330 cd:  -30.286 pos_prob:   43.661 prob_neg:   73.946 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    33 rec:   12.402 mi: 2.05874562 zkl:  169.330 cd:  -30.286 pos_prob:   43.661 prob_neg:   73.946 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:55:42,484 - 6:46:25 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    33 rec:   11.834 mi: 2.09392190 zkl:  169.099 cd:  -32.215 pos_prob:   42.418 prob_neg:   74.632 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    33 rec:   11.834 mi: 2.09392190 zkl:  169.099 cd:  -32.215 pos_prob:   42.418 prob_neg:   74.632 kl_weight:    0.000 do_ae_train: True
2022-12-28 23:55:42,496 - 6:46:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-252.741
Langevin prior   1/ 40: energy=-252.741
2022-12-28 23:55:42,502 - 6:46:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1080.179
Langevin prior   6/ 40: energy=-1080.179
2022-12-28 23:55:42,507 - 6:46:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1601.640
Langevin prior  11/ 40: energy=-1601.640
2022-12-28 23:55:42,512 - 6:46:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1963.742
Langevin prior  16/ 40: energy=-1963.742
2022-12-28 23:55:42,519 - 6:46:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2063.581
Langevin prior  21/ 40: energy=-2063.581
2022-12-28 23:55:42,526 - 6:46:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2224.765
Langevin prior  26/ 40: energy=-2224.765
2022-12-28 23:55:42,535 - 6:46:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2230.439
Langevin prior  31/ 40: energy=-2230.439
2022-12-28 23:55:42,544 - 6:46:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2252.563
Langevin prior  36/ 40: energy=-2252.563
2022-12-28 23:55:42,553 - 6:46:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2235.729
Langevin prior  40/ 40: energy=-2235.729
2022-12-28 23:55:45,188 - 6:46:28 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-033-test-greedy.txt
Generation: 188 batches
2022-12-28 23:55:50,879 - 6:46:34 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:55:52,392 - 6:46:35 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:55:52,392 - 6:46:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:55:54,828 - 6:46:38 - 2.4s - INFO - root - --- bleu: BLEU = 31.32, 54.7/33.4/27.3/24.2 (BP=0.945, ratio=0.947, hyp_len=160702, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.32, 54.7/33.4/27.3/24.2 (BP=0.945, ratio=0.947, hyp_len=160702, ref_len=169777)
--- bleu: BLEU = 31.32, 54.7/33.4/27.3/24.2 (BP=0.945, ratio=0.947, hyp_len=160702, ref_len=169777)

2022-12-28 23:55:54,829 - 6:46:38 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:55:54,830 - 6:46:38 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:56:04,211 - 6:46:47 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    34 rec:   12.715 mi: 2.13515902 zkl:   96.768 cd:  -12.260 pos_prob:   59.457 prob_neg:   71.717 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    34 rec:   12.715 mi: 2.13515902 zkl:   96.768 cd:  -12.260 pos_prob:   59.457 prob_neg:   71.717 kl_weight:    0.062 do_ae_train: False
2022-12-28 23:56:13,758 - 6:46:56 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    34 rec:   10.684 mi: 2.01352334 zkl:   79.841 cd:   -1.460 pos_prob:   61.766 prob_neg:   63.226 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    34 rec:   10.684 mi: 2.01352334 zkl:   79.841 cd:   -1.460 pos_prob:   61.766 prob_neg:   63.226 kl_weight:    0.125 do_ae_train: False
2022-12-28 23:56:23,410 - 6:47:06 - 9.7s - INFO - root - batch/max_batch/ep:   300/   529/    34 rec:   12.456 mi: 2.21896744 zkl:   80.073 cd:   -4.757 pos_prob:   58.682 prob_neg:   63.438 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    34 rec:   12.456 mi: 2.21896744 zkl:   80.073 cd:   -4.757 pos_prob:   58.682 prob_neg:   63.438 kl_weight:    0.188 do_ae_train: False
2022-12-28 23:56:32,793 - 6:47:15 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    34 rec:   13.936 mi: 2.07683086 zkl:   66.678 cd:   -3.298 pos_prob:   61.418 prob_neg:   64.716 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    34 rec:   13.936 mi: 2.07683086 zkl:   66.678 cd:   -3.298 pos_prob:   61.418 prob_neg:   64.716 kl_weight:    0.251 do_ae_train: False
2022-12-28 23:56:42,148 - 6:47:25 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    34 rec:   16.552 mi: 2.09951997 zkl:   69.594 cd:   -2.088 pos_prob:   60.898 prob_neg:   62.986 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    34 rec:   16.552 mi: 2.09951997 zkl:   69.594 cd:   -2.088 pos_prob:   60.898 prob_neg:   62.986 kl_weight:    0.314 do_ae_train: False
2022-12-28 23:56:42,161 - 6:47:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-269.774
Langevin prior   1/ 40: energy=-269.774
2022-12-28 23:56:42,167 - 6:47:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-977.462
Langevin prior   6/ 40: energy=-977.462
2022-12-28 23:56:42,172 - 6:47:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1437.282
Langevin prior  11/ 40: energy=-1437.282
2022-12-28 23:56:42,177 - 6:47:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1651.893
Langevin prior  16/ 40: energy=-1651.893
2022-12-28 23:56:42,182 - 6:47:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1790.487
Langevin prior  21/ 40: energy=-1790.487
2022-12-28 23:56:42,188 - 6:47:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1835.894
Langevin prior  26/ 40: energy=-1835.894
2022-12-28 23:56:42,196 - 6:47:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1824.862
Langevin prior  31/ 40: energy=-1824.862
2022-12-28 23:56:42,204 - 6:47:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1779.990
Langevin prior  36/ 40: energy=-1779.990
2022-12-28 23:56:42,212 - 6:47:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1810.361
Langevin prior  40/ 40: energy=-1810.361
2022-12-28 23:56:44,863 - 6:47:28 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-034-test-greedy.txt
Generation: 188 batches
2022-12-28 23:56:50,561 - 6:47:33 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:56:52,053 - 6:47:35 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-28 23:56:52,053 - 6:47:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-28 23:56:54,486 - 6:47:37 - 2.4s - INFO - root - --- bleu: BLEU = 30.14, 54.3/33.0/27.0/23.8 (BP=0.920, ratio=0.923, hyp_len=156758, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.14, 54.3/33.0/27.0/23.8 (BP=0.920, ratio=0.923, hyp_len=156758, ref_len=169777)
--- bleu: BLEU = 30.14, 54.3/33.0/27.0/23.8 (BP=0.920, ratio=0.923, hyp_len=156758, ref_len=169777)

2022-12-28 23:56:54,487 - 6:47:37 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-28 23:56:54,488 - 6:47:37 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-28 23:57:03,762 - 6:47:46 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    35 rec:   12.608 mi: 2.05294728 zkl:   61.073 cd:   -0.550 pos_prob:   62.450 prob_neg:   63.000 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    35 rec:   12.608 mi: 2.05294728 zkl:   61.073 cd:   -0.550 pos_prob:   62.450 prob_neg:   63.000 kl_weight:    0.396 do_ae_train: False
2022-12-28 23:57:13,207 - 6:47:56 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    35 rec:   18.373 mi: 2.18078685 zkl:   59.893 cd:   -1.972 pos_prob:   61.920 prob_neg:   63.893 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    35 rec:   18.373 mi: 2.18078685 zkl:   59.893 cd:   -1.972 pos_prob:   61.920 prob_neg:   63.893 kl_weight:    0.459 do_ae_train: False
2022-12-28 23:57:22,513 - 6:48:05 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    35 rec:   17.770 mi: 2.12514830 zkl:   56.113 cd:   -2.842 pos_prob:   61.193 prob_neg:   64.036 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    35 rec:   17.770 mi: 2.12514830 zkl:   56.113 cd:   -2.842 pos_prob:   61.193 prob_neg:   64.036 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:57:31,964 - 6:48:15 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    35 rec:   19.185 mi: 2.08320141 zkl:   62.425 cd:    0.414 pos_prob:   65.819 prob_neg:   65.405 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    35 rec:   19.185 mi: 2.08320141 zkl:   62.425 cd:    0.414 pos_prob:   65.819 prob_neg:   65.405 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:57:41,736 - 6:48:24 - 9.8s - INFO - root - batch/max_batch/ep:   500/   529/    35 rec:   14.950 mi: 1.95642018 zkl:   56.430 cd:   -3.153 pos_prob:   61.179 prob_neg:   64.331 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    35 rec:   14.950 mi: 1.95642018 zkl:   56.430 cd:   -3.153 pos_prob:   61.179 prob_neg:   64.331 kl_weight:    0.500 do_ae_train: False
2022-12-28 23:57:41,748 - 6:48:24 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-240.841
Langevin prior   1/ 40: energy=-240.841
2022-12-28 23:57:41,754 - 6:48:24 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-961.387
Langevin prior   6/ 40: energy=-961.387
2022-12-28 23:57:41,759 - 6:48:24 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1422.768
Langevin prior  11/ 40: energy=-1422.768
2022-12-28 23:57:41,764 - 6:48:24 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1701.049
Langevin prior  16/ 40: energy=-1701.049
2022-12-28 23:57:41,770 - 6:48:24 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1834.303
Langevin prior  21/ 40: energy=-1834.303
2022-12-28 23:57:41,776 - 6:48:24 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1858.023
Langevin prior  26/ 40: energy=-1858.023
2022-12-28 23:57:41,784 - 6:48:24 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1900.677
Langevin prior  31/ 40: energy=-1900.677
2022-12-28 23:57:41,792 - 6:48:24 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1982.824
Langevin prior  36/ 40: energy=-1982.824
2022-12-28 23:57:41,800 - 6:48:24 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2036.763
Langevin prior  40/ 40: energy=-2036.763
2022-12-29 00:07:27,644 - 6:58:10 - 585.8s - INFO - root - Negative Log-likehood -172.023711
200 done. -149.49801475055813
400 done. -144.57815696328228
600 done. -143.60416087486004
800 done. -144.35638088484657
1000 done. -143.3508704971408
1200 done. -144.0099670237268
1400 done. -145.13830844775214
1600 done. -143.5507618961052
1800 done. -143.0635211806374
2000 done. -159.8346459856315
2200 done. -170.9476833895551
2400 done. -182.7433539304919
2600 done. -192.79283140278963
2800 done. -199.6261757036623
3000 done. -208.1791117850449
3200 done. -213.61373295885528
3400 done. -220.4560130008866
3600 done. -224.72204542988175
3800 done. -229.6939269007976
4000 done. -233.29141625725248
4200 done. -224.1794060975071
4400 done. -214.83716492430008
4600 done. -206.48974286267347
4800 done. -198.75015143065315
5000 done. -191.8316125739481
5200 done. -185.30981511062262
5400 done. -179.37089862918927
5600 done. -173.84925558514146
Negative Log-likehood -172.023711
2022-12-29 00:07:27,644 - 6:58:10 - 0.0s - INFO - root - log-likelihood:   -172.024
log-likelihood:   -172.024
2022-12-29 00:08:15,207 - 6:58:58 - 47.6s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-035-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 00:08:15,228 - 6:58:58 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-035-test-greedy.txt
Generation: 188 batches
2022-12-29 00:08:20,950 - 6:59:04 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:08:22,488 - 6:59:05 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:08:22,488 - 6:59:05 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:08:24,972 - 6:59:08 - 2.5s - INFO - root - --- bleu: BLEU = 29.88, 51.3/30.9/25.2/22.3 (BP=0.972, ratio=0.972, hyp_len=165097, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.88, 51.3/30.9/25.2/22.3 (BP=0.972, ratio=0.972, hyp_len=165097, ref_len=169777)
--- bleu: BLEU = 29.88, 51.3/30.9/25.2/22.3 (BP=0.972, ratio=0.972, hyp_len=165097, ref_len=169777)

2022-12-29 00:08:24,973 - 6:59:08 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:08:24,974 - 6:59:08 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:08:34,404 - 6:59:17 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    36 rec:   17.111 mi: 2.02857852 zkl:   57.315 cd:    0.778 pos_prob:   62.947 prob_neg:   62.169 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    36 rec:   17.111 mi: 2.02857852 zkl:   57.315 cd:    0.778 pos_prob:   62.947 prob_neg:   62.169 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:08:43,999 - 6:59:27 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/    36 rec:   17.688 mi: 2.06631708 zkl:   58.873 cd:    6.170 pos_prob:   67.880 prob_neg:   61.710 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    36 rec:   17.688 mi: 2.06631708 zkl:   58.873 cd:    6.170 pos_prob:   67.880 prob_neg:   61.710 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:08:53,407 - 6:59:36 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    36 rec:   14.806 mi: 2.04243302 zkl:   60.863 cd:   -0.707 pos_prob:   67.694 prob_neg:   68.401 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    36 rec:   14.806 mi: 2.04243302 zkl:   60.863 cd:   -0.707 pos_prob:   67.694 prob_neg:   68.401 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:09:03,170 - 6:59:46 - 9.8s - INFO - root - batch/max_batch/ep:   400/   529/    36 rec:   17.200 mi: 1.99058974 zkl:   56.965 cd:   -1.285 pos_prob:   70.131 prob_neg:   71.416 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    36 rec:   17.200 mi: 1.99058974 zkl:   56.965 cd:   -1.285 pos_prob:   70.131 prob_neg:   71.416 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:09:12,759 - 6:59:55 - 9.6s - INFO - root - batch/max_batch/ep:   500/   529/    36 rec:   15.375 mi: 2.18045855 zkl:   60.687 cd:    5.030 pos_prob:   70.444 prob_neg:   65.414 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    36 rec:   15.375 mi: 2.18045855 zkl:   60.687 cd:    5.030 pos_prob:   70.444 prob_neg:   65.414 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:09:12,771 - 6:59:55 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-283.066
Langevin prior   1/ 40: energy=-283.066
2022-12-29 00:09:12,776 - 6:59:55 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-992.769
Langevin prior   6/ 40: energy=-992.769
2022-12-29 00:09:12,781 - 6:59:55 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1512.436
Langevin prior  11/ 40: energy=-1512.436
2022-12-29 00:09:12,786 - 6:59:55 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1780.486
Langevin prior  16/ 40: energy=-1780.486
2022-12-29 00:09:12,791 - 6:59:55 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1932.849
Langevin prior  21/ 40: energy=-1932.849
2022-12-29 00:09:12,797 - 6:59:55 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1919.755
Langevin prior  26/ 40: energy=-1919.755
2022-12-29 00:09:12,802 - 6:59:55 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1959.307
Langevin prior  31/ 40: energy=-1959.307
2022-12-29 00:09:12,810 - 6:59:56 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1987.425
Langevin prior  36/ 40: energy=-1987.425
2022-12-29 00:09:12,816 - 6:59:56 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2014.195
Langevin prior  40/ 40: energy=-2014.195
2022-12-29 00:18:59,150 - 7:09:42 - 586.3s - INFO - root - Negative Log-likehood -173.749107
200 done. -149.6062780590556
400 done. -144.5520086198416
600 done. -144.037513450627
800 done. -144.74819328244598
1000 done. -143.7288127544218
1200 done. -144.3295131395115
1400 done. -145.52263847127804
1600 done. -143.90917166788475
1800 done. -143.37296278150663
2000 done. -160.44452712984452
2200 done. -171.62600055342284
2400 done. -183.5332170928501
2600 done. -193.91868537184791
2800 done. -200.904872229846
3000 done. -209.76848791140762
3200 done. -215.39077164342882
3400 done. -222.45550392887762
3600 done. -226.79536244906663
3800 done. -231.82802715696934
4000 done. -235.65481995601257
4200 done. -226.43971445767102
4400 done. -217.0262684239871
4600 done. -208.58748268737136
4800 done. -200.78358139473326
5000 done. -193.79847848744896
5200 done. -187.20047398917924
5400 done. -181.19022139197045
5600 done. -175.59489798679147
Negative Log-likehood -173.749107
2022-12-29 00:18:59,150 - 7:09:42 - 0.0s - INFO - root - log-likelihood:   -173.749
log-likelihood:   -173.749
2022-12-29 00:19:47,135 - 7:10:30 - 48.0s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-036-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 00:19:47,156 - 7:10:30 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-036-test-greedy.txt
Generation: 188 batches
2022-12-29 00:19:52,843 - 7:10:36 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:19:54,369 - 7:10:37 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:19:54,369 - 7:10:37 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:19:57,084 - 7:10:40 - 2.7s - INFO - root - --- bleu: BLEU = 29.93, 52.5/31.9/26.1/23.2 (BP=0.943, ratio=0.945, hyp_len=160387, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.93, 52.5/31.9/26.1/23.2 (BP=0.943, ratio=0.945, hyp_len=160387, ref_len=169777)
--- bleu: BLEU = 29.93, 52.5/31.9/26.1/23.2 (BP=0.943, ratio=0.945, hyp_len=160387, ref_len=169777)

2022-12-29 00:19:57,084 - 7:10:40 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:19:57,086 - 7:10:40 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:20:06,281 - 7:10:49 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    37 rec:    6.673 mi: 2.12190843 zkl:  142.471 cd:   -6.804 pos_prob:   63.926 prob_neg:   70.730 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    37 rec:    6.673 mi: 2.12190843 zkl:  142.471 cd:   -6.804 pos_prob:   63.926 prob_neg:   70.730 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:20:15,752 - 7:10:58 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    37 rec:    9.159 mi: 2.04163814 zkl:  148.541 cd:  -12.167 pos_prob:   56.115 prob_neg:   68.282 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    37 rec:    9.159 mi: 2.04163814 zkl:  148.541 cd:  -12.167 pos_prob:   56.115 prob_neg:   68.282 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:20:25,346 - 7:11:08 - 9.6s - INFO - root - batch/max_batch/ep:   300/   529/    37 rec:   12.191 mi: 2.15061879 zkl:  154.809 cd:  -12.803 pos_prob:   55.589 prob_neg:   68.392 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    37 rec:   12.191 mi: 2.15061879 zkl:  154.809 cd:  -12.803 pos_prob:   55.589 prob_neg:   68.392 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:20:34,529 - 7:11:17 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    37 rec:   12.779 mi: 2.20238805 zkl:  151.981 cd:  -20.936 pos_prob:   47.015 prob_neg:   67.951 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    37 rec:   12.779 mi: 2.20238805 zkl:  151.981 cd:  -20.936 pos_prob:   47.015 prob_neg:   67.951 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:20:43,854 - 7:11:27 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    37 rec:   13.601 mi: 2.13353372 zkl:  157.969 cd:  -18.041 pos_prob:   48.006 prob_neg:   66.047 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    37 rec:   13.601 mi: 2.13353372 zkl:  157.969 cd:  -18.041 pos_prob:   48.006 prob_neg:   66.047 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:20:43,866 - 7:11:27 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-288.075
Langevin prior   1/ 40: energy=-288.075
2022-12-29 00:20:43,872 - 7:11:27 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-988.004
Langevin prior   6/ 40: energy=-988.004
2022-12-29 00:20:43,877 - 7:11:27 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1567.929
Langevin prior  11/ 40: energy=-1567.929
2022-12-29 00:20:43,883 - 7:11:27 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1747.407
Langevin prior  16/ 40: energy=-1747.407
2022-12-29 00:20:43,889 - 7:11:27 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1835.304
Langevin prior  21/ 40: energy=-1835.304
2022-12-29 00:20:43,897 - 7:11:27 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1959.001
Langevin prior  26/ 40: energy=-1959.001
2022-12-29 00:20:43,906 - 7:11:27 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2025.271
Langevin prior  31/ 40: energy=-2025.271
2022-12-29 00:20:43,915 - 7:11:27 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2069.154
Langevin prior  36/ 40: energy=-2069.154
2022-12-29 00:20:43,922 - 7:11:27 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2058.612
Langevin prior  40/ 40: energy=-2058.612
2022-12-29 00:20:46,594 - 7:11:29 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-037-test-greedy.txt
Generation: 188 batches
2022-12-29 00:20:52,297 - 7:11:35 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:20:53,803 - 7:11:36 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:20:53,803 - 7:11:36 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:20:56,296 - 7:11:39 - 2.5s - INFO - root - --- bleu: BLEU = 31.12, 54.2/33.0/27.0/24.0 (BP=0.949, ratio=0.950, hyp_len=161257, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.12, 54.2/33.0/27.0/24.0 (BP=0.949, ratio=0.950, hyp_len=161257, ref_len=169777)
--- bleu: BLEU = 31.12, 54.2/33.0/27.0/24.0 (BP=0.949, ratio=0.950, hyp_len=161257, ref_len=169777)

2022-12-29 00:20:56,297 - 7:11:39 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:20:56,298 - 7:11:39 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:21:05,388 - 7:11:48 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    38 rec:   14.417 mi: 2.13736653 zkl:  160.818 cd:  -21.342 pos_prob:   44.638 prob_neg:   65.980 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    38 rec:   14.417 mi: 2.13736653 zkl:  160.818 cd:  -21.342 pos_prob:   44.638 prob_neg:   65.980 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:21:14,399 - 7:11:57 - 9.0s - INFO - root - batch/max_batch/ep:   200/   529/    38 rec:   11.449 mi: 2.16350317 zkl:  166.653 cd:  -20.548 pos_prob:   49.021 prob_neg:   69.569 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    38 rec:   11.449 mi: 2.16350317 zkl:  166.653 cd:  -20.548 pos_prob:   49.021 prob_neg:   69.569 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:21:23,800 - 7:12:06 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    38 rec:   12.228 mi: 1.94472861 zkl:  159.009 cd:  -18.164 pos_prob:   46.195 prob_neg:   64.359 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    38 rec:   12.228 mi: 1.94472861 zkl:  159.009 cd:  -18.164 pos_prob:   46.195 prob_neg:   64.359 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:21:33,612 - 7:12:16 - 9.8s - INFO - root - batch/max_batch/ep:   400/   529/    38 rec:   11.607 mi: 1.92013752 zkl:  166.716 cd:  -22.705 pos_prob:   46.743 prob_neg:   69.449 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    38 rec:   11.607 mi: 1.92013752 zkl:  166.716 cd:  -22.705 pos_prob:   46.743 prob_neg:   69.449 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:21:42,669 - 7:12:25 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    38 rec:   13.676 mi: 2.02850437 zkl:  161.122 cd:  -25.595 pos_prob:   40.759 prob_neg:   66.353 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    38 rec:   13.676 mi: 2.02850437 zkl:  161.122 cd:  -25.595 pos_prob:   40.759 prob_neg:   66.353 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:21:42,681 - 7:12:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-274.593
Langevin prior   1/ 40: energy=-274.593
2022-12-29 00:21:42,687 - 7:12:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1012.073
Langevin prior   6/ 40: energy=-1012.073
2022-12-29 00:21:42,692 - 7:12:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1527.129
Langevin prior  11/ 40: energy=-1527.129
2022-12-29 00:21:42,697 - 7:12:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1826.777
Langevin prior  16/ 40: energy=-1826.777
2022-12-29 00:21:42,703 - 7:12:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1942.002
Langevin prior  21/ 40: energy=-1942.002
2022-12-29 00:21:42,710 - 7:12:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2083.443
Langevin prior  26/ 40: energy=-2083.443
2022-12-29 00:21:42,718 - 7:12:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2084.732
Langevin prior  31/ 40: energy=-2084.732
2022-12-29 00:21:42,726 - 7:12:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2123.135
Langevin prior  36/ 40: energy=-2123.135
2022-12-29 00:21:42,734 - 7:12:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2115.276
Langevin prior  40/ 40: energy=-2115.276
2022-12-29 00:21:45,340 - 7:12:28 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-038-test-greedy.txt
Generation: 188 batches
2022-12-29 00:21:51,032 - 7:12:34 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:21:52,562 - 7:12:35 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:21:52,562 - 7:12:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:21:55,039 - 7:12:38 - 2.5s - INFO - root - --- bleu: BLEU = 31.29, 53.5/32.6/26.6/23.5 (BP=0.968, ratio=0.969, hyp_len=164503, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.29, 53.5/32.6/26.6/23.5 (BP=0.968, ratio=0.969, hyp_len=164503, ref_len=169777)
--- bleu: BLEU = 31.29, 53.5/32.6/26.6/23.5 (BP=0.968, ratio=0.969, hyp_len=164503, ref_len=169777)

2022-12-29 00:21:55,040 - 7:12:38 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:21:55,041 - 7:12:38 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:22:04,320 - 7:12:47 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    39 rec:   10.180 mi: 2.05129695 zkl:  170.202 cd:  -21.815 pos_prob:   47.417 prob_neg:   69.232 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    39 rec:   10.180 mi: 2.05129695 zkl:  170.202 cd:  -21.815 pos_prob:   47.417 prob_neg:   69.232 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:22:13,775 - 7:12:56 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    39 rec:   14.767 mi: 2.17039776 zkl:  165.862 cd:  -31.228 pos_prob:   39.319 prob_neg:   70.547 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    39 rec:   14.767 mi: 2.17039776 zkl:  165.862 cd:  -31.228 pos_prob:   39.319 prob_neg:   70.547 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:22:22,907 - 7:13:06 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    39 rec:   17.004 mi: 2.14535427 zkl:  168.536 cd:  -32.256 pos_prob:   35.073 prob_neg:   67.329 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    39 rec:   17.004 mi: 2.14535427 zkl:  168.536 cd:  -32.256 pos_prob:   35.073 prob_neg:   67.329 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:22:32,146 - 7:13:15 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    39 rec:   14.626 mi: 2.00252581 zkl:  168.449 cd:  -33.610 pos_prob:   34.061 prob_neg:   67.670 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    39 rec:   14.626 mi: 2.00252581 zkl:  168.449 cd:  -33.610 pos_prob:   34.061 prob_neg:   67.670 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:22:41,424 - 7:13:24 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    39 rec:   11.239 mi: 2.09357834 zkl:  165.300 cd:  -24.339 pos_prob:   38.958 prob_neg:   63.297 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    39 rec:   11.239 mi: 2.09357834 zkl:  165.300 cd:  -24.339 pos_prob:   38.958 prob_neg:   63.297 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:22:41,437 - 7:13:24 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-300.381
Langevin prior   1/ 40: energy=-300.381
2022-12-29 00:22:41,444 - 7:13:24 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1117.663
Langevin prior   6/ 40: energy=-1117.663
2022-12-29 00:22:41,453 - 7:13:24 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1591.929
Langevin prior  11/ 40: energy=-1591.929
2022-12-29 00:22:41,465 - 7:13:24 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1825.680
Langevin prior  16/ 40: energy=-1825.680
2022-12-29 00:22:41,477 - 7:13:24 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1968.487
Langevin prior  21/ 40: energy=-1968.487
2022-12-29 00:22:41,489 - 7:13:24 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1951.552
Langevin prior  26/ 40: energy=-1951.552
2022-12-29 00:22:41,500 - 7:13:24 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1920.202
Langevin prior  31/ 40: energy=-1920.202
2022-12-29 00:22:41,513 - 7:13:24 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1965.719
Langevin prior  36/ 40: energy=-1965.719
2022-12-29 00:22:41,523 - 7:13:24 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2016.171
Langevin prior  40/ 40: energy=-2016.171
2022-12-29 00:22:44,173 - 7:13:27 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-039-test-greedy.txt
Generation: 188 batches
2022-12-29 00:22:49,872 - 7:13:33 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:22:51,379 - 7:13:34 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:22:51,379 - 7:13:34 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:22:53,817 - 7:13:37 - 2.4s - INFO - root - --- bleu: BLEU = 31.22, 54.4/33.2/27.3/24.3 (BP=0.944, ratio=0.946, hyp_len=160594, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.22, 54.4/33.2/27.3/24.3 (BP=0.944, ratio=0.946, hyp_len=160594, ref_len=169777)
--- bleu: BLEU = 31.22, 54.4/33.2/27.3/24.3 (BP=0.944, ratio=0.946, hyp_len=160594, ref_len=169777)

2022-12-29 00:22:53,817 - 7:13:37 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:22:53,819 - 7:13:37 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:23:03,341 - 7:13:46 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    40 rec:   12.357 mi: 2.06124187 zkl:  100.066 cd:   -7.340 pos_prob:   58.424 prob_neg:   65.764 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    40 rec:   12.357 mi: 2.06124187 zkl:  100.066 cd:   -7.340 pos_prob:   58.424 prob_neg:   65.764 kl_weight:    0.062 do_ae_train: False
2022-12-29 00:23:13,140 - 7:13:56 - 9.8s - INFO - root - batch/max_batch/ep:   200/   529/    40 rec:   10.241 mi: 2.29782271 zkl:   87.235 cd:   -6.823 pos_prob:   67.142 prob_neg:   73.965 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    40 rec:   10.241 mi: 2.29782271 zkl:   87.235 cd:   -6.823 pos_prob:   67.142 prob_neg:   73.965 kl_weight:    0.125 do_ae_train: False
2022-12-29 00:23:22,652 - 7:14:05 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    40 rec:    9.716 mi: 2.23615623 zkl:   80.952 cd:   -3.600 pos_prob:   72.857 prob_neg:   76.457 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    40 rec:    9.716 mi: 2.23615623 zkl:   80.952 cd:   -3.600 pos_prob:   72.857 prob_neg:   76.457 kl_weight:    0.188 do_ae_train: False
2022-12-29 00:23:32,073 - 7:14:15 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    40 rec:    9.226 mi: 2.07767916 zkl:   82.670 cd:    6.301 pos_prob:   80.638 prob_neg:   74.337 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    40 rec:    9.226 mi: 2.07767916 zkl:   82.670 cd:    6.301 pos_prob:   80.638 prob_neg:   74.337 kl_weight:    0.251 do_ae_train: False
2022-12-29 00:23:41,947 - 7:14:25 - 9.9s - INFO - root - batch/max_batch/ep:   500/   529/    40 rec:   18.134 mi: 2.34829664 zkl:   81.582 cd:   -1.447 pos_prob:   77.165 prob_neg:   78.611 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    40 rec:   18.134 mi: 2.34829664 zkl:   81.582 cd:   -1.447 pos_prob:   77.165 prob_neg:   78.611 kl_weight:    0.314 do_ae_train: False
2022-12-29 00:23:41,960 - 7:14:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-334.638
Langevin prior   1/ 40: energy=-334.638
2022-12-29 00:23:41,965 - 7:14:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1255.936
Langevin prior   6/ 40: energy=-1255.936
2022-12-29 00:23:41,970 - 7:14:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1735.516
Langevin prior  11/ 40: energy=-1735.516
2022-12-29 00:23:41,975 - 7:14:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2032.473
Langevin prior  16/ 40: energy=-2032.473
2022-12-29 00:23:41,980 - 7:14:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2157.030
Langevin prior  21/ 40: energy=-2157.030
2022-12-29 00:23:41,987 - 7:14:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2242.111
Langevin prior  26/ 40: energy=-2242.111
2022-12-29 00:23:41,995 - 7:14:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2236.266
Langevin prior  31/ 40: energy=-2236.266
2022-12-29 00:23:42,006 - 7:14:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2280.000
Langevin prior  36/ 40: energy=-2280.000
2022-12-29 00:23:42,015 - 7:14:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2266.624
Langevin prior  40/ 40: energy=-2266.624
2022-12-29 00:23:44,841 - 7:14:28 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-040-test-greedy.txt
Generation: 188 batches
2022-12-29 00:23:50,522 - 7:14:33 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:23:52,035 - 7:14:35 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:23:52,035 - 7:14:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:23:54,504 - 7:14:37 - 2.5s - INFO - root - --- bleu: BLEU = 30.36, 53.3/32.3/26.4/23.5 (BP=0.944, ratio=0.946, hyp_len=160567, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.36, 53.3/32.3/26.4/23.5 (BP=0.944, ratio=0.946, hyp_len=160567, ref_len=169777)
--- bleu: BLEU = 30.36, 53.3/32.3/26.4/23.5 (BP=0.944, ratio=0.946, hyp_len=160567, ref_len=169777)

2022-12-29 00:23:54,504 - 7:14:37 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:23:54,506 - 7:14:37 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:24:03,960 - 7:14:47 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    41 rec:   17.265 mi: 2.15946937 zkl:   82.419 cd:    9.223 pos_prob:   84.748 prob_neg:   75.525 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    41 rec:   17.265 mi: 2.15946937 zkl:   82.419 cd:    9.223 pos_prob:   84.748 prob_neg:   75.525 kl_weight:    0.396 do_ae_train: False
2022-12-29 00:24:13,386 - 7:14:56 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    41 rec:   17.914 mi: 2.44259119 zkl:   75.843 cd:   -1.333 pos_prob:   82.320 prob_neg:   83.653 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    41 rec:   17.914 mi: 2.44259119 zkl:   75.843 cd:   -1.333 pos_prob:   82.320 prob_neg:   83.653 kl_weight:    0.459 do_ae_train: False
2022-12-29 00:24:22,876 - 7:15:06 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    41 rec:   17.865 mi: 2.42465782 zkl:   73.111 cd:   -0.871 pos_prob:   85.589 prob_neg:   86.460 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    41 rec:   17.865 mi: 2.42465782 zkl:   73.111 cd:   -0.871 pos_prob:   85.589 prob_neg:   86.460 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:24:32,209 - 7:15:15 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    41 rec:   14.463 mi: 2.24151349 zkl:   72.017 cd:    1.001 pos_prob:   87.703 prob_neg:   86.702 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    41 rec:   14.463 mi: 2.24151349 zkl:   72.017 cd:    1.001 pos_prob:   87.703 prob_neg:   86.702 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:24:41,725 - 7:15:24 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    41 rec:   19.941 mi: 2.56974244 zkl:   80.554 cd:    4.886 pos_prob:   93.473 prob_neg:   88.588 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    41 rec:   19.941 mi: 2.56974244 zkl:   80.554 cd:    4.886 pos_prob:   93.473 prob_neg:   88.588 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:24:41,737 - 7:15:24 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-313.154
Langevin prior   1/ 40: energy=-313.154
2022-12-29 00:24:41,743 - 7:15:24 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1352.458
Langevin prior   6/ 40: energy=-1352.458
2022-12-29 00:24:41,748 - 7:15:24 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1983.024
Langevin prior  11/ 40: energy=-1983.024
2022-12-29 00:24:41,753 - 7:15:24 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2311.585
Langevin prior  16/ 40: energy=-2311.585
2022-12-29 00:24:41,760 - 7:15:24 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2447.270
Langevin prior  21/ 40: energy=-2447.270
2022-12-29 00:24:41,768 - 7:15:24 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2578.143
Langevin prior  26/ 40: energy=-2578.143
2022-12-29 00:24:41,778 - 7:15:24 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2505.366
Langevin prior  31/ 40: energy=-2505.366
2022-12-29 00:24:41,789 - 7:15:24 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2518.017
Langevin prior  36/ 40: energy=-2518.017
2022-12-29 00:24:41,798 - 7:15:24 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2590.579
Langevin prior  40/ 40: energy=-2590.579
2022-12-29 00:34:27,235 - 7:25:10 - 585.4s - INFO - root - Negative Log-likehood -178.257293
200 done. -154.63742677197612
400 done. -150.0378512201559
600 done. -149.5540941597542
800 done. -150.38470090152413
1000 done. -149.29301263331928
1200 done. -149.97386531743373
1400 done. -151.02451161375473
1600 done. -149.28158010143815
1800 done. -148.80740497361995
2000 done. -166.06115360996586
2200 done. -177.5414963221252
2400 done. -189.71189283886758
2600 done. -200.06667061158447
2800 done. -206.9789165584437
3000 done. -215.95722549420202
3200 done. -221.36599049676894
3400 done. -228.40738810900868
3600 done. -232.7201654112933
3800 done. -238.05977505026536
4000 done. -241.80185294749128
4200 done. -232.34875489033786
4400 done. -222.64269472120537
4600 done. -213.9634272719634
4800 done. -205.9459635252691
5000 done. -198.789146733751
5200 done. -192.02362017849524
5400 done. -185.85879061968458
5600 done. -180.14398116847792
Negative Log-likehood -178.257293
2022-12-29 00:34:27,235 - 7:25:10 - 0.0s - INFO - root - log-likelihood:   -178.257
log-likelihood:   -178.257
2022-12-29 00:35:15,585 - 7:25:58 - 48.4s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-041-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 00:35:15,588 - 7:25:58 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-041-test-greedy.txt
Generation: 188 batches
2022-12-29 00:35:21,289 - 7:26:04 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:35:22,817 - 7:26:06 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:35:22,817 - 7:26:06 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:35:25,337 - 7:26:08 - 2.5s - INFO - root - --- bleu: BLEU = 30.05, 52.2/31.5/25.7/22.8 (BP=0.960, ratio=0.961, hyp_len=163123, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.05, 52.2/31.5/25.7/22.8 (BP=0.960, ratio=0.961, hyp_len=163123, ref_len=169777)
--- bleu: BLEU = 30.05, 52.2/31.5/25.7/22.8 (BP=0.960, ratio=0.961, hyp_len=163123, ref_len=169777)

2022-12-29 00:35:25,338 - 7:26:08 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:35:25,339 - 7:26:08 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:35:34,803 - 7:26:17 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    42 rec:   14.075 mi: 2.25243473 zkl:   76.898 cd:    0.880 pos_prob:   92.127 prob_neg:   91.246 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    42 rec:   14.075 mi: 2.25243473 zkl:   76.898 cd:    0.880 pos_prob:   92.127 prob_neg:   91.246 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:35:44,011 - 7:26:27 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    42 rec:   17.634 mi: 2.36684084 zkl:   74.395 cd:   -1.863 pos_prob:   90.814 prob_neg:   92.677 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    42 rec:   17.634 mi: 2.36684084 zkl:   74.395 cd:   -1.863 pos_prob:   90.814 prob_neg:   92.677 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:35:54,079 - 7:26:37 - 10.1s - INFO - root - batch/max_batch/ep:   300/   529/    42 rec:   13.585 mi: 2.51338696 zkl:   78.036 cd:    1.792 pos_prob:   96.973 prob_neg:   95.181 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    42 rec:   13.585 mi: 2.51338696 zkl:   78.036 cd:    1.792 pos_prob:   96.973 prob_neg:   95.181 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:36:03,533 - 7:26:46 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    42 rec:   14.493 mi: 2.42912030 zkl:   81.971 cd:    1.021 pos_prob:   98.985 prob_neg:   97.963 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    42 rec:   14.493 mi: 2.42912030 zkl:   81.971 cd:    1.021 pos_prob:   98.985 prob_neg:   97.963 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:36:13,327 - 7:26:56 - 9.8s - INFO - root - batch/max_batch/ep:   500/   529/    42 rec:   22.019 mi: 2.29875064 zkl:   76.216 cd:   -0.322 pos_prob:   93.907 prob_neg:   94.229 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    42 rec:   22.019 mi: 2.29875064 zkl:   76.216 cd:   -0.322 pos_prob:   93.907 prob_neg:   94.229 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:36:13,340 - 7:26:56 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-419.015
Langevin prior   1/ 40: energy=-419.015
2022-12-29 00:36:13,346 - 7:26:56 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1528.164
Langevin prior   6/ 40: energy=-1528.164
2022-12-29 00:36:13,351 - 7:26:56 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2228.304
Langevin prior  11/ 40: energy=-2228.304
2022-12-29 00:36:13,356 - 7:26:56 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2665.229
Langevin prior  16/ 40: energy=-2665.229
2022-12-29 00:36:13,362 - 7:26:56 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2865.236
Langevin prior  21/ 40: energy=-2865.236
2022-12-29 00:36:13,368 - 7:26:56 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2864.987
Langevin prior  26/ 40: energy=-2864.987
2022-12-29 00:36:13,375 - 7:26:56 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2895.367
Langevin prior  31/ 40: energy=-2895.367
2022-12-29 00:36:13,384 - 7:26:56 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2909.223
Langevin prior  36/ 40: energy=-2909.223
2022-12-29 00:36:13,391 - 7:26:56 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2887.477
Langevin prior  40/ 40: energy=-2887.477
2022-12-29 00:45:58,735 - 7:36:41 - 585.3s - INFO - root - Negative Log-likehood -176.910700
200 done. -153.4726994883352
400 done. -148.48103411157092
600 done. -148.02461209236284
800 done. -148.95605461343203
1000 done. -147.8728110180923
1200 done. -148.4654713175558
1400 done. -149.5952616967103
1600 done. -147.86712509178022
1800 done. -147.34429456443948
2000 done. -164.85294912816954
2200 done. -176.34474089987674
2400 done. -188.42545405084564
2600 done. -198.77237260569123
2800 done. -205.86909263131867
3000 done. -214.69420771246163
3200 done. -220.10548051114304
3400 done. -227.2526742922613
3600 done. -231.51327074987418
3800 done. -236.7283404753435
4000 done. -240.5291828942087
4200 done. -231.0940691986813
4400 done. -221.40027511850943
4600 done. -212.71281608128638
4800 done. -204.66956293205203
5000 done. -197.48014958752177
5200 done. -190.70211135795012
5400 done. -184.53326262443647
5600 done. -178.8007505024183
Negative Log-likehood -176.910700
2022-12-29 00:45:58,736 - 7:36:41 - 0.0s - INFO - root - log-likelihood:   -176.911
log-likelihood:   -176.911
2022-12-29 00:46:46,248 - 7:37:29 - 47.5s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-042-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 00:46:46,252 - 7:37:29 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-042-test-greedy.txt
Generation: 188 batches
2022-12-29 00:46:51,927 - 7:37:35 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:46:53,442 - 7:37:36 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:46:53,442 - 7:37:36 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:46:55,881 - 7:37:39 - 2.4s - INFO - root - --- bleu: BLEU = 29.69, 52.2/31.5/25.8/22.9 (BP=0.944, ratio=0.946, hyp_len=160600, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.69, 52.2/31.5/25.8/22.9 (BP=0.944, ratio=0.946, hyp_len=160600, ref_len=169777)
--- bleu: BLEU = 29.69, 52.2/31.5/25.8/22.9 (BP=0.944, ratio=0.946, hyp_len=160600, ref_len=169777)

2022-12-29 00:46:55,882 - 7:37:39 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:46:55,883 - 7:37:39 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:47:05,172 - 7:37:48 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    43 rec:   12.096 mi: 2.36436796 zkl:  154.070 cd:  -14.297 pos_prob:   84.266 prob_neg:   98.563 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    43 rec:   12.096 mi: 2.36436796 zkl:  154.070 cd:  -14.297 pos_prob:   84.266 prob_neg:   98.563 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:47:14,377 - 7:37:57 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    43 rec:    9.969 mi: 2.27741551 zkl:  160.219 cd:  -22.030 pos_prob:   75.217 prob_neg:   97.247 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    43 rec:    9.969 mi: 2.27741551 zkl:  160.219 cd:  -22.030 pos_prob:   75.217 prob_neg:   97.247 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:47:23,607 - 7:38:06 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    43 rec:    9.770 mi: 2.49672771 zkl:  161.712 cd:  -31.773 pos_prob:   66.000 prob_neg:   97.773 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    43 rec:    9.770 mi: 2.49672771 zkl:  161.712 cd:  -31.773 pos_prob:   66.000 prob_neg:   97.773 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:47:32,891 - 7:38:16 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    43 rec:   11.082 mi: 2.32204962 zkl:  163.277 cd:  -29.544 pos_prob:   66.264 prob_neg:   95.808 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    43 rec:   11.082 mi: 2.32204962 zkl:  163.277 cd:  -29.544 pos_prob:   66.264 prob_neg:   95.808 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:47:41,984 - 7:38:25 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    43 rec:   11.832 mi: 2.27635312 zkl:  166.588 cd:  -34.634 pos_prob:   62.273 prob_neg:   96.907 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    43 rec:   11.832 mi: 2.27635312 zkl:  166.588 cd:  -34.634 pos_prob:   62.273 prob_neg:   96.907 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:47:41,997 - 7:38:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-337.556
Langevin prior   1/ 40: energy=-337.556
2022-12-29 00:47:42,002 - 7:38:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1467.858
Langevin prior   6/ 40: energy=-1467.858
2022-12-29 00:47:42,008 - 7:38:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2174.312
Langevin prior  11/ 40: energy=-2174.312
2022-12-29 00:47:42,013 - 7:38:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2542.871
Langevin prior  16/ 40: energy=-2542.871
2022-12-29 00:47:42,019 - 7:38:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2719.526
Langevin prior  21/ 40: energy=-2719.526
2022-12-29 00:47:42,026 - 7:38:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2836.219
Langevin prior  26/ 40: energy=-2836.219
2022-12-29 00:47:42,035 - 7:38:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2865.088
Langevin prior  31/ 40: energy=-2865.088
2022-12-29 00:47:42,044 - 7:38:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2855.153
Langevin prior  36/ 40: energy=-2855.153
2022-12-29 00:47:42,051 - 7:38:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2903.900
Langevin prior  40/ 40: energy=-2903.900
2022-12-29 00:47:44,642 - 7:38:27 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-043-test-greedy.txt
Generation: 188 batches
2022-12-29 00:47:50,293 - 7:38:33 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:47:52,059 - 7:38:35 - 1.8s - INFO - root - Generation Done
Generation Done
2022-12-29 00:47:52,059 - 7:38:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:47:54,580 - 7:38:37 - 2.5s - INFO - root - --- bleu: BLEU = 31.06, 54.7/33.4/27.4/24.3 (BP=0.936, ratio=0.938, hyp_len=159190, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.06, 54.7/33.4/27.4/24.3 (BP=0.936, ratio=0.938, hyp_len=159190, ref_len=169777)
--- bleu: BLEU = 31.06, 54.7/33.4/27.4/24.3 (BP=0.936, ratio=0.938, hyp_len=159190, ref_len=169777)

2022-12-29 00:47:54,581 - 7:38:37 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:47:54,583 - 7:38:37 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:48:03,805 - 7:38:47 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    44 rec:    9.100 mi: 2.24031568 zkl:  166.242 cd:  -36.860 pos_prob:   59.192 prob_neg:   96.052 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    44 rec:    9.100 mi: 2.24031568 zkl:  166.242 cd:  -36.860 pos_prob:   59.192 prob_neg:   96.052 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:48:13,130 - 7:38:56 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    44 rec:   10.065 mi: 2.34719706 zkl:  165.799 cd:  -41.729 pos_prob:   52.597 prob_neg:   94.326 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    44 rec:   10.065 mi: 2.34719706 zkl:  165.799 cd:  -41.729 pos_prob:   52.597 prob_neg:   94.326 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:48:22,257 - 7:39:05 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    44 rec:    8.827 mi: 2.36173415 zkl:  162.256 cd:  -41.113 pos_prob:   55.538 prob_neg:   96.651 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    44 rec:    8.827 mi: 2.36173415 zkl:  162.256 cd:  -41.113 pos_prob:   55.538 prob_neg:   96.651 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:48:31,497 - 7:39:14 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    44 rec:   10.714 mi: 2.36849928 zkl:  163.844 cd:  -49.748 pos_prob:   49.264 prob_neg:   99.012 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    44 rec:   10.714 mi: 2.36849928 zkl:  163.844 cd:  -49.748 pos_prob:   49.264 prob_neg:   99.012 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:48:40,671 - 7:39:23 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    44 rec:   13.453 mi: 2.19471812 zkl:  159.871 cd:  -54.642 pos_prob:   44.203 prob_neg:   98.845 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    44 rec:   13.453 mi: 2.19471812 zkl:  159.871 cd:  -54.642 pos_prob:   44.203 prob_neg:   98.845 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:48:40,684 - 7:39:23 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-370.703
Langevin prior   1/ 40: energy=-370.703
2022-12-29 00:48:40,689 - 7:39:23 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1532.848
Langevin prior   6/ 40: energy=-1532.848
2022-12-29 00:48:40,695 - 7:39:23 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2302.188
Langevin prior  11/ 40: energy=-2302.188
2022-12-29 00:48:40,700 - 7:39:23 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2661.186
Langevin prior  16/ 40: energy=-2661.186
2022-12-29 00:48:40,705 - 7:39:23 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2797.331
Langevin prior  21/ 40: energy=-2797.331
2022-12-29 00:48:40,711 - 7:39:23 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2913.476
Langevin prior  26/ 40: energy=-2913.476
2022-12-29 00:48:40,718 - 7:39:23 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2919.542
Langevin prior  31/ 40: energy=-2919.542
2022-12-29 00:48:40,726 - 7:39:23 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2940.651
Langevin prior  36/ 40: energy=-2940.651
2022-12-29 00:48:40,733 - 7:39:23 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2972.858
Langevin prior  40/ 40: energy=-2972.858
2022-12-29 00:48:43,511 - 7:39:26 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-044-test-greedy.txt
Generation: 188 batches
2022-12-29 00:48:49,214 - 7:39:32 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:48:50,736 - 7:39:33 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:48:50,736 - 7:39:33 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:48:53,272 - 7:39:36 - 2.5s - INFO - root - --- bleu: BLEU = 31.13, 53.6/32.7/26.8/23.8 (BP=0.958, ratio=0.959, hyp_len=162747, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.13, 53.6/32.7/26.8/23.8 (BP=0.958, ratio=0.959, hyp_len=162747, ref_len=169777)
--- bleu: BLEU = 31.13, 53.6/32.7/26.8/23.8 (BP=0.958, ratio=0.959, hyp_len=162747, ref_len=169777)

2022-12-29 00:48:53,272 - 7:39:36 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:48:53,274 - 7:39:36 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:49:02,321 - 7:39:45 - 9.0s - INFO - root - batch/max_batch/ep:   100/   529/    45 rec:    9.898 mi: 2.39010525 zkl:  166.244 cd:  -46.612 pos_prob:   49.650 prob_neg:   96.262 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    45 rec:    9.898 mi: 2.39010525 zkl:  166.244 cd:  -46.612 pos_prob:   49.650 prob_neg:   96.262 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:49:11,628 - 7:39:54 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    45 rec:   11.891 mi: 2.37875533 zkl:  168.494 cd:  -45.961 pos_prob:   50.177 prob_neg:   96.137 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    45 rec:   11.891 mi: 2.37875533 zkl:  168.494 cd:  -45.961 pos_prob:   50.177 prob_neg:   96.137 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:49:20,863 - 7:40:04 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    45 rec:    6.876 mi: 2.30619955 zkl:  167.844 cd:  -42.642 pos_prob:   55.541 prob_neg:   98.182 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    45 rec:    6.876 mi: 2.30619955 zkl:  167.844 cd:  -42.642 pos_prob:   55.541 prob_neg:   98.182 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:49:29,846 - 7:40:13 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    45 rec:    9.065 mi: 2.06416106 zkl:  167.181 cd:  -41.386 pos_prob:   55.225 prob_neg:   96.611 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    45 rec:    9.065 mi: 2.06416106 zkl:  167.181 cd:  -41.386 pos_prob:   55.225 prob_neg:   96.611 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:49:39,178 - 7:40:22 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    45 rec:   14.113 mi: 2.24658394 zkl:  168.670 cd:  -50.806 pos_prob:   46.034 prob_neg:   96.840 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    45 rec:   14.113 mi: 2.24658394 zkl:  168.670 cd:  -50.806 pos_prob:   46.034 prob_neg:   96.840 kl_weight:    0.000 do_ae_train: True
2022-12-29 00:49:39,191 - 7:40:22 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-339.944
Langevin prior   1/ 40: energy=-339.944
2022-12-29 00:49:39,196 - 7:40:22 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1518.002
Langevin prior   6/ 40: energy=-1518.002
2022-12-29 00:49:39,202 - 7:40:22 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2256.691
Langevin prior  11/ 40: energy=-2256.691
2022-12-29 00:49:39,207 - 7:40:22 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2663.314
Langevin prior  16/ 40: energy=-2663.314
2022-12-29 00:49:39,215 - 7:40:22 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2752.694
Langevin prior  21/ 40: energy=-2752.694
2022-12-29 00:49:39,224 - 7:40:22 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2890.986
Langevin prior  26/ 40: energy=-2890.986
2022-12-29 00:49:39,236 - 7:40:22 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2968.569
Langevin prior  31/ 40: energy=-2968.569
2022-12-29 00:49:39,247 - 7:40:22 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2983.244
Langevin prior  36/ 40: energy=-2983.244
2022-12-29 00:49:39,256 - 7:40:22 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2964.776
Langevin prior  40/ 40: energy=-2964.776
2022-12-29 00:49:41,815 - 7:40:25 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-045-test-greedy.txt
Generation: 188 batches
2022-12-29 00:49:47,485 - 7:40:30 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:49:48,991 - 7:40:32 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:49:48,991 - 7:40:32 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:49:51,425 - 7:40:34 - 2.4s - INFO - root - --- bleu: BLEU = 31.23, 54.7/33.5/27.4/24.3 (BP=0.940, ratio=0.942, hyp_len=159884, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.23, 54.7/33.5/27.4/24.3 (BP=0.940, ratio=0.942, hyp_len=159884, ref_len=169777)
--- bleu: BLEU = 31.23, 54.7/33.5/27.4/24.3 (BP=0.940, ratio=0.942, hyp_len=159884, ref_len=169777)

2022-12-29 00:49:51,426 - 7:40:34 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:49:51,427 - 7:40:34 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:50:00,825 - 7:40:44 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    46 rec:    7.672 mi: 2.44485497 zkl:  106.679 cd:  -17.370 pos_prob:   80.672 prob_neg:   98.042 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    46 rec:    7.672 mi: 2.44485497 zkl:  106.679 cd:  -17.370 pos_prob:   80.672 prob_neg:   98.042 kl_weight:    0.062 do_ae_train: False
2022-12-29 00:50:10,439 - 7:40:53 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/    46 rec:   10.502 mi: 2.36683273 zkl:   92.111 cd:  -17.262 pos_prob:   85.985 prob_neg:  103.248 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    46 rec:   10.502 mi: 2.36683273 zkl:   92.111 cd:  -17.262 pos_prob:   85.985 prob_neg:  103.248 kl_weight:    0.125 do_ae_train: False
2022-12-29 00:50:20,010 - 7:41:03 - 9.6s - INFO - root - batch/max_batch/ep:   300/   529/    46 rec:   14.204 mi: 2.62758017 zkl:   91.010 cd:   -4.680 pos_prob:   97.490 prob_neg:  102.170 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    46 rec:   14.204 mi: 2.62758017 zkl:   91.010 cd:   -4.680 pos_prob:   97.490 prob_neg:  102.170 kl_weight:    0.188 do_ae_train: False
2022-12-29 00:50:29,366 - 7:41:12 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    46 rec:   15.350 mi: 2.44104266 zkl:   85.010 cd:  -11.332 pos_prob:   91.848 prob_neg:  103.180 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    46 rec:   15.350 mi: 2.44104266 zkl:   85.010 cd:  -11.332 pos_prob:   91.848 prob_neg:  103.180 kl_weight:    0.251 do_ae_train: False
2022-12-29 00:50:38,910 - 7:41:22 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    46 rec:   15.869 mi: 2.37030697 zkl:   92.348 cd:    3.114 pos_prob:  106.491 prob_neg:  103.376 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    46 rec:   15.869 mi: 2.37030697 zkl:   92.348 cd:    3.114 pos_prob:  106.491 prob_neg:  103.376 kl_weight:    0.314 do_ae_train: False
2022-12-29 00:50:38,923 - 7:41:22 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-310.731
Langevin prior   1/ 40: energy=-310.731
2022-12-29 00:50:38,928 - 7:41:22 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1654.746
Langevin prior   6/ 40: energy=-1654.746
2022-12-29 00:50:38,933 - 7:41:22 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2381.595
Langevin prior  11/ 40: energy=-2381.595
2022-12-29 00:50:38,938 - 7:41:22 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2668.555
Langevin prior  16/ 40: energy=-2668.555
2022-12-29 00:50:38,943 - 7:41:22 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2893.575
Langevin prior  21/ 40: energy=-2893.575
2022-12-29 00:50:38,949 - 7:41:22 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3103.206
Langevin prior  26/ 40: energy=-3103.206
2022-12-29 00:50:38,957 - 7:41:22 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3219.989
Langevin prior  31/ 40: energy=-3219.989
2022-12-29 00:50:38,965 - 7:41:22 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3232.507
Langevin prior  36/ 40: energy=-3232.507
2022-12-29 00:50:38,972 - 7:41:22 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3213.113
Langevin prior  40/ 40: energy=-3213.113
2022-12-29 00:50:41,682 - 7:41:24 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-046-test-greedy.txt
Generation: 188 batches
2022-12-29 00:50:47,378 - 7:41:30 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:50:48,906 - 7:41:32 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 00:50:48,907 - 7:41:32 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 00:50:51,375 - 7:41:34 - 2.5s - INFO - root - --- bleu: BLEU = 30.25, 52.2/31.4/25.7/22.7 (BP=0.967, ratio=0.968, hyp_len=164332, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.25, 52.2/31.4/25.7/22.7 (BP=0.967, ratio=0.968, hyp_len=164332, ref_len=169777)
--- bleu: BLEU = 30.25, 52.2/31.4/25.7/22.7 (BP=0.967, ratio=0.968, hyp_len=164332, ref_len=169777)

2022-12-29 00:50:51,376 - 7:41:34 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 00:50:51,377 - 7:41:34 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 00:51:00,957 - 7:41:44 - 9.6s - INFO - root - batch/max_batch/ep:   100/   529/    47 rec:   12.363 mi: 2.51397824 zkl:   86.244 cd:   -2.941 pos_prob:  101.879 prob_neg:  104.820 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    47 rec:   12.363 mi: 2.51397824 zkl:   86.244 cd:   -2.941 pos_prob:  101.879 prob_neg:  104.820 kl_weight:    0.396 do_ae_train: False
2022-12-29 00:51:10,292 - 7:41:53 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    47 rec:   16.396 mi: 2.40012789 zkl:   79.543 cd:   -5.979 pos_prob:   99.818 prob_neg:  105.798 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    47 rec:   16.396 mi: 2.40012789 zkl:   79.543 cd:   -5.979 pos_prob:   99.818 prob_neg:  105.798 kl_weight:    0.459 do_ae_train: False
2022-12-29 00:51:19,725 - 7:42:02 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    47 rec:   11.974 mi: 2.56011915 zkl:   80.373 cd:   -2.036 pos_prob:  105.164 prob_neg:  107.200 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    47 rec:   11.974 mi: 2.56011915 zkl:   80.373 cd:   -2.036 pos_prob:  105.164 prob_neg:  107.200 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:51:29,140 - 7:42:12 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    47 rec:   15.202 mi: 2.46979976 zkl:   78.315 cd:   -3.224 pos_prob:  102.689 prob_neg:  105.913 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    47 rec:   15.202 mi: 2.46979976 zkl:   78.315 cd:   -3.224 pos_prob:  102.689 prob_neg:  105.913 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:51:38,772 - 7:42:21 - 9.6s - INFO - root - batch/max_batch/ep:   500/   529/    47 rec:   19.126 mi: 2.38424683 zkl:   77.799 cd:   -5.547 pos_prob:   99.178 prob_neg:  104.725 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    47 rec:   19.126 mi: 2.38424683 zkl:   77.799 cd:   -5.547 pos_prob:   99.178 prob_neg:  104.725 kl_weight:    0.500 do_ae_train: False
2022-12-29 00:51:38,785 - 7:42:21 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-403.408
Langevin prior   1/ 40: energy=-403.408
2022-12-29 00:51:38,790 - 7:42:21 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1576.166
Langevin prior   6/ 40: energy=-1576.166
2022-12-29 00:51:38,795 - 7:42:21 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2392.854
Langevin prior  11/ 40: energy=-2392.854
2022-12-29 00:51:38,801 - 7:42:21 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2761.083
Langevin prior  16/ 40: energy=-2761.083
2022-12-29 00:51:38,806 - 7:42:22 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2928.649
Langevin prior  21/ 40: energy=-2928.649
2022-12-29 00:51:38,812 - 7:42:22 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2906.532
Langevin prior  26/ 40: energy=-2906.532
2022-12-29 00:51:38,819 - 7:42:22 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2971.196
Langevin prior  31/ 40: energy=-2971.196
2022-12-29 00:51:38,827 - 7:42:22 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3144.476
Langevin prior  36/ 40: energy=-3144.476
2022-12-29 00:51:38,834 - 7:42:22 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3150.881
Langevin prior  40/ 40: energy=-3150.881
2022-12-29 01:01:24,158 - 7:52:07 - 585.3s - INFO - root - Negative Log-likehood -180.036444
200 done. -158.03424984765869
400 done. -152.59723192631276
600 done. -151.7734546695988
800 done. -152.6321026694984
1000 done. -151.4766684971832
1200 done. -152.2747006852681
1400 done. -153.35245236029792
1600 done. -151.50610031764143
1800 done. -150.94209855159733
2000 done. -168.79301652568682
2200 done. -180.24202935359472
2400 done. -192.50484195186996
2600 done. -203.02393725543155
2800 done. -210.12347657535872
3000 done. -219.0889783573407
3200 done. -224.6587408019275
3400 done. -231.7908680133445
3600 done. -236.15736871364192
3800 done. -241.407701018883
4000 done. -245.22817481534065
4200 done. -235.54956668857605
4400 done. -225.63313173470436
4600 done. -216.76024807595286
4800 done. -208.51896977129766
5000 done. -201.14563395880552
5200 done. -194.19229143199925
5400 done. -187.85837173134206
5600 done. -181.98242860636105
Negative Log-likehood -180.036444
2022-12-29 01:01:24,158 - 7:52:07 - 0.0s - INFO - root - log-likelihood:   -180.036
log-likelihood:   -180.036
2022-12-29 01:02:11,076 - 7:52:54 - 46.9s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-047-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 01:02:11,079 - 7:52:54 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-047-test-greedy.txt
Generation: 188 batches
2022-12-29 01:02:16,762 - 7:52:59 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:02:18,267 - 7:53:01 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:02:18,267 - 7:53:01 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:02:20,727 - 7:53:03 - 2.5s - INFO - root - --- bleu: BLEU = 30.21, 53.2/32.6/26.8/23.8 (BP=0.932, ratio=0.934, hyp_len=158542, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.21, 53.2/32.6/26.8/23.8 (BP=0.932, ratio=0.934, hyp_len=158542, ref_len=169777)
--- bleu: BLEU = 30.21, 53.2/32.6/26.8/23.8 (BP=0.932, ratio=0.934, hyp_len=158542, ref_len=169777)

2022-12-29 01:02:20,728 - 7:53:03 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:02:20,729 - 7:53:03 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:02:30,104 - 7:53:13 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    48 rec:   14.474 mi: 2.38516736 zkl:   82.237 cd:    5.584 pos_prob:  108.893 prob_neg:  103.310 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    48 rec:   14.474 mi: 2.38516736 zkl:   82.237 cd:    5.584 pos_prob:  108.893 prob_neg:  103.310 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:02:39,316 - 7:53:22 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    48 rec:   19.750 mi: 2.35019636 zkl:   86.422 cd:    1.426 pos_prob:  109.589 prob_neg:  108.163 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    48 rec:   19.750 mi: 2.35019636 zkl:   86.422 cd:    1.426 pos_prob:  109.589 prob_neg:  108.163 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:02:48,698 - 7:53:31 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    48 rec:   18.810 mi: 2.36093378 zkl:   78.668 cd:   -6.306 pos_prob:  102.830 prob_neg:  109.137 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    48 rec:   18.810 mi: 2.36093378 zkl:   78.668 cd:   -6.306 pos_prob:  102.830 prob_neg:  109.137 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:02:57,999 - 7:53:41 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    48 rec:   14.809 mi: 2.30997658 zkl:   75.621 cd:   -3.028 pos_prob:  101.892 prob_neg:  104.920 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    48 rec:   14.809 mi: 2.30997658 zkl:   75.621 cd:   -3.028 pos_prob:  101.892 prob_neg:  104.920 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:03:07,341 - 7:53:50 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    48 rec:   18.095 mi: 2.41883016 zkl:   82.365 cd:   -5.757 pos_prob:  106.544 prob_neg:  112.301 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    48 rec:   18.095 mi: 2.41883016 zkl:   82.365 cd:   -5.757 pos_prob:  106.544 prob_neg:  112.301 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:03:07,354 - 7:53:50 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-276.122
Langevin prior   1/ 40: energy=-276.122
2022-12-29 01:03:07,359 - 7:53:50 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1637.570
Langevin prior   6/ 40: energy=-1637.570
2022-12-29 01:03:07,364 - 7:53:50 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2362.010
Langevin prior  11/ 40: energy=-2362.010
2022-12-29 01:03:07,370 - 7:53:50 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2777.439
Langevin prior  16/ 40: energy=-2777.439
2022-12-29 01:03:07,375 - 7:53:50 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-3010.310
Langevin prior  21/ 40: energy=-3010.310
2022-12-29 01:03:07,383 - 7:53:50 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3173.746
Langevin prior  26/ 40: energy=-3173.746
2022-12-29 01:03:07,391 - 7:53:50 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3141.769
Langevin prior  31/ 40: energy=-3141.769
2022-12-29 01:03:07,400 - 7:53:50 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3139.481
Langevin prior  36/ 40: energy=-3139.481
2022-12-29 01:03:07,408 - 7:53:50 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3220.820
Langevin prior  40/ 40: energy=-3220.820
2022-12-29 01:12:53,436 - 8:03:36 - 586.0s - INFO - root - Negative Log-likehood -180.965809
200 done. -158.1625073054968
400 done. -153.0314047015963
600 done. -152.25329263173853
800 done. -153.17415895498578
1000 done. -152.00433589049274
1200 done. -152.74436664454188
1400 done. -153.86401340569262
1600 done. -152.0052518508842
1800 done. -151.54219730504423
2000 done. -169.2987298708918
2200 done. -180.74721192609036
2400 done. -193.02585164459794
2600 done. -203.63926209290545
2800 done. -210.84287567929366
3000 done. -219.85983216421297
3200 done. -225.50218092760335
3400 done. -232.65542404564815
3600 done. -237.06279362977435
3800 done. -242.30213710790898
4000 done. -246.07592713852387
4200 done. -236.3722881290268
4400 done. -226.44422900135203
4600 done. -217.58726290679937
4800 done. -209.37292279859471
5000 done. -202.0274931597794
5200 done. -195.09472867249843
5400 done. -188.76731864750428
5600 done. -182.90462966565454
Negative Log-likehood -180.965809
2022-12-29 01:12:53,436 - 8:03:36 - 0.0s - INFO - root - log-likelihood:   -180.966
log-likelihood:   -180.966
2022-12-29 01:13:41,762 - 8:04:24 - 48.3s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-048-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 01:13:41,783 - 8:04:24 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-048-test-greedy.txt
Generation: 188 batches
2022-12-29 01:13:47,480 - 8:04:30 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:13:49,005 - 8:04:32 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:13:49,006 - 8:04:32 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:13:51,477 - 8:04:34 - 2.5s - INFO - root - --- bleu: BLEU = 29.43, 51.4/31.2/25.5/22.6 (BP=0.949, ratio=0.950, hyp_len=161255, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.43, 51.4/31.2/25.5/22.6 (BP=0.949, ratio=0.950, hyp_len=161255, ref_len=169777)
--- bleu: BLEU = 29.43, 51.4/31.2/25.5/22.6 (BP=0.949, ratio=0.950, hyp_len=161255, ref_len=169777)

2022-12-29 01:13:51,478 - 8:04:34 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:13:51,482 - 8:04:34 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:14:00,938 - 8:04:44 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    49 rec:   12.005 mi: 2.44237232 zkl:  154.336 cd:  -22.476 pos_prob:   87.581 prob_neg:  110.057 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    49 rec:   12.005 mi: 2.44237232 zkl:  154.336 cd:  -22.476 pos_prob:   87.581 prob_neg:  110.057 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:14:10,219 - 8:04:53 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    49 rec:    9.776 mi: 2.35305572 zkl:  156.832 cd:  -36.452 pos_prob:   74.988 prob_neg:  111.440 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    49 rec:    9.776 mi: 2.35305572 zkl:  156.832 cd:  -36.452 pos_prob:   74.988 prob_neg:  111.440 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:14:19,439 - 8:05:02 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    49 rec:   11.344 mi: 2.40864301 zkl:  162.414 cd:  -41.990 pos_prob:   69.833 prob_neg:  111.822 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    49 rec:   11.344 mi: 2.40864301 zkl:  162.414 cd:  -41.990 pos_prob:   69.833 prob_neg:  111.822 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:14:28,666 - 8:05:11 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    49 rec:    9.057 mi: 2.47167826 zkl:  163.418 cd:  -45.376 pos_prob:   64.808 prob_neg:  110.184 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    49 rec:    9.057 mi: 2.47167826 zkl:  163.418 cd:  -45.376 pos_prob:   64.808 prob_neg:  110.184 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:14:38,647 - 8:05:21 - 10.0s - INFO - root - batch/max_batch/ep:   500/   529/    49 rec:   10.115 mi: 2.33881474 zkl:  166.630 cd:  -38.874 pos_prob:   68.203 prob_neg:  107.077 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    49 rec:   10.115 mi: 2.33881474 zkl:  166.630 cd:  -38.874 pos_prob:   68.203 prob_neg:  107.077 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:14:38,659 - 8:05:21 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-387.639
Langevin prior   1/ 40: energy=-387.639
2022-12-29 01:14:38,665 - 8:05:21 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1758.099
Langevin prior   6/ 40: energy=-1758.099
2022-12-29 01:14:38,670 - 8:05:21 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2633.722
Langevin prior  11/ 40: energy=-2633.722
2022-12-29 01:14:38,675 - 8:05:21 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-3063.466
Langevin prior  16/ 40: energy=-3063.466
2022-12-29 01:14:38,680 - 8:05:21 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-3336.366
Langevin prior  21/ 40: energy=-3336.366
2022-12-29 01:14:38,687 - 8:05:21 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3274.763
Langevin prior  26/ 40: energy=-3274.763
2022-12-29 01:14:38,695 - 8:05:21 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3365.156
Langevin prior  31/ 40: energy=-3365.156
2022-12-29 01:14:38,704 - 8:05:21 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3357.380
Langevin prior  36/ 40: energy=-3357.380
2022-12-29 01:14:38,711 - 8:05:21 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3486.580
Langevin prior  40/ 40: energy=-3486.580
2022-12-29 01:14:41,345 - 8:05:24 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-049-test-greedy.txt
Generation: 188 batches
2022-12-29 01:14:47,030 - 8:05:30 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:14:48,538 - 8:05:31 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:14:48,538 - 8:05:31 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:14:51,020 - 8:05:34 - 2.5s - INFO - root - --- bleu: BLEU = 30.90, 54.0/33.1/27.2/24.1 (BP=0.939, ratio=0.941, hyp_len=159778, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.90, 54.0/33.1/27.2/24.1 (BP=0.939, ratio=0.941, hyp_len=159778, ref_len=169777)
--- bleu: BLEU = 30.90, 54.0/33.1/27.2/24.1 (BP=0.939, ratio=0.941, hyp_len=159778, ref_len=169777)

2022-12-29 01:14:51,021 - 8:05:34 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:14:51,022 - 8:05:34 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:15:00,395 - 8:05:43 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    50 rec:   11.337 mi: 2.22095466 zkl:  158.018 cd:  -56.984 pos_prob:   60.174 prob_neg:  117.158 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    50 rec:   11.337 mi: 2.22095466 zkl:  158.018 cd:  -56.984 pos_prob:   60.174 prob_neg:  117.158 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:15:10,006 - 8:05:53 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/    50 rec:    9.420 mi: 2.62401986 zkl:  163.493 cd:  -54.484 pos_prob:   60.166 prob_neg:  114.650 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    50 rec:    9.420 mi: 2.62401986 zkl:  163.493 cd:  -54.484 pos_prob:   60.166 prob_neg:  114.650 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:15:19,139 - 8:06:02 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    50 rec:   13.022 mi: 2.28788805 zkl:  161.328 cd:  -55.316 pos_prob:   59.455 prob_neg:  114.770 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    50 rec:   13.022 mi: 2.28788805 zkl:  161.328 cd:  -55.316 pos_prob:   59.455 prob_neg:  114.770 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:15:28,331 - 8:06:11 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    50 rec:    9.478 mi: 2.23792219 zkl:  161.811 cd:  -54.551 pos_prob:   55.075 prob_neg:  109.625 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    50 rec:    9.478 mi: 2.23792219 zkl:  161.811 cd:  -54.551 pos_prob:   55.075 prob_neg:  109.625 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:15:37,804 - 8:06:20 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    50 rec:    9.032 mi: 2.39773679 zkl:  164.723 cd:  -56.133 pos_prob:   52.088 prob_neg:  108.222 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    50 rec:    9.032 mi: 2.39773679 zkl:  164.723 cd:  -56.133 pos_prob:   52.088 prob_neg:  108.222 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:15:37,816 - 8:06:21 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-335.091
Langevin prior   1/ 40: energy=-335.091
2022-12-29 01:15:37,822 - 8:06:21 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1861.727
Langevin prior   6/ 40: energy=-1861.727
2022-12-29 01:15:37,827 - 8:06:21 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2706.636
Langevin prior  11/ 40: energy=-2706.636
2022-12-29 01:15:37,833 - 8:06:21 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-3098.370
Langevin prior  16/ 40: energy=-3098.370
2022-12-29 01:15:37,838 - 8:06:21 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-3156.451
Langevin prior  21/ 40: energy=-3156.451
2022-12-29 01:15:37,844 - 8:06:21 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3166.790
Langevin prior  26/ 40: energy=-3166.790
2022-12-29 01:15:37,852 - 8:06:21 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3255.105
Langevin prior  31/ 40: energy=-3255.105
2022-12-29 01:15:37,861 - 8:06:21 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3258.310
Langevin prior  36/ 40: energy=-3258.310
2022-12-29 01:15:37,868 - 8:06:21 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3249.418
Langevin prior  40/ 40: energy=-3249.418
2022-12-29 01:15:40,493 - 8:06:23 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-050-test-greedy.txt
Generation: 188 batches
2022-12-29 01:15:46,180 - 8:06:29 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:15:47,954 - 8:06:31 - 1.8s - INFO - root - Generation Done
Generation Done
2022-12-29 01:15:47,954 - 8:06:31 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:15:50,398 - 8:06:33 - 2.4s - INFO - root - --- bleu: BLEU = 31.36, 54.7/33.5/27.4/24.3 (BP=0.943, ratio=0.945, hyp_len=160377, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.36, 54.7/33.5/27.4/24.3 (BP=0.943, ratio=0.945, hyp_len=160377, ref_len=169777)
--- bleu: BLEU = 31.36, 54.7/33.5/27.4/24.3 (BP=0.943, ratio=0.945, hyp_len=160377, ref_len=169777)

2022-12-29 01:15:50,398 - 8:06:33 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:15:50,400 - 8:06:33 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:16:00,291 - 8:06:43 - 9.9s - INFO - root - batch/max_batch/ep:   100/   529/    51 rec:    8.396 mi: 2.15253258 zkl:  164.927 cd:  -52.197 pos_prob:   56.507 prob_neg:  108.703 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    51 rec:    8.396 mi: 2.15253258 zkl:  164.927 cd:  -52.197 pos_prob:   56.507 prob_neg:  108.703 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:16:09,579 - 8:06:52 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    51 rec:   11.390 mi: 2.20702124 zkl:  170.410 cd:  -55.235 pos_prob:   54.898 prob_neg:  110.133 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    51 rec:   11.390 mi: 2.20702124 zkl:  170.410 cd:  -55.235 pos_prob:   54.898 prob_neg:  110.133 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:16:18,813 - 8:07:02 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    51 rec:   10.720 mi: 2.31777763 zkl:  167.101 cd:  -49.476 pos_prob:   55.550 prob_neg:  105.025 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    51 rec:   10.720 mi: 2.31777763 zkl:  167.101 cd:  -49.476 pos_prob:   55.550 prob_neg:  105.025 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:16:28,316 - 8:07:11 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    51 rec:   13.889 mi: 2.47554088 zkl:  165.559 cd:  -59.959 pos_prob:   48.279 prob_neg:  108.238 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    51 rec:   13.889 mi: 2.47554088 zkl:  165.559 cd:  -59.959 pos_prob:   48.279 prob_neg:  108.238 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:16:37,509 - 8:07:20 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    51 rec:   12.437 mi: 2.27504516 zkl:  168.634 cd:  -63.863 pos_prob:   46.823 prob_neg:  110.686 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    51 rec:   12.437 mi: 2.27504516 zkl:  168.634 cd:  -63.863 pos_prob:   46.823 prob_neg:  110.686 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:16:37,521 - 8:07:20 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-307.141
Langevin prior   1/ 40: energy=-307.141
2022-12-29 01:16:37,527 - 8:07:20 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1613.074
Langevin prior   6/ 40: energy=-1613.074
2022-12-29 01:16:37,533 - 8:07:20 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2404.687
Langevin prior  11/ 40: energy=-2404.687
2022-12-29 01:16:37,538 - 8:07:20 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2868.697
Langevin prior  16/ 40: energy=-2868.697
2022-12-29 01:16:37,545 - 8:07:20 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-3035.346
Langevin prior  21/ 40: energy=-3035.346
2022-12-29 01:16:37,551 - 8:07:20 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3261.378
Langevin prior  26/ 40: energy=-3261.378
2022-12-29 01:16:37,560 - 8:07:20 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3334.651
Langevin prior  31/ 40: energy=-3334.651
2022-12-29 01:16:37,568 - 8:07:20 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3318.055
Langevin prior  36/ 40: energy=-3318.055
2022-12-29 01:16:37,576 - 8:07:20 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3351.404
Langevin prior  40/ 40: energy=-3351.404
2022-12-29 01:16:40,155 - 8:07:23 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-051-test-greedy.txt
Generation: 188 batches
2022-12-29 01:16:45,829 - 8:07:29 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:16:47,327 - 8:07:30 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:16:47,327 - 8:07:30 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:16:49,751 - 8:07:32 - 2.4s - INFO - root - --- bleu: BLEU = 31.14, 55.1/33.6/27.6/24.5 (BP=0.932, ratio=0.934, hyp_len=158542, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.14, 55.1/33.6/27.6/24.5 (BP=0.932, ratio=0.934, hyp_len=158542, ref_len=169777)
--- bleu: BLEU = 31.14, 55.1/33.6/27.6/24.5 (BP=0.932, ratio=0.934, hyp_len=158542, ref_len=169777)

2022-12-29 01:16:49,752 - 8:07:32 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:16:49,753 - 8:07:32 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:16:59,245 - 8:07:42 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    52 rec:   10.874 mi: 2.33260918 zkl:  104.603 cd:  -34.358 pos_prob:   85.638 prob_neg:  119.997 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    52 rec:   10.874 mi: 2.33260918 zkl:  104.603 cd:  -34.358 pos_prob:   85.638 prob_neg:  119.997 kl_weight:    0.062 do_ae_train: False
2022-12-29 01:17:08,692 - 8:07:51 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    52 rec:   12.076 mi: 2.39923811 zkl:   93.009 cd:  -21.284 pos_prob:   86.230 prob_neg:  107.514 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    52 rec:   12.076 mi: 2.39923811 zkl:   93.009 cd:  -21.284 pos_prob:   86.230 prob_neg:  107.514 kl_weight:    0.125 do_ae_train: False
2022-12-29 01:17:18,030 - 8:08:01 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    52 rec:   10.987 mi: 2.54892945 zkl:   96.998 cd:  -11.914 pos_prob:   97.625 prob_neg:  109.539 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    52 rec:   10.987 mi: 2.54892945 zkl:   96.998 cd:  -11.914 pos_prob:   97.625 prob_neg:  109.539 kl_weight:    0.188 do_ae_train: False
2022-12-29 01:17:27,434 - 8:08:10 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    52 rec:   13.293 mi: 2.64335132 zkl:   86.624 cd:  -12.614 pos_prob:   95.952 prob_neg:  108.566 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    52 rec:   13.293 mi: 2.64335132 zkl:   86.624 cd:  -12.614 pos_prob:   95.952 prob_neg:  108.566 kl_weight:    0.251 do_ae_train: False
2022-12-29 01:17:36,768 - 8:08:19 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    52 rec:   18.768 mi: 2.43371725 zkl:   89.963 cd:   -2.027 pos_prob:  100.530 prob_neg:  102.557 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    52 rec:   18.768 mi: 2.43371725 zkl:   89.963 cd:   -2.027 pos_prob:  100.530 prob_neg:  102.557 kl_weight:    0.314 do_ae_train: False
2022-12-29 01:17:36,781 - 8:08:19 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-386.058
Langevin prior   1/ 40: energy=-386.058
2022-12-29 01:17:36,786 - 8:08:19 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1808.533
Langevin prior   6/ 40: energy=-1808.533
2022-12-29 01:17:36,791 - 8:08:19 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2506.591
Langevin prior  11/ 40: energy=-2506.591
2022-12-29 01:17:36,797 - 8:08:19 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2938.636
Langevin prior  16/ 40: energy=-2938.636
2022-12-29 01:17:36,802 - 8:08:19 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-3105.057
Langevin prior  21/ 40: energy=-3105.057
2022-12-29 01:17:36,808 - 8:08:20 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3172.752
Langevin prior  26/ 40: energy=-3172.752
2022-12-29 01:17:36,814 - 8:08:20 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3377.269
Langevin prior  31/ 40: energy=-3377.269
2022-12-29 01:17:36,821 - 8:08:20 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3273.661
Langevin prior  36/ 40: energy=-3273.661
2022-12-29 01:17:36,827 - 8:08:20 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3200.160
Langevin prior  40/ 40: energy=-3200.160
2022-12-29 01:17:39,515 - 8:08:22 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-052-test-greedy.txt
Generation: 188 batches
2022-12-29 01:17:45,227 - 8:08:28 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:17:46,759 - 8:08:29 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:17:46,759 - 8:08:29 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:17:49,215 - 8:08:32 - 2.5s - INFO - root - --- bleu: BLEU = 30.34, 52.6/31.8/26.0/23.0 (BP=0.960, ratio=0.961, hyp_len=163083, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.34, 52.6/31.8/26.0/23.0 (BP=0.960, ratio=0.961, hyp_len=163083, ref_len=169777)
--- bleu: BLEU = 30.34, 52.6/31.8/26.0/23.0 (BP=0.960, ratio=0.961, hyp_len=163083, ref_len=169777)

2022-12-29 01:17:49,216 - 8:08:32 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:17:49,217 - 8:08:32 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:17:58,561 - 8:08:41 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    53 rec:   18.054 mi: 2.56928897 zkl:   85.218 cd:   -5.143 pos_prob:  101.703 prob_neg:  106.846 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    53 rec:   18.054 mi: 2.56928897 zkl:   85.218 cd:   -5.143 pos_prob:  101.703 prob_neg:  106.846 kl_weight:    0.396 do_ae_train: False
2022-12-29 01:18:08,068 - 8:08:51 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    53 rec:   16.044 mi: 2.41738057 zkl:   86.726 cd:    4.282 pos_prob:  110.044 prob_neg:  105.762 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    53 rec:   16.044 mi: 2.41738057 zkl:   86.726 cd:    4.282 pos_prob:  110.044 prob_neg:  105.762 kl_weight:    0.459 do_ae_train: False
2022-12-29 01:18:17,689 - 8:09:00 - 9.6s - INFO - root - batch/max_batch/ep:   300/   529/    53 rec:   16.979 mi: 2.44161510 zkl:   84.599 cd:   -2.110 pos_prob:  104.239 prob_neg:  106.349 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    53 rec:   16.979 mi: 2.44161510 zkl:   84.599 cd:   -2.110 pos_prob:  104.239 prob_neg:  106.349 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:18:27,369 - 8:09:10 - 9.7s - INFO - root - batch/max_batch/ep:   400/   529/    53 rec:   17.898 mi: 2.45994115 zkl:   77.951 cd:   -6.529 pos_prob:  101.428 prob_neg:  107.957 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    53 rec:   17.898 mi: 2.45994115 zkl:   77.951 cd:   -6.529 pos_prob:  101.428 prob_neg:  107.957 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:18:37,229 - 8:09:20 - 9.9s - INFO - root - batch/max_batch/ep:   500/   529/    53 rec:   18.568 mi: 2.17265511 zkl:   76.878 cd:   -3.379 pos_prob:   99.217 prob_neg:  102.596 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    53 rec:   18.568 mi: 2.17265511 zkl:   76.878 cd:   -3.379 pos_prob:   99.217 prob_neg:  102.596 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:18:37,241 - 8:09:20 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-355.704
Langevin prior   1/ 40: energy=-355.704
2022-12-29 01:18:37,247 - 8:09:20 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1632.129
Langevin prior   6/ 40: energy=-1632.129
2022-12-29 01:18:37,252 - 8:09:20 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2472.028
Langevin prior  11/ 40: energy=-2472.028
2022-12-29 01:18:37,257 - 8:09:20 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2817.374
Langevin prior  16/ 40: energy=-2817.374
2022-12-29 01:18:37,262 - 8:09:20 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-3007.866
Langevin prior  21/ 40: energy=-3007.866
2022-12-29 01:18:37,268 - 8:09:20 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3050.946
Langevin prior  26/ 40: energy=-3050.946
2022-12-29 01:18:37,276 - 8:09:20 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3056.486
Langevin prior  31/ 40: energy=-3056.486
2022-12-29 01:18:37,284 - 8:09:20 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2988.286
Langevin prior  36/ 40: energy=-2988.286
2022-12-29 01:18:37,291 - 8:09:20 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2956.072
Langevin prior  40/ 40: energy=-2956.072
2022-12-29 01:28:23,121 - 8:19:06 - 585.8s - INFO - root - Negative Log-likehood -185.700201
200 done. -161.69240059720417
400 done. -156.38795825276407
600 done. -155.87088537732356
800 done. -156.9261277231088
1000 done. -155.8479970682694
1200 done. -156.6166578024515
1400 done. -157.7611300297914
1600 done. -155.9236428611973
1800 done. -155.3750753374186
2000 done. -173.34601782262862
2200 done. -184.94712417068678
2400 done. -197.59022325577715
2600 done. -208.42001996050888
2800 done. -215.60191397856406
3000 done. -224.87491851592688
3200 done. -230.70909915624756
3400 done. -238.1300955339468
3600 done. -242.76206587164913
3800 done. -248.19551360030613
4000 done. -252.13706461068588
4200 done. -242.28786952853153
4400 done. -232.12726329207322
4600 done. -223.07469426297345
4800 done. -214.6840279634102
5000 done. -207.19449965764375
5200 done. -200.12290483135914
5400 done. -193.6772074314536
5600 done. -187.687752917213
Negative Log-likehood -185.700201
2022-12-29 01:28:23,121 - 8:19:06 - 0.0s - INFO - root - log-likelihood:   -185.700
log-likelihood:   -185.700
2022-12-29 01:29:10,892 - 8:19:54 - 47.8s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-053-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 01:29:10,913 - 8:19:54 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-053-test-greedy.txt
Generation: 188 batches
2022-12-29 01:29:16,570 - 8:19:59 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:29:18,087 - 8:20:01 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:29:18,087 - 8:20:01 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:29:20,529 - 8:20:03 - 2.4s - INFO - root - --- bleu: BLEU = 29.99, 52.6/32.1/26.4/23.4 (BP=0.938, ratio=0.940, hyp_len=159608, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.99, 52.6/32.1/26.4/23.4 (BP=0.938, ratio=0.940, hyp_len=159608, ref_len=169777)
--- bleu: BLEU = 29.99, 52.6/32.1/26.4/23.4 (BP=0.938, ratio=0.940, hyp_len=159608, ref_len=169777)

2022-12-29 01:29:20,529 - 8:20:03 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:29:20,531 - 8:20:03 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:29:30,011 - 8:20:13 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    54 rec:   16.115 mi: 2.34753513 zkl:   73.983 cd:   -8.744 pos_prob:   97.937 prob_neg:  106.681 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    54 rec:   16.115 mi: 2.34753513 zkl:   73.983 cd:   -8.744 pos_prob:   97.937 prob_neg:  106.681 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:29:39,645 - 8:20:22 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/    54 rec:   20.094 mi: 2.39798832 zkl:   77.665 cd:    0.769 pos_prob:  104.800 prob_neg:  104.031 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    54 rec:   20.094 mi: 2.39798832 zkl:   77.665 cd:    0.769 pos_prob:  104.800 prob_neg:  104.031 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:29:49,064 - 8:20:32 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    54 rec:   17.387 mi: 2.47478962 zkl:   78.960 cd:   -0.779 pos_prob:  105.681 prob_neg:  106.459 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    54 rec:   17.387 mi: 2.47478962 zkl:   78.960 cd:   -0.779 pos_prob:  105.681 prob_neg:  106.459 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:29:58,732 - 8:20:41 - 9.7s - INFO - root - batch/max_batch/ep:   400/   529/    54 rec:   19.840 mi: 2.50125074 zkl:   82.101 cd:   -4.213 pos_prob:  103.704 prob_neg:  107.917 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    54 rec:   19.840 mi: 2.50125074 zkl:   82.101 cd:   -4.213 pos_prob:  103.704 prob_neg:  107.917 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:30:08,217 - 8:20:51 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    54 rec:   17.355 mi: 2.59616017 zkl:   74.970 cd:   -4.934 pos_prob:  104.143 prob_neg:  109.076 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    54 rec:   17.355 mi: 2.59616017 zkl:   74.970 cd:   -4.934 pos_prob:  104.143 prob_neg:  109.076 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:30:08,230 - 8:20:51 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-335.667
Langevin prior   1/ 40: energy=-335.667
2022-12-29 01:30:08,235 - 8:20:51 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1783.239
Langevin prior   6/ 40: energy=-1783.239
2022-12-29 01:30:08,240 - 8:20:51 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2368.937
Langevin prior  11/ 40: energy=-2368.937
2022-12-29 01:30:08,246 - 8:20:51 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2760.271
Langevin prior  16/ 40: energy=-2760.271
2022-12-29 01:30:08,251 - 8:20:51 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2854.335
Langevin prior  21/ 40: energy=-2854.335
2022-12-29 01:30:08,257 - 8:20:51 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3019.386
Langevin prior  26/ 40: energy=-3019.386
2022-12-29 01:30:08,264 - 8:20:51 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3090.088
Langevin prior  31/ 40: energy=-3090.088
2022-12-29 01:30:08,272 - 8:20:51 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3133.825
Langevin prior  36/ 40: energy=-3133.825
2022-12-29 01:30:08,278 - 8:20:51 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3219.681
Langevin prior  40/ 40: energy=-3219.681
2022-12-29 01:39:53,684 - 8:30:36 - 585.4s - INFO - root - Negative Log-likehood -186.291851
200 done. -162.11287750331397
400 done. -156.58464415577413
600 done. -155.89055271243421
800 done. -156.78870305214878
1000 done. -155.60670259800833
1200 done. -156.32341496527943
1400 done. -157.42952521914265
1600 done. -155.64034906148436
1800 done. -155.18915903057007
2000 done. -173.5370092372739
2200 done. -185.23685004326046
2400 done. -197.81503047997091
2600 done. -208.5776157350423
2800 done. -215.8133734317987
3000 done. -225.32976554437207
3200 done. -231.24937946301827
3400 done. -238.72230015117105
3600 done. -243.49325726258184
3800 done. -248.9592337005466
4000 done. -252.8689970261476
4200 done. -242.96842011229992
4400 done. -232.8133755233812
4600 done. -223.73620200090824
4800 done. -215.33317121995893
5000 done. -207.82127379583895
5200 done. -200.72448224914882
5400 done. -194.2728345925096
5600 done. -188.27421820401793
Negative Log-likehood -186.291851
2022-12-29 01:39:53,685 - 8:30:36 - 0.0s - INFO - root - log-likelihood:   -186.292
log-likelihood:   -186.292
2022-12-29 01:40:41,455 - 8:31:24 - 47.8s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-054-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 01:40:41,458 - 8:31:24 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-054-test-greedy.txt
Generation: 188 batches
2022-12-29 01:40:47,150 - 8:31:30 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:40:48,700 - 8:31:31 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:40:48,700 - 8:31:31 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:40:51,169 - 8:31:34 - 2.5s - INFO - root - --- bleu: BLEU = 29.87, 51.6/31.5/25.8/22.9 (BP=0.953, ratio=0.954, hyp_len=162008, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.87, 51.6/31.5/25.8/22.9 (BP=0.953, ratio=0.954, hyp_len=162008, ref_len=169777)
--- bleu: BLEU = 29.87, 51.6/31.5/25.8/22.9 (BP=0.953, ratio=0.954, hyp_len=162008, ref_len=169777)

2022-12-29 01:40:51,170 - 8:31:34 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:40:51,171 - 8:31:34 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:41:00,543 - 8:31:43 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    55 rec:   12.001 mi: 2.56730819 zkl:  163.399 cd:  -13.898 pos_prob:   92.858 prob_neg:  106.755 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    55 rec:   12.001 mi: 2.56730819 zkl:  163.399 cd:  -13.898 pos_prob:   92.858 prob_neg:  106.755 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:41:09,999 - 8:31:53 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    55 rec:   11.764 mi: 2.34372854 zkl:  161.673 cd:  -27.917 pos_prob:   80.229 prob_neg:  108.146 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    55 rec:   11.764 mi: 2.34372854 zkl:  161.673 cd:  -27.917 pos_prob:   80.229 prob_neg:  108.146 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:41:19,271 - 8:32:02 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    55 rec:   11.035 mi: 2.46355557 zkl:  165.112 cd:  -36.869 pos_prob:   72.850 prob_neg:  109.719 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    55 rec:   11.035 mi: 2.46355557 zkl:  165.112 cd:  -36.869 pos_prob:   72.850 prob_neg:  109.719 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:41:28,538 - 8:32:11 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    55 rec:   13.387 mi: 2.32230186 zkl:  165.509 cd:  -36.642 pos_prob:   67.123 prob_neg:  103.765 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    55 rec:   13.387 mi: 2.32230186 zkl:  165.509 cd:  -36.642 pos_prob:   67.123 prob_neg:  103.765 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:41:37,710 - 8:32:20 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    55 rec:   15.002 mi: 2.37566352 zkl:  166.281 cd:  -48.569 pos_prob:   64.021 prob_neg:  112.590 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    55 rec:   15.002 mi: 2.37566352 zkl:  166.281 cd:  -48.569 pos_prob:   64.021 prob_neg:  112.590 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:41:37,722 - 8:32:20 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-364.558
Langevin prior   1/ 40: energy=-364.558
2022-12-29 01:41:37,727 - 8:32:20 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1838.261
Langevin prior   6/ 40: energy=-1838.261
2022-12-29 01:41:37,732 - 8:32:20 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2636.778
Langevin prior  11/ 40: energy=-2636.778
2022-12-29 01:41:37,737 - 8:32:20 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2974.871
Langevin prior  16/ 40: energy=-2974.871
2022-12-29 01:41:37,743 - 8:32:20 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-3205.006
Langevin prior  21/ 40: energy=-3205.006
2022-12-29 01:41:37,748 - 8:32:20 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3255.772
Langevin prior  26/ 40: energy=-3255.772
2022-12-29 01:41:37,754 - 8:32:20 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3308.946
Langevin prior  31/ 40: energy=-3308.946
2022-12-29 01:41:37,760 - 8:32:20 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3346.092
Langevin prior  36/ 40: energy=-3346.092
2022-12-29 01:41:37,766 - 8:32:20 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3238.542
Langevin prior  40/ 40: energy=-3238.542
2022-12-29 01:41:40,430 - 8:32:23 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-055-test-greedy.txt
Generation: 188 batches
2022-12-29 01:41:46,118 - 8:32:29 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:41:47,644 - 8:32:30 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:41:47,644 - 8:32:30 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:41:50,132 - 8:32:33 - 2.5s - INFO - root - --- bleu: BLEU = 31.01, 53.6/32.7/26.9/23.8 (BP=0.953, ratio=0.954, hyp_len=161948, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.01, 53.6/32.7/26.9/23.8 (BP=0.953, ratio=0.954, hyp_len=161948, ref_len=169777)
--- bleu: BLEU = 31.01, 53.6/32.7/26.9/23.8 (BP=0.953, ratio=0.954, hyp_len=161948, ref_len=169777)

2022-12-29 01:41:50,133 - 8:32:33 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:41:50,134 - 8:32:33 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:41:59,435 - 8:32:42 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    56 rec:   12.452 mi: 2.44847798 zkl:  167.181 cd:  -57.457 pos_prob:   53.343 prob_neg:  110.800 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    56 rec:   12.452 mi: 2.44847798 zkl:  167.181 cd:  -57.457 pos_prob:   53.343 prob_neg:  110.800 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:42:08,685 - 8:32:51 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    56 rec:    8.916 mi: 2.40675998 zkl:  166.257 cd:  -45.583 pos_prob:   60.090 prob_neg:  105.673 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    56 rec:    8.916 mi: 2.40675998 zkl:  166.257 cd:  -45.583 pos_prob:   60.090 prob_neg:  105.673 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:42:18,665 - 8:33:01 - 10.0s - INFO - root - batch/max_batch/ep:   300/   529/    56 rec:    9.332 mi: 2.19790912 zkl:  168.162 cd:  -47.349 pos_prob:   61.156 prob_neg:  108.505 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    56 rec:    9.332 mi: 2.19790912 zkl:  168.162 cd:  -47.349 pos_prob:   61.156 prob_neg:  108.505 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:42:29,168 - 8:33:12 - 10.5s - INFO - root - batch/max_batch/ep:   400/   529/    56 rec:   12.326 mi: 2.49411941 zkl:  168.361 cd:  -51.778 pos_prob:   54.633 prob_neg:  106.411 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    56 rec:   12.326 mi: 2.49411941 zkl:  168.361 cd:  -51.778 pos_prob:   54.633 prob_neg:  106.411 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:42:38,766 - 8:33:21 - 9.6s - INFO - root - batch/max_batch/ep:   500/   529/    56 rec:   12.779 mi: 2.30114913 zkl:  162.981 cd:  -59.838 pos_prob:   50.477 prob_neg:  110.315 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    56 rec:   12.779 mi: 2.30114913 zkl:  162.981 cd:  -59.838 pos_prob:   50.477 prob_neg:  110.315 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:42:38,782 - 8:33:21 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-407.940
Langevin prior   1/ 40: energy=-407.940
2022-12-29 01:42:38,792 - 8:33:21 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1738.852
Langevin prior   6/ 40: energy=-1738.852
2022-12-29 01:42:38,803 - 8:33:21 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2539.629
Langevin prior  11/ 40: energy=-2539.629
2022-12-29 01:42:38,815 - 8:33:22 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2992.493
Langevin prior  16/ 40: energy=-2992.493
2022-12-29 01:42:38,826 - 8:33:22 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-3232.229
Langevin prior  21/ 40: energy=-3232.229
2022-12-29 01:42:38,837 - 8:33:22 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3272.357
Langevin prior  26/ 40: energy=-3272.357
2022-12-29 01:42:38,848 - 8:33:22 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3324.097
Langevin prior  31/ 40: energy=-3324.097
2022-12-29 01:42:38,861 - 8:33:22 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3293.904
Langevin prior  36/ 40: energy=-3293.904
2022-12-29 01:42:38,871 - 8:33:22 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3294.626
Langevin prior  40/ 40: energy=-3294.626
2022-12-29 01:42:41,917 - 8:33:25 - 3.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-056-test-greedy.txt
Generation: 188 batches
2022-12-29 01:42:47,587 - 8:33:30 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:42:49,353 - 8:33:32 - 1.8s - INFO - root - Generation Done
Generation Done
2022-12-29 01:42:49,353 - 8:33:32 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:42:51,783 - 8:33:34 - 2.4s - INFO - root - --- bleu: BLEU = 31.09, 54.2/33.2/27.2/24.2 (BP=0.942, ratio=0.944, hyp_len=160259, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.09, 54.2/33.2/27.2/24.2 (BP=0.942, ratio=0.944, hyp_len=160259, ref_len=169777)
--- bleu: BLEU = 31.09, 54.2/33.2/27.2/24.2 (BP=0.942, ratio=0.944, hyp_len=160259, ref_len=169777)

2022-12-29 01:42:51,784 - 8:33:34 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:42:51,785 - 8:33:34 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:43:00,953 - 8:33:44 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    57 rec:    9.538 mi: 2.19933677 zkl:  169.603 cd:  -48.149 pos_prob:   60.896 prob_neg:  109.045 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    57 rec:    9.538 mi: 2.19933677 zkl:  169.603 cd:  -48.149 pos_prob:   60.896 prob_neg:  109.045 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:43:10,164 - 8:33:53 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    57 rec:   11.709 mi: 2.25930023 zkl:  165.173 cd:  -51.947 pos_prob:   53.603 prob_neg:  105.550 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    57 rec:   11.709 mi: 2.25930023 zkl:  165.173 cd:  -51.947 pos_prob:   53.603 prob_neg:  105.550 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:43:19,204 - 8:34:02 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    57 rec:   13.632 mi: 2.37736607 zkl:  170.548 cd:  -58.605 pos_prob:   46.380 prob_neg:  104.986 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    57 rec:   13.632 mi: 2.37736607 zkl:  170.548 cd:  -58.605 pos_prob:   46.380 prob_neg:  104.986 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:43:28,407 - 8:34:11 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    57 rec:    9.757 mi: 2.27940154 zkl:  168.917 cd:  -55.180 pos_prob:   52.593 prob_neg:  107.773 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    57 rec:    9.757 mi: 2.27940154 zkl:  168.917 cd:  -55.180 pos_prob:   52.593 prob_neg:  107.773 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:43:37,623 - 8:34:20 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    57 rec:   11.015 mi: 2.18810105 zkl:  171.668 cd:  -58.516 pos_prob:   52.401 prob_neg:  110.917 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    57 rec:   11.015 mi: 2.18810105 zkl:  171.668 cd:  -58.516 pos_prob:   52.401 prob_neg:  110.917 kl_weight:    0.000 do_ae_train: True
2022-12-29 01:43:37,636 - 8:34:20 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-384.772
Langevin prior   1/ 40: energy=-384.772
2022-12-29 01:43:37,642 - 8:34:20 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1745.436
Langevin prior   6/ 40: energy=-1745.436
2022-12-29 01:43:37,647 - 8:34:20 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2512.304
Langevin prior  11/ 40: energy=-2512.304
2022-12-29 01:43:37,652 - 8:34:20 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2827.279
Langevin prior  16/ 40: energy=-2827.279
2022-12-29 01:43:37,658 - 8:34:20 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2924.340
Langevin prior  21/ 40: energy=-2924.340
2022-12-29 01:43:37,663 - 8:34:20 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3061.057
Langevin prior  26/ 40: energy=-3061.057
2022-12-29 01:43:37,670 - 8:34:20 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3104.039
Langevin prior  31/ 40: energy=-3104.039
2022-12-29 01:43:37,678 - 8:34:20 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3169.615
Langevin prior  36/ 40: energy=-3169.615
2022-12-29 01:43:37,685 - 8:34:20 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3182.773
Langevin prior  40/ 40: energy=-3182.773
2022-12-29 01:43:40,348 - 8:34:23 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-057-test-greedy.txt
Generation: 188 batches
2022-12-29 01:43:46,042 - 8:34:29 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:43:47,556 - 8:34:30 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:43:47,556 - 8:34:30 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:43:50,005 - 8:34:33 - 2.4s - INFO - root - --- bleu: BLEU = 31.26, 54.3/33.4/27.4/24.4 (BP=0.941, ratio=0.943, hyp_len=160108, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.26, 54.3/33.4/27.4/24.4 (BP=0.941, ratio=0.943, hyp_len=160108, ref_len=169777)
--- bleu: BLEU = 31.26, 54.3/33.4/27.4/24.4 (BP=0.941, ratio=0.943, hyp_len=160108, ref_len=169777)

2022-12-29 01:43:50,006 - 8:34:33 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:43:50,007 - 8:34:33 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:43:59,460 - 8:34:42 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    58 rec:   11.098 mi: 2.62864923 zkl:  108.608 cd:  -27.058 pos_prob:   81.302 prob_neg:  108.360 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    58 rec:   11.098 mi: 2.62864923 zkl:  108.608 cd:  -27.058 pos_prob:   81.302 prob_neg:  108.360 kl_weight:    0.062 do_ae_train: False
2022-12-29 01:44:08,780 - 8:34:51 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    58 rec:    9.694 mi: 2.29082108 zkl:   94.628 cd:  -19.189 pos_prob:   91.257 prob_neg:  110.445 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    58 rec:    9.694 mi: 2.29082108 zkl:   94.628 cd:  -19.189 pos_prob:   91.257 prob_neg:  110.445 kl_weight:    0.125 do_ae_train: False
2022-12-29 01:44:18,121 - 8:35:01 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    58 rec:   12.839 mi: 2.47278190 zkl:   90.543 cd:   -9.170 pos_prob:   97.748 prob_neg:  106.918 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    58 rec:   12.839 mi: 2.47278190 zkl:   90.543 cd:   -9.170 pos_prob:   97.748 prob_neg:  106.918 kl_weight:    0.188 do_ae_train: False
2022-12-29 01:44:27,637 - 8:35:10 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    58 rec:   14.998 mi: 2.36337447 zkl:   90.087 cd:   -8.412 pos_prob:  101.259 prob_neg:  109.670 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    58 rec:   14.998 mi: 2.36337447 zkl:   90.087 cd:   -8.412 pos_prob:  101.259 prob_neg:  109.670 kl_weight:    0.251 do_ae_train: False
2022-12-29 01:44:37,081 - 8:35:20 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    58 rec:   17.404 mi: 2.61861944 zkl:   90.655 cd:   -5.755 pos_prob:  103.361 prob_neg:  109.116 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    58 rec:   17.404 mi: 2.61861944 zkl:   90.655 cd:   -5.755 pos_prob:  103.361 prob_neg:  109.116 kl_weight:    0.314 do_ae_train: False
2022-12-29 01:44:37,094 - 8:35:20 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-445.915
Langevin prior   1/ 40: energy=-445.915
2022-12-29 01:44:37,100 - 8:35:20 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1853.323
Langevin prior   6/ 40: energy=-1853.323
2022-12-29 01:44:37,105 - 8:35:20 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2491.311
Langevin prior  11/ 40: energy=-2491.311
2022-12-29 01:44:37,110 - 8:35:20 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2875.449
Langevin prior  16/ 40: energy=-2875.449
2022-12-29 01:44:37,115 - 8:35:20 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-3025.082
Langevin prior  21/ 40: energy=-3025.082
2022-12-29 01:44:37,121 - 8:35:20 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3114.283
Langevin prior  26/ 40: energy=-3114.283
2022-12-29 01:44:37,129 - 8:35:20 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3142.700
Langevin prior  31/ 40: energy=-3142.700
2022-12-29 01:44:37,137 - 8:35:20 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3164.429
Langevin prior  36/ 40: energy=-3164.429
2022-12-29 01:44:37,144 - 8:35:20 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3192.447
Langevin prior  40/ 40: energy=-3192.447
2022-12-29 01:44:39,729 - 8:35:22 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-058-test-greedy.txt
Generation: 188 batches
2022-12-29 01:44:45,397 - 8:35:28 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:44:46,895 - 8:35:30 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:44:46,895 - 8:35:30 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:44:49,317 - 8:35:32 - 2.4s - INFO - root - --- bleu: BLEU = 30.42, 53.5/32.8/27.0/24.0 (BP=0.931, ratio=0.933, hyp_len=158479, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.42, 53.5/32.8/27.0/24.0 (BP=0.931, ratio=0.933, hyp_len=158479, ref_len=169777)
--- bleu: BLEU = 30.42, 53.5/32.8/27.0/24.0 (BP=0.931, ratio=0.933, hyp_len=158479, ref_len=169777)

2022-12-29 01:44:49,317 - 8:35:32 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:44:49,319 - 8:35:32 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:44:58,739 - 8:35:41 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    59 rec:    8.930 mi: 2.41712117 zkl:   81.275 cd:    0.939 pos_prob:  109.512 prob_neg:  108.573 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    59 rec:    8.930 mi: 2.41712117 zkl:   81.275 cd:    0.939 pos_prob:  109.512 prob_neg:  108.573 kl_weight:    0.396 do_ae_train: False
2022-12-29 01:45:08,587 - 8:35:51 - 9.8s - INFO - root - batch/max_batch/ep:   200/   529/    59 rec:   15.586 mi: 2.44386387 zkl:   85.072 cd:   -5.522 pos_prob:  102.907 prob_neg:  108.429 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    59 rec:   15.586 mi: 2.44386387 zkl:   85.072 cd:   -5.522 pos_prob:  102.907 prob_neg:  108.429 kl_weight:    0.459 do_ae_train: False
2022-12-29 01:45:18,127 - 8:36:01 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    59 rec:   13.768 mi: 2.24627566 zkl:   76.441 cd:    0.498 pos_prob:  108.133 prob_neg:  107.635 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    59 rec:   13.768 mi: 2.24627566 zkl:   76.441 cd:    0.498 pos_prob:  108.133 prob_neg:  107.635 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:45:27,928 - 8:36:11 - 9.8s - INFO - root - batch/max_batch/ep:   400/   529/    59 rec:   15.187 mi: 2.46287847 zkl:   76.118 cd:   -7.548 pos_prob:   99.700 prob_neg:  107.248 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    59 rec:   15.187 mi: 2.46287847 zkl:   76.118 cd:   -7.548 pos_prob:   99.700 prob_neg:  107.248 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:45:37,299 - 8:36:20 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    59 rec:   14.584 mi: 2.29607630 zkl:   75.924 cd:   -7.253 pos_prob:  101.223 prob_neg:  108.476 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    59 rec:   14.584 mi: 2.29607630 zkl:   75.924 cd:   -7.253 pos_prob:  101.223 prob_neg:  108.476 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:45:37,312 - 8:36:20 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-383.374
Langevin prior   1/ 40: energy=-383.374
2022-12-29 01:45:37,318 - 8:36:20 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1681.220
Langevin prior   6/ 40: energy=-1681.220
2022-12-29 01:45:37,324 - 8:36:20 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2418.179
Langevin prior  11/ 40: energy=-2418.179
2022-12-29 01:45:37,333 - 8:36:20 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2791.085
Langevin prior  16/ 40: energy=-2791.085
2022-12-29 01:45:37,342 - 8:36:20 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2905.869
Langevin prior  21/ 40: energy=-2905.869
2022-12-29 01:45:37,352 - 8:36:20 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-3025.742
Langevin prior  26/ 40: energy=-3025.742
2022-12-29 01:45:37,362 - 8:36:20 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3065.022
Langevin prior  31/ 40: energy=-3065.022
2022-12-29 01:45:37,373 - 8:36:20 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3108.941
Langevin prior  36/ 40: energy=-3108.941
2022-12-29 01:45:37,382 - 8:36:20 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3056.179
Langevin prior  40/ 40: energy=-3056.179
2022-12-29 01:55:22,776 - 8:46:05 - 585.4s - INFO - root - Negative Log-likehood -190.312620
200 done. -165.4444761374808
400 done. -160.30702375842355
600 done. -159.78143060130301
800 done. -160.60767649952047
1000 done. -159.35358968461796
1200 done. -160.0356888132661
1400 done. -161.24325991519584
1600 done. -159.37335849122914
1800 done. -158.85101243754593
2000 done. -177.55460212960463
2200 done. -189.50065469981695
2400 done. -202.66925977642003
2600 done. -213.76778565076654
2800 done. -221.1488235687968
3000 done. -230.47545386534594
3200 done. -236.4911475620757
3400 done. -244.06433537997913
3600 done. -248.84475481417883
3800 done. -254.3583414937802
4000 done. -258.4878478323258
4200 done. -248.35357975894726
4400 done. -237.98346072675562
4600 done. -228.6994371406546
4800 done. -220.07394930416353
5000 done. -212.38256848566982
5200 done. -205.11909307434843
5400 done. -198.49882093216635
5600 done. -192.34691907813627
Negative Log-likehood -190.312620
2022-12-29 01:55:22,776 - 8:46:05 - 0.0s - INFO - root - log-likelihood:   -190.313
log-likelihood:   -190.313
2022-12-29 01:56:10,071 - 8:46:53 - 47.3s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-059-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 01:56:10,077 - 8:46:53 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-059-test-greedy.txt
Generation: 188 batches
2022-12-29 01:56:15,745 - 8:46:58 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:56:17,269 - 8:47:00 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 01:56:17,270 - 8:47:00 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 01:56:19,730 - 8:47:02 - 2.5s - INFO - root - --- bleu: BLEU = 30.07, 52.0/31.6/25.8/22.9 (BP=0.958, ratio=0.959, hyp_len=162811, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.07, 52.0/31.6/25.8/22.9 (BP=0.958, ratio=0.959, hyp_len=162811, ref_len=169777)
--- bleu: BLEU = 30.07, 52.0/31.6/25.8/22.9 (BP=0.958, ratio=0.959, hyp_len=162811, ref_len=169777)

2022-12-29 01:56:19,731 - 8:47:02 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 01:56:19,732 - 8:47:02 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 01:56:29,016 - 8:47:12 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    60 rec:   14.102 mi: 2.37927961 zkl:   79.842 cd:   -5.298 pos_prob:  105.187 prob_neg:  110.485 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    60 rec:   14.102 mi: 2.37927961 zkl:   79.842 cd:   -5.298 pos_prob:  105.187 prob_neg:  110.485 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:56:38,252 - 8:47:21 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    60 rec:   16.075 mi: 2.51457691 zkl:   81.894 cd:   -4.239 pos_prob:  102.776 prob_neg:  107.015 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    60 rec:   16.075 mi: 2.51457691 zkl:   81.894 cd:   -4.239 pos_prob:  102.776 prob_neg:  107.015 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:56:47,759 - 8:47:30 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    60 rec:   15.295 mi: 2.39844084 zkl:   78.272 cd:   -5.086 pos_prob:  102.537 prob_neg:  107.623 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    60 rec:   15.295 mi: 2.39844084 zkl:   78.272 cd:   -5.086 pos_prob:  102.537 prob_neg:  107.623 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:56:57,312 - 8:47:40 - 9.6s - INFO - root - batch/max_batch/ep:   400/   529/    60 rec:   18.363 mi: 2.32368612 zkl:   79.097 cd:   -1.851 pos_prob:  101.439 prob_neg:  103.290 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    60 rec:   18.363 mi: 2.32368612 zkl:   79.097 cd:   -1.851 pos_prob:  101.439 prob_neg:  103.290 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:57:06,716 - 8:47:49 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    60 rec:   20.690 mi: 2.34533453 zkl:   78.083 cd:   -6.210 pos_prob:  103.520 prob_neg:  109.729 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    60 rec:   20.690 mi: 2.34533453 zkl:   78.083 cd:   -6.210 pos_prob:  103.520 prob_neg:  109.729 kl_weight:    0.500 do_ae_train: False
2022-12-29 01:57:06,728 - 8:47:49 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-329.572
Langevin prior   1/ 40: energy=-329.572
2022-12-29 01:57:06,734 - 8:47:49 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1567.105
Langevin prior   6/ 40: energy=-1567.105
2022-12-29 01:57:06,739 - 8:47:49 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2323.550
Langevin prior  11/ 40: energy=-2323.550
2022-12-29 01:57:06,744 - 8:47:49 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2699.857
Langevin prior  16/ 40: energy=-2699.857
2022-12-29 01:57:06,750 - 8:47:49 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2873.894
Langevin prior  21/ 40: energy=-2873.894
2022-12-29 01:57:06,756 - 8:47:49 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2935.459
Langevin prior  26/ 40: energy=-2935.459
2022-12-29 01:57:06,762 - 8:47:49 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-3038.824
Langevin prior  31/ 40: energy=-3038.824
2022-12-29 01:57:06,770 - 8:47:49 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-3044.334
Langevin prior  36/ 40: energy=-3044.334
2022-12-29 01:57:06,777 - 8:47:49 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-3048.829
Langevin prior  40/ 40: energy=-3048.829
2022-12-29 02:06:51,968 - 8:57:35 - 585.2s - INFO - root - Negative Log-likehood -189.381940
200 done. -165.37499915386346
400 done. -159.89600468055124
600 done. -159.14500283589075
800 done. -159.9714402940047
1000 done. -158.83077512005093
1200 done. -159.55698072463815
1400 done. -160.73474446045606
1600 done. -158.81836405327147
1800 done. -158.33774861136303
2000 done. -176.81327646066717
2200 done. -188.54270754014246
2400 done. -201.64482774924852
2600 done. -212.62130973249606
2800 done. -219.97110449066344
3000 done. -229.41299408391745
3200 done. -235.37669182595403
3400 done. -242.88784773036585
3600 done. -247.5764436472938
3800 done. -253.21797148952982
4000 done. -257.39042091459095
4200 done. -247.31704445679316
4400 done. -236.96749488931061
4600 done. -227.68243471937186
4800 done. -219.08735369367596
5000 done. -211.39756500280188
5200 done. -204.1392673392775
5400 done. -197.5432511150128
5600 done. -191.4091010079688
Negative Log-likehood -189.381940
2022-12-29 02:06:51,968 - 8:57:35 - 0.0s - INFO - root - log-likelihood:   -189.382
log-likelihood:   -189.382
2022-12-29 02:07:40,073 - 8:58:23 - 48.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-060-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 02:07:40,076 - 8:58:23 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-060-test-greedy.txt
Generation: 188 batches
2022-12-29 02:07:45,750 - 8:58:28 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:07:47,297 - 8:58:30 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:07:47,297 - 8:58:30 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:07:49,772 - 8:58:32 - 2.5s - INFO - root - --- bleu: BLEU = 30.01, 51.4/31.1/25.5/22.6 (BP=0.968, ratio=0.969, hyp_len=164469, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.01, 51.4/31.1/25.5/22.6 (BP=0.968, ratio=0.969, hyp_len=164469, ref_len=169777)
--- bleu: BLEU = 30.01, 51.4/31.1/25.5/22.6 (BP=0.968, ratio=0.969, hyp_len=164469, ref_len=169777)

2022-12-29 02:07:50,211 - 8:58:33 - 0.4s - INFO - root - task_id=2
task_id=2
Done loading corpus
Done loading corpus
/home/xiaodi/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
2022-12-29 02:08:03,293 - 8:58:46 - 13.1s - INFO - root - GMVAE4Lamol (
  (embedding): Embedding(40489, 512, padding_idx=40479), parameters=20730368
  (dec_embedding): Embedding(40489, 512, padding_idx=40479), parameters=20730368
  (x_encoder): EncoderRNN(
    (input_dropout): Dropout(p=0, inplace=False)
    (rnn): GRU(512, 512, batch_first=True, dropout=0.3, bidirectional=True)
  ), parameters=3151872
  (decoder): DecoderRNN(
    (input_dropout): Dropout(p=0.3, inplace=False)
    (rnn): GRU(552, 512, batch_first=True, dropout=0.3)
    (embedding): Embedding(40489, 512, padding_idx=40479)
    (output_dropout): Dropout(p=0.3, inplace=False)
  ), parameters=22367744
  (q_y_mean): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (q_y_logvar): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (dec_init_connector): LinearConnector(
    (linear): Linear(in_features=40, out_features=512, bias=False)
  ), parameters=20480
  (nll_loss): NLLEntropy(
    (nll_loss): NLLLoss()
  ), parameters=0
  (ebm): Sequential (
    (0): Linear(in_features=40, out_features=200, bias=True), weights=((200, 40), (200,)), parameters=8200
    (1): GELU(), weights=(), parameters=0
    (2): Linear(in_features=200, out_features=200, bias=True), weights=((200, 200), (200,)), parameters=40200
    (3): GELU(), weights=(), parameters=0
    (4): Linear(in_features=200, out_features=20, bias=True), weights=((20, 200), (20,)), parameters=4020
  ) Total Parameters=52420, parameters=52420
) Total Parameters=67135252
Done loading corpus
Max len 40 and min len 11 and avg len 31.746755
Max len 40 and min len 11 and avg len 31.068278
Max len 40 and min len 11 and avg len 31.068278
GMVAE4Lamol (
  (embedding): Embedding(40489, 512, padding_idx=40479), parameters=20730368
  (dec_embedding): Embedding(40489, 512, padding_idx=40479), parameters=20730368
  (x_encoder): EncoderRNN(
    (input_dropout): Dropout(p=0, inplace=False)
    (rnn): GRU(512, 512, batch_first=True, dropout=0.3, bidirectional=True)
  ), parameters=3151872
  (decoder): DecoderRNN(
    (input_dropout): Dropout(p=0.3, inplace=False)
    (rnn): GRU(552, 512, batch_first=True, dropout=0.3)
    (embedding): Embedding(40489, 512, padding_idx=40479)
    (output_dropout): Dropout(p=0.3, inplace=False)
  ), parameters=22367744
  (q_y_mean): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (q_y_logvar): Linear(in_features=1024, out_features=40, bias=True), parameters=41000
  (dec_init_connector): LinearConnector(
    (linear): Linear(in_features=40, out_features=512, bias=False)
  ), parameters=20480
  (nll_loss): NLLEntropy(
    (nll_loss): NLLLoss()
  ), parameters=0
  (ebm): Sequential (
    (0): Linear(in_features=40, out_features=200, bias=True), weights=((200, 40), (200,)), parameters=8200
    (1): GELU(), weights=(), parameters=0
    (2): Linear(in_features=200, out_features=200, bias=True), weights=((200, 200), (200,)), parameters=40200
    (3): GELU(), weights=(), parameters=0
    (4): Linear(in_features=200, out_features=20, bias=True), weights=((20, 200), (20,)), parameters=4020
  ) Total Parameters=52420, parameters=52420
) Total Parameters=67135252
2022-12-29 02:08:03,293 - 8:58:46 - 0.0s - INFO - root - **** Training Begins ****
**** Training Begins ****
2022-12-29 02:08:03,293 - 8:58:46 - 0.0s - INFO - root - **** Epoch 0/60 ****
**** Epoch 0/60 ****
2022-12-29 02:08:03,303 - 8:58:46 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:08:03,303 - 8:58:46 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:08:12,162 - 8:58:55 - 8.9s - INFO - root - batch/max_batch/ep:   100/   529/     1 rec:  174.556 mi: 0.00061417 zkl:  110.318 cd:   -0.013 pos_prob:    2.987 prob_neg:    3.000 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     1 rec:  174.556 mi: 0.00061417 zkl:  110.318 cd:   -0.013 pos_prob:    2.987 prob_neg:    3.000 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:08:21,186 - 8:59:04 - 9.0s - INFO - root - batch/max_batch/ep:   200/   529/     1 rec:  151.585 mi: 0.00174761 zkl:  126.757 cd:    0.001 pos_prob:    2.991 prob_neg:    2.990 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     1 rec:  151.585 mi: 0.00174761 zkl:  126.757 cd:    0.001 pos_prob:    2.991 prob_neg:    2.990 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:08:30,355 - 8:59:13 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/     1 rec:  140.889 mi: 0.00156903 zkl:  138.693 cd:    0.002 pos_prob:    2.985 prob_neg:    2.983 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     1 rec:  140.889 mi: 0.00156903 zkl:  138.693 cd:    0.002 pos_prob:    2.985 prob_neg:    2.983 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:08:39,403 - 8:59:22 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/     1 rec:  139.900 mi: 0.00277615 zkl:  160.006 cd:   -0.002 pos_prob:    2.988 prob_neg:    2.990 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     1 rec:  139.900 mi: 0.00277615 zkl:  160.006 cd:   -0.002 pos_prob:    2.988 prob_neg:    2.990 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:08:48,654 - 8:59:31 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/     1 rec:  117.148 mi: 0.00269818 zkl:  163.015 cd:   -0.000 pos_prob:    2.984 prob_neg:    2.984 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     1 rec:  117.148 mi: 0.00269818 zkl:  163.015 cd:   -0.000 pos_prob:    2.984 prob_neg:    2.984 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:08:48,666 - 8:59:31 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -89.643
Langevin prior   1/ 40: energy= -89.643
2022-12-29 02:08:48,671 - 8:59:31 - 0.0s - INFO - root - Langevin prior   6/ 40: energy= -89.630
Langevin prior   6/ 40: energy= -89.630
2022-12-29 02:08:48,676 - 8:59:31 - 0.0s - INFO - root - Langevin prior  11/ 40: energy= -89.652
Langevin prior  11/ 40: energy= -89.652
2022-12-29 02:08:48,681 - 8:59:31 - 0.0s - INFO - root - Langevin prior  16/ 40: energy= -89.732
Langevin prior  16/ 40: energy= -89.732
2022-12-29 02:08:48,687 - 8:59:31 - 0.0s - INFO - root - Langevin prior  21/ 40: energy= -89.716
Langevin prior  21/ 40: energy= -89.716
2022-12-29 02:08:48,693 - 8:59:31 - 0.0s - INFO - root - Langevin prior  26/ 40: energy= -89.749
Langevin prior  26/ 40: energy= -89.749
2022-12-29 02:08:48,699 - 8:59:31 - 0.0s - INFO - root - Langevin prior  31/ 40: energy= -89.768
Langevin prior  31/ 40: energy= -89.768
2022-12-29 02:08:48,707 - 8:59:31 - 0.0s - INFO - root - Langevin prior  36/ 40: energy= -89.624
Langevin prior  36/ 40: energy= -89.624
2022-12-29 02:08:48,714 - 8:59:31 - 0.0s - INFO - root - Langevin prior  40/ 40: energy= -89.694
Langevin prior  40/ 40: energy= -89.694
2022-12-29 02:08:51,351 - 8:59:34 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-001-test-greedy.txt
Generation: 188 batches
2022-12-29 02:08:57,112 - 8:59:40 - 5.8s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:08:58,463 - 8:59:41 - 1.4s - INFO - root - Generation Done
Generation Done
2022-12-29 02:08:58,463 - 8:59:41 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:09:00,694 - 8:59:43 - 2.2s - INFO - root - --- bleu: BLEU = 25.47, 51.7/29.4/22.3/19.0 (BP=0.898, ratio=0.903, hyp_len=153309, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 25.47, 51.7/29.4/22.3/19.0 (BP=0.898, ratio=0.903, hyp_len=153309, ref_len=169777)
--- bleu: BLEU = 25.47, 51.7/29.4/22.3/19.0 (BP=0.898, ratio=0.903, hyp_len=153309, ref_len=169777)

2022-12-29 02:09:00,694 - 8:59:43 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:09:00,696 - 8:59:43 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:09:09,969 - 8:59:53 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/     2 rec:  119.424 mi: 0.00410724 zkl:  183.426 cd:    0.003 pos_prob:    2.991 prob_neg:    2.987 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     2 rec:  119.424 mi: 0.00410724 zkl:  183.426 cd:    0.003 pos_prob:    2.991 prob_neg:    2.987 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:09:19,352 - 9:00:02 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/     2 rec:  113.942 mi: 0.00415540 zkl:  180.591 cd:   -0.004 pos_prob:    2.981 prob_neg:    2.985 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     2 rec:  113.942 mi: 0.00415540 zkl:  180.591 cd:   -0.004 pos_prob:    2.981 prob_neg:    2.985 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:09:28,433 - 9:00:11 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/     2 rec:   91.667 mi: 0.00474334 zkl:  191.436 cd:   -0.015 pos_prob:    2.978 prob_neg:    2.994 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     2 rec:   91.667 mi: 0.00474334 zkl:  191.436 cd:   -0.015 pos_prob:    2.978 prob_neg:    2.994 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:09:37,606 - 9:00:20 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/     2 rec:   92.704 mi: 0.00611663 zkl:  200.280 cd:   -0.003 pos_prob:    2.987 prob_neg:    2.990 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     2 rec:   92.704 mi: 0.00611663 zkl:  200.280 cd:   -0.003 pos_prob:    2.987 prob_neg:    2.990 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:09:46,759 - 9:00:29 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/     2 rec:   70.079 mi: 0.00544500 zkl:  202.756 cd:    0.005 pos_prob:    2.992 prob_neg:    2.987 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     2 rec:   70.079 mi: 0.00544500 zkl:  202.756 cd:    0.005 pos_prob:    2.992 prob_neg:    2.987 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:09:46,772 - 9:00:29 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -89.696
Langevin prior   1/ 40: energy= -89.696
2022-12-29 02:09:46,778 - 9:00:29 - 0.0s - INFO - root - Langevin prior   6/ 40: energy= -89.567
Langevin prior   6/ 40: energy= -89.567
2022-12-29 02:09:46,784 - 9:00:29 - 0.0s - INFO - root - Langevin prior  11/ 40: energy= -89.609
Langevin prior  11/ 40: energy= -89.609
2022-12-29 02:09:46,789 - 9:00:29 - 0.0s - INFO - root - Langevin prior  16/ 40: energy= -89.678
Langevin prior  16/ 40: energy= -89.678
2022-12-29 02:09:46,796 - 9:00:29 - 0.0s - INFO - root - Langevin prior  21/ 40: energy= -89.703
Langevin prior  21/ 40: energy= -89.703
2022-12-29 02:09:46,805 - 9:00:30 - 0.0s - INFO - root - Langevin prior  26/ 40: energy= -89.688
Langevin prior  26/ 40: energy= -89.688
2022-12-29 02:09:46,814 - 9:00:30 - 0.0s - INFO - root - Langevin prior  31/ 40: energy= -89.750
Langevin prior  31/ 40: energy= -89.750
2022-12-29 02:09:46,823 - 9:00:30 - 0.0s - INFO - root - Langevin prior  36/ 40: energy= -89.481
Langevin prior  36/ 40: energy= -89.481
2022-12-29 02:09:46,831 - 9:00:30 - 0.0s - INFO - root - Langevin prior  40/ 40: energy= -89.487
Langevin prior  40/ 40: energy= -89.487
2022-12-29 02:09:49,419 - 9:00:32 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-002-test-greedy.txt
Generation: 188 batches
2022-12-29 02:09:55,208 - 9:00:38 - 5.8s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:09:56,675 - 9:00:39 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:09:56,675 - 9:00:39 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:09:59,084 - 9:00:42 - 2.4s - INFO - root - --- bleu: BLEU = 28.88, 52.6/30.4/23.6/20.5 (BP=0.973, ratio=0.973, hyp_len=165257, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 28.88, 52.6/30.4/23.6/20.5 (BP=0.973, ratio=0.973, hyp_len=165257, ref_len=169777)
--- bleu: BLEU = 28.88, 52.6/30.4/23.6/20.5 (BP=0.973, ratio=0.973, hyp_len=165257, ref_len=169777)

2022-12-29 02:09:59,085 - 9:00:42 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:09:59,086 - 9:00:42 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:10:08,437 - 9:00:51 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/     3 rec:   72.039 mi: 0.00701642 zkl:  216.143 cd:    0.007 pos_prob:    2.991 prob_neg:    2.984 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     3 rec:   72.039 mi: 0.00701642 zkl:  216.143 cd:    0.007 pos_prob:    2.991 prob_neg:    2.984 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:10:17,849 - 9:01:01 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/     3 rec:   64.937 mi: 0.00625062 zkl:  209.874 cd:   -0.011 pos_prob:    2.983 prob_neg:    2.994 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     3 rec:   64.937 mi: 0.00625062 zkl:  209.874 cd:   -0.011 pos_prob:    2.983 prob_neg:    2.994 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:10:27,371 - 9:01:10 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/     3 rec:   71.125 mi: 0.00698543 zkl:  216.872 cd:    0.003 pos_prob:    2.992 prob_neg:    2.989 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     3 rec:   71.125 mi: 0.00698543 zkl:  216.872 cd:    0.003 pos_prob:    2.992 prob_neg:    2.989 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:10:36,673 - 9:01:19 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/     3 rec:   50.137 mi: 0.00736833 zkl:  222.558 cd:   -0.001 pos_prob:    2.991 prob_neg:    2.992 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     3 rec:   50.137 mi: 0.00736833 zkl:  222.558 cd:   -0.001 pos_prob:    2.991 prob_neg:    2.992 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:10:45,916 - 9:01:29 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/     3 rec:   76.832 mi: 0.00509143 zkl:  205.487 cd:   -0.005 pos_prob:    2.984 prob_neg:    2.989 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     3 rec:   76.832 mi: 0.00509143 zkl:  205.487 cd:   -0.005 pos_prob:    2.984 prob_neg:    2.989 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:10:45,929 - 9:01:29 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -89.480
Langevin prior   1/ 40: energy= -89.480
2022-12-29 02:10:45,935 - 9:01:29 - 0.0s - INFO - root - Langevin prior   6/ 40: energy= -89.600
Langevin prior   6/ 40: energy= -89.600
2022-12-29 02:10:45,940 - 9:01:29 - 0.0s - INFO - root - Langevin prior  11/ 40: energy= -89.498
Langevin prior  11/ 40: energy= -89.498
2022-12-29 02:10:45,945 - 9:01:29 - 0.0s - INFO - root - Langevin prior  16/ 40: energy= -89.471
Langevin prior  16/ 40: energy= -89.471
2022-12-29 02:10:45,951 - 9:01:29 - 0.0s - INFO - root - Langevin prior  21/ 40: energy= -89.585
Langevin prior  21/ 40: energy= -89.585
2022-12-29 02:10:45,957 - 9:01:29 - 0.0s - INFO - root - Langevin prior  26/ 40: energy= -89.738
Langevin prior  26/ 40: energy= -89.738
2022-12-29 02:10:45,965 - 9:01:29 - 0.0s - INFO - root - Langevin prior  31/ 40: energy= -89.639
Langevin prior  31/ 40: energy= -89.639
2022-12-29 02:10:45,974 - 9:01:29 - 0.0s - INFO - root - Langevin prior  36/ 40: energy= -89.639
Langevin prior  36/ 40: energy= -89.639
2022-12-29 02:10:45,982 - 9:01:29 - 0.0s - INFO - root - Langevin prior  40/ 40: energy= -89.677
Langevin prior  40/ 40: energy= -89.677
2022-12-29 02:10:48,636 - 9:01:31 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-003-test-greedy.txt
Generation: 188 batches
2022-12-29 02:10:54,361 - 9:01:37 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:10:55,827 - 9:01:39 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:10:55,828 - 9:01:39 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:10:58,204 - 9:01:41 - 2.4s - INFO - root - --- bleu: BLEU = 30.34, 55.5/32.7/25.9/22.8 (BP=0.943, ratio=0.944, hyp_len=160324, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.34, 55.5/32.7/25.9/22.8 (BP=0.943, ratio=0.944, hyp_len=160324, ref_len=169777)
--- bleu: BLEU = 30.34, 55.5/32.7/25.9/22.8 (BP=0.943, ratio=0.944, hyp_len=160324, ref_len=169777)

2022-12-29 02:10:58,204 - 9:01:41 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:10:58,205 - 9:01:41 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:11:07,580 - 9:01:50 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/     4 rec:   61.378 mi: 0.84674120 zkl:  185.564 cd:    3.051 pos_prob:    6.038 prob_neg:    2.987 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/     4 rec:   61.378 mi: 0.84674120 zkl:  185.564 cd:    3.051 pos_prob:    6.038 prob_neg:    2.987 kl_weight:    0.062 do_ae_train: False
2022-12-29 02:11:17,040 - 9:02:00 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/     4 rec:   47.103 mi: 1.27627802 zkl:  103.338 cd:    4.556 pos_prob:    7.570 prob_neg:    3.014 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/     4 rec:   47.103 mi: 1.27627802 zkl:  103.338 cd:    4.556 pos_prob:    7.570 prob_neg:    3.014 kl_weight:    0.125 do_ae_train: False
2022-12-29 02:11:26,560 - 9:02:09 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/     4 rec:   56.500 mi: 1.34155357 zkl:   75.881 cd:    6.161 pos_prob:    9.243 prob_neg:    3.082 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/     4 rec:   56.500 mi: 1.34155357 zkl:   75.881 cd:    6.161 pos_prob:    9.243 prob_neg:    3.082 kl_weight:    0.188 do_ae_train: False
2022-12-29 02:11:35,718 - 9:02:18 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/     4 rec:   67.937 mi: 1.46218610 zkl:   60.754 cd:    6.196 pos_prob:    9.544 prob_neg:    3.348 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/     4 rec:   67.937 mi: 1.46218610 zkl:   60.754 cd:    6.196 pos_prob:    9.544 prob_neg:    3.348 kl_weight:    0.251 do_ae_train: False
2022-12-29 02:11:45,000 - 9:02:28 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/     4 rec:   50.036 mi: 1.63269544 zkl:   49.328 cd:    6.884 pos_prob:   11.648 prob_neg:    4.764 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/     4 rec:   50.036 mi: 1.63269544 zkl:   49.328 cd:    6.884 pos_prob:   11.648 prob_neg:    4.764 kl_weight:    0.314 do_ae_train: False
2022-12-29 02:11:45,012 - 9:02:28 - 0.0s - INFO - root - Langevin prior   1/ 40: energy= -97.735
Langevin prior   1/ 40: energy= -97.735
2022-12-29 02:11:45,018 - 9:02:28 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-112.398
Langevin prior   6/ 40: energy=-112.398
2022-12-29 02:11:45,023 - 9:02:28 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-118.751
Langevin prior  11/ 40: energy=-118.751
2022-12-29 02:11:45,028 - 9:02:28 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-124.480
Langevin prior  16/ 40: energy=-124.480
2022-12-29 02:11:45,034 - 9:02:28 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-126.495
Langevin prior  21/ 40: energy=-126.495
2022-12-29 02:11:45,040 - 9:02:28 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-116.290
Langevin prior  26/ 40: energy=-116.290
2022-12-29 02:11:45,046 - 9:02:28 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-118.133
Langevin prior  31/ 40: energy=-118.133
2022-12-29 02:11:45,054 - 9:02:28 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-124.659
Langevin prior  36/ 40: energy=-124.659
2022-12-29 02:11:45,061 - 9:02:28 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-124.433
Langevin prior  40/ 40: energy=-124.433
2022-12-29 02:11:47,719 - 9:02:30 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-004-test-greedy.txt
Generation: 188 batches
2022-12-29 02:11:53,465 - 9:02:36 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:11:54,917 - 9:02:38 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:11:54,917 - 9:02:38 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:11:57,522 - 9:02:40 - 2.6s - INFO - root - --- bleu: BLEU = 28.89, 54.0/31.9/25.4/22.3 (BP=0.919, ratio=0.922, hyp_len=156597, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 28.89, 54.0/31.9/25.4/22.3 (BP=0.919, ratio=0.922, hyp_len=156597, ref_len=169777)
--- bleu: BLEU = 28.89, 54.0/31.9/25.4/22.3 (BP=0.919, ratio=0.922, hyp_len=156597, ref_len=169777)

2022-12-29 02:11:57,523 - 9:02:40 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:11:57,524 - 9:02:40 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:12:06,839 - 9:02:50 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/     5 rec:   45.481 mi: 1.79709196 zkl:   48.720 cd:    7.644 pos_prob:   13.370 prob_neg:    5.727 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/     5 rec:   45.481 mi: 1.79709196 zkl:   48.720 cd:    7.644 pos_prob:   13.370 prob_neg:    5.727 kl_weight:    0.396 do_ae_train: False
2022-12-29 02:12:16,096 - 9:02:59 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/     5 rec:   45.076 mi: 1.62561417 zkl:   44.943 cd:    6.869 pos_prob:   14.758 prob_neg:    7.889 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/     5 rec:   45.076 mi: 1.62561417 zkl:   44.943 cd:    6.869 pos_prob:   14.758 prob_neg:    7.889 kl_weight:    0.459 do_ae_train: False
2022-12-29 02:12:25,407 - 9:03:08 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/     5 rec:   50.729 mi: 1.77270484 zkl:   39.154 cd:    7.339 pos_prob:   16.036 prob_neg:    8.697 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/     5 rec:   50.729 mi: 1.77270484 zkl:   39.154 cd:    7.339 pos_prob:   16.036 prob_neg:    8.697 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:12:34,843 - 9:03:18 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/     5 rec:   57.429 mi: 1.96411145 zkl:   42.074 cd:    7.418 pos_prob:   19.557 prob_neg:   12.139 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/     5 rec:   57.429 mi: 1.96411145 zkl:   42.074 cd:    7.418 pos_prob:   19.557 prob_neg:   12.139 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:12:44,441 - 9:03:27 - 9.6s - INFO - root - batch/max_batch/ep:   500/   529/     5 rec:   67.508 mi: 1.50889134 zkl:   36.305 cd:    5.896 pos_prob:   20.125 prob_neg:   14.230 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/     5 rec:   67.508 mi: 1.50889134 zkl:   36.305 cd:    5.896 pos_prob:   20.125 prob_neg:   14.230 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:12:44,453 - 9:03:27 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-151.697
Langevin prior   1/ 40: energy=-151.697
2022-12-29 02:12:44,459 - 9:03:27 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-239.496
Langevin prior   6/ 40: energy=-239.496
2022-12-29 02:12:44,464 - 9:03:27 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-319.187
Langevin prior  11/ 40: energy=-319.187
2022-12-29 02:12:44,469 - 9:03:27 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-357.825
Langevin prior  16/ 40: energy=-357.825
2022-12-29 02:12:44,476 - 9:03:27 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-377.086
Langevin prior  21/ 40: energy=-377.086
2022-12-29 02:12:44,483 - 9:03:27 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-418.222
Langevin prior  26/ 40: energy=-418.222
2022-12-29 02:12:44,491 - 9:03:27 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-453.832
Langevin prior  31/ 40: energy=-453.832
2022-12-29 02:12:44,501 - 9:03:27 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-404.166
Langevin prior  36/ 40: energy=-404.166
2022-12-29 02:12:44,508 - 9:03:27 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-402.011
Langevin prior  40/ 40: energy=-402.011
2022-12-29 02:22:29,429 - 9:13:12 - 584.9s - INFO - root - Negative Log-likehood -134.216733
200 done. -122.18088921348539
400 done. -117.80790008294372
600 done. -117.38889615483579
800 done. -117.9614271002612
1000 done. -117.03756224439657
1200 done. -117.74148826994063
1400 done. -118.6200670748741
1600 done. -117.31213365485483
1800 done. -116.98890541766977
2000 done. -128.81264680667564
2200 done. -136.23588474861603
2400 done. -144.2354870928871
2600 done. -151.5189627016281
2800 done. -156.36145701266253
3000 done. -162.82853724562986
3200 done. -166.57506099301486
3400 done. -171.45021241743
3600 done. -174.55451114825973
3800 done. -178.28340259463334
4000 done. -180.87847004513569
4200 done. -173.85769314474376
4400 done. -166.69031585858394
4600 done. -160.35986121916466
4800 done. -154.4799027963667
5000 done. -149.24033930234015
5200 done. -144.29162020365834
5400 done. -139.80107150493433
5600 done. -135.61558468336432
Negative Log-likehood -134.216733
2022-12-29 02:22:29,429 - 9:13:12 - 0.0s - INFO - root - log-likelihood:   -134.217
log-likelihood:   -134.217
2022-12-29 02:23:17,169 - 9:14:00 - 47.7s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-005-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 02:23:17,171 - 9:14:00 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-005-test-greedy.txt
Generation: 188 batches
2022-12-29 02:23:22,918 - 9:14:06 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:23:24,401 - 9:14:07 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:23:24,401 - 9:14:07 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:23:26,809 - 9:14:10 - 2.4s - INFO - root - --- bleu: BLEU = 28.96, 53.6/31.5/25.1/22.1 (BP=0.931, ratio=0.933, hyp_len=158436, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 28.96, 53.6/31.5/25.1/22.1 (BP=0.931, ratio=0.933, hyp_len=158436, ref_len=169777)
--- bleu: BLEU = 28.96, 53.6/31.5/25.1/22.1 (BP=0.931, ratio=0.933, hyp_len=158436, ref_len=169777)

2022-12-29 02:23:26,810 - 9:14:10 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:23:26,811 - 9:14:10 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:23:36,186 - 9:14:19 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/     6 rec:   46.200 mi: 1.97203636 zkl:   44.038 cd:    9.966 pos_prob:   25.339 prob_neg:   15.374 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/     6 rec:   46.200 mi: 1.97203636 zkl:   44.038 cd:    9.966 pos_prob:   25.339 prob_neg:   15.374 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:23:45,777 - 9:14:28 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/     6 rec:   46.360 mi: 1.97238004 zkl:   40.501 cd:    6.403 pos_prob:   26.014 prob_neg:   19.611 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/     6 rec:   46.360 mi: 1.97238004 zkl:   40.501 cd:    6.403 pos_prob:   26.014 prob_neg:   19.611 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:23:55,222 - 9:14:38 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/     6 rec:   40.680 mi: 1.76834595 zkl:   44.012 cd:    6.648 pos_prob:   28.378 prob_neg:   21.730 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/     6 rec:   40.680 mi: 1.76834595 zkl:   44.012 cd:    6.648 pos_prob:   28.378 prob_neg:   21.730 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:24:04,575 - 9:14:47 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/     6 rec:   49.774 mi: 1.88491440 zkl:   40.718 cd:    7.987 pos_prob:   31.064 prob_neg:   23.077 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/     6 rec:   49.774 mi: 1.88491440 zkl:   40.718 cd:    7.987 pos_prob:   31.064 prob_neg:   23.077 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:24:13,845 - 9:14:57 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/     6 rec:   55.314 mi: 1.90248275 zkl:   45.820 cd:    7.798 pos_prob:   33.683 prob_neg:   25.885 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/     6 rec:   55.314 mi: 1.90248275 zkl:   45.820 cd:    7.798 pos_prob:   33.683 prob_neg:   25.885 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:24:13,858 - 9:14:57 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-154.047
Langevin prior   1/ 40: energy=-154.047
2022-12-29 02:24:13,864 - 9:14:57 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-372.804
Langevin prior   6/ 40: energy=-372.804
2022-12-29 02:24:13,869 - 9:14:57 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-509.996
Langevin prior  11/ 40: energy=-509.996
2022-12-29 02:24:13,874 - 9:14:57 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-628.161
Langevin prior  16/ 40: energy=-628.161
2022-12-29 02:24:13,881 - 9:14:57 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-681.364
Langevin prior  21/ 40: energy=-681.364
2022-12-29 02:24:13,891 - 9:14:57 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-727.546
Langevin prior  26/ 40: energy=-727.546
2022-12-29 02:24:13,902 - 9:14:57 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-758.966
Langevin prior  31/ 40: energy=-758.966
2022-12-29 02:24:13,912 - 9:14:57 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-753.172
Langevin prior  36/ 40: energy=-753.172
2022-12-29 02:24:13,921 - 9:14:57 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-752.071
Langevin prior  40/ 40: energy=-752.071
2022-12-29 02:33:59,141 - 9:24:42 - 585.2s - INFO - root - Negative Log-likehood -136.262814
200 done. -121.56189101230791
400 done. -117.41116204969921
600 done. -117.23349737240505
800 done. -117.79670746456529
1000 done. -116.79346387405896
1200 done. -117.51596698879088
1400 done. -118.38858487340012
1600 done. -117.07771650859857
1800 done. -116.69334542675738
2000 done. -128.88451475543556
2200 done. -136.56787297287417
2400 done. -144.80727848689384
2600 done. -152.33744605896538
2800 done. -157.36627846119245
3000 done. -164.05721899221248
3200 done. -167.98212810792108
3400 done. -172.99162755566317
3600 done. -176.18969420725466
3800 done. -180.08754165841435
4000 done. -182.84860901142312
4200 done. -175.75004348811058
4400 done. -168.56918333220676
4600 done. -162.24942699872744
4800 done. -156.40550961652593
5000 done. -151.2100685602021
5200 done. -146.2856112423728
5400 done. -141.82125514578732
5600 done. -137.66007966723794
Negative Log-likehood -136.262814
2022-12-29 02:33:59,141 - 9:24:42 - 0.0s - INFO - root - log-likelihood:   -136.263
log-likelihood:   -136.263
2022-12-29 02:34:46,137 - 9:25:29 - 47.0s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-006-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 02:34:46,139 - 9:25:29 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-006-test-greedy.txt
Generation: 188 batches
2022-12-29 02:34:51,855 - 9:25:35 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:34:53,385 - 9:25:36 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:34:53,385 - 9:25:36 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:34:55,811 - 9:25:39 - 2.4s - INFO - root - --- bleu: BLEU = 27.91, 51.6/29.9/23.4/20.3 (BP=0.953, ratio=0.954, hyp_len=161999, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 27.91, 51.6/29.9/23.4/20.3 (BP=0.953, ratio=0.954, hyp_len=161999, ref_len=169777)
--- bleu: BLEU = 27.91, 51.6/29.9/23.4/20.3 (BP=0.953, ratio=0.954, hyp_len=161999, ref_len=169777)

2022-12-29 02:34:55,812 - 9:25:39 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:34:55,813 - 9:25:39 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:35:05,004 - 9:25:48 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/     7 rec:   27.961 mi: 1.88527620 zkl:  126.314 cd:    1.257 pos_prob:   27.746 prob_neg:   26.489 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     7 rec:   27.961 mi: 1.88527620 zkl:  126.314 cd:    1.257 pos_prob:   27.746 prob_neg:   26.489 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:35:14,454 - 9:25:57 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/     7 rec:   39.601 mi: 1.95459521 zkl:  133.990 cd:   -5.350 pos_prob:   25.423 prob_neg:   30.773 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     7 rec:   39.601 mi: 1.95459521 zkl:  133.990 cd:   -5.350 pos_prob:   25.423 prob_neg:   30.773 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:35:23,637 - 9:26:06 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/     7 rec:   43.099 mi: 1.90127468 zkl:  138.524 cd:   -2.952 pos_prob:   24.014 prob_neg:   26.967 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     7 rec:   43.099 mi: 1.90127468 zkl:  138.524 cd:   -2.952 pos_prob:   24.014 prob_neg:   26.967 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:35:32,742 - 9:26:15 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/     7 rec:   31.769 mi: 1.88783658 zkl:  142.345 cd:   -1.525 pos_prob:   25.006 prob_neg:   26.531 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     7 rec:   31.769 mi: 1.88783658 zkl:  142.345 cd:   -1.525 pos_prob:   25.006 prob_neg:   26.531 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:35:41,983 - 9:26:25 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/     7 rec:   64.028 mi: 1.61080754 zkl:  145.619 cd:   -6.554 pos_prob:   21.164 prob_neg:   27.718 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     7 rec:   64.028 mi: 1.61080754 zkl:  145.619 cd:   -6.554 pos_prob:   21.164 prob_neg:   27.718 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:35:41,996 - 9:26:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-175.474
Langevin prior   1/ 40: energy=-175.474
2022-12-29 02:35:42,002 - 9:26:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-411.897
Langevin prior   6/ 40: energy=-411.897
2022-12-29 02:35:42,007 - 9:26:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-585.148
Langevin prior  11/ 40: energy=-585.148
2022-12-29 02:35:42,013 - 9:26:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-691.336
Langevin prior  16/ 40: energy=-691.336
2022-12-29 02:35:42,021 - 9:26:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-775.461
Langevin prior  21/ 40: energy=-775.461
2022-12-29 02:35:42,031 - 9:26:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-769.258
Langevin prior  26/ 40: energy=-769.258
2022-12-29 02:35:42,042 - 9:26:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-791.057
Langevin prior  31/ 40: energy=-791.057
2022-12-29 02:35:42,052 - 9:26:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-810.441
Langevin prior  36/ 40: energy=-810.441
2022-12-29 02:35:42,061 - 9:26:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-820.202
Langevin prior  40/ 40: energy=-820.202
2022-12-29 02:35:44,811 - 9:26:28 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-007-test-greedy.txt
Generation: 188 batches
2022-12-29 02:35:50,544 - 9:26:33 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:35:52,020 - 9:26:35 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:35:52,020 - 9:26:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:35:54,455 - 9:26:37 - 2.4s - INFO - root - --- bleu: BLEU = 30.60, 56.1/33.7/27.1/23.9 (BP=0.920, ratio=0.923, hyp_len=156706, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.60, 56.1/33.7/27.1/23.9 (BP=0.920, ratio=0.923, hyp_len=156706, ref_len=169777)
--- bleu: BLEU = 30.60, 56.1/33.7/27.1/23.9 (BP=0.920, ratio=0.923, hyp_len=156706, ref_len=169777)

2022-12-29 02:35:54,455 - 9:26:37 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:35:54,456 - 9:26:37 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:36:03,535 - 9:26:46 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/     8 rec:   38.180 mi: 1.92994535 zkl:  152.086 cd:   -5.141 pos_prob:   22.615 prob_neg:   27.756 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     8 rec:   38.180 mi: 1.92994535 zkl:  152.086 cd:   -5.141 pos_prob:   22.615 prob_neg:   27.756 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:36:12,814 - 9:26:56 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/     8 rec:   35.079 mi: 1.91849756 zkl:  159.924 cd:   -7.400 pos_prob:   22.059 prob_neg:   29.459 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     8 rec:   35.079 mi: 1.91849756 zkl:  159.924 cd:   -7.400 pos_prob:   22.059 prob_neg:   29.459 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:36:22,032 - 9:27:05 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/     8 rec:   34.590 mi: 1.90370846 zkl:  155.048 cd:   -4.355 pos_prob:   23.362 prob_neg:   27.718 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     8 rec:   34.590 mi: 1.90370846 zkl:  155.048 cd:   -4.355 pos_prob:   23.362 prob_neg:   27.718 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:36:31,146 - 9:27:14 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/     8 rec:   35.294 mi: 1.81295514 zkl:  160.563 cd:   -5.886 pos_prob:   22.453 prob_neg:   28.339 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     8 rec:   35.294 mi: 1.81295514 zkl:  160.563 cd:   -5.886 pos_prob:   22.453 prob_neg:   28.339 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:36:40,341 - 9:27:23 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/     8 rec:   31.054 mi: 2.05943227 zkl:  156.920 cd:   -5.874 pos_prob:   21.530 prob_neg:   27.403 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     8 rec:   31.054 mi: 2.05943227 zkl:  156.920 cd:   -5.874 pos_prob:   21.530 prob_neg:   27.403 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:36:40,354 - 9:27:23 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-193.739
Langevin prior   1/ 40: energy=-193.739
2022-12-29 02:36:40,360 - 9:27:23 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-398.159
Langevin prior   6/ 40: energy=-398.159
2022-12-29 02:36:40,365 - 9:27:23 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-595.276
Langevin prior  11/ 40: energy=-595.276
2022-12-29 02:36:40,371 - 9:27:23 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-669.601
Langevin prior  16/ 40: energy=-669.601
2022-12-29 02:36:40,379 - 9:27:23 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-777.104
Langevin prior  21/ 40: energy=-777.104
2022-12-29 02:36:40,388 - 9:27:23 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-826.807
Langevin prior  26/ 40: energy=-826.807
2022-12-29 02:36:40,396 - 9:27:23 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-804.460
Langevin prior  31/ 40: energy=-804.460
2022-12-29 02:36:40,405 - 9:27:23 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-815.287
Langevin prior  36/ 40: energy=-815.287
2022-12-29 02:36:40,413 - 9:27:23 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-860.039
Langevin prior  40/ 40: energy=-860.039
2022-12-29 02:36:42,979 - 9:27:26 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-008-test-greedy.txt
Generation: 188 batches
2022-12-29 02:36:48,672 - 9:27:31 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:36:50,160 - 9:27:33 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:36:50,160 - 9:27:33 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:36:52,562 - 9:27:35 - 2.4s - INFO - root - --- bleu: BLEU = 31.17, 55.8/33.7/27.2/24.1 (BP=0.935, ratio=0.937, hyp_len=159114, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.17, 55.8/33.7/27.2/24.1 (BP=0.935, ratio=0.937, hyp_len=159114, ref_len=169777)
--- bleu: BLEU = 31.17, 55.8/33.7/27.2/24.1 (BP=0.935, ratio=0.937, hyp_len=159114, ref_len=169777)

2022-12-29 02:36:52,563 - 9:27:35 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:36:52,564 - 9:27:35 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:37:01,793 - 9:27:44 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/     9 rec:   24.226 mi: 1.92571747 zkl:  167.226 cd:   -5.272 pos_prob:   22.702 prob_neg:   27.973 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/     9 rec:   24.226 mi: 1.92571747 zkl:  167.226 cd:   -5.272 pos_prob:   22.702 prob_neg:   27.973 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:37:11,034 - 9:27:54 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/     9 rec:   34.517 mi: 1.78737962 zkl:  164.647 cd:   -6.251 pos_prob:   20.499 prob_neg:   26.751 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/     9 rec:   34.517 mi: 1.78737962 zkl:  164.647 cd:   -6.251 pos_prob:   20.499 prob_neg:   26.751 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:37:20,360 - 9:28:03 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/     9 rec:   31.650 mi: 1.80095029 zkl:  166.240 cd:   -6.131 pos_prob:   21.564 prob_neg:   27.695 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/     9 rec:   31.650 mi: 1.80095029 zkl:  166.240 cd:   -6.131 pos_prob:   21.564 prob_neg:   27.695 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:37:29,509 - 9:28:12 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/     9 rec:   32.456 mi: 1.91627717 zkl:  167.446 cd:   -6.968 pos_prob:   22.557 prob_neg:   29.524 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/     9 rec:   32.456 mi: 1.91627717 zkl:  167.446 cd:   -6.968 pos_prob:   22.557 prob_neg:   29.524 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:37:38,500 - 9:28:21 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/     9 rec:   35.023 mi: 1.82456970 zkl:  170.301 cd:   -6.786 pos_prob:   22.427 prob_neg:   29.213 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/     9 rec:   35.023 mi: 1.82456970 zkl:  170.301 cd:   -6.786 pos_prob:   22.427 prob_neg:   29.213 kl_weight:    0.000 do_ae_train: True
2022-12-29 02:37:38,512 - 9:28:21 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-189.577
Langevin prior   1/ 40: energy=-189.577
2022-12-29 02:37:38,518 - 9:28:21 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-420.764
Langevin prior   6/ 40: energy=-420.764
2022-12-29 02:37:38,523 - 9:28:21 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-585.271
Langevin prior  11/ 40: energy=-585.271
2022-12-29 02:37:38,529 - 9:28:21 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-727.945
Langevin prior  16/ 40: energy=-727.945
2022-12-29 02:37:38,534 - 9:28:21 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-814.965
Langevin prior  21/ 40: energy=-814.965
2022-12-29 02:37:38,540 - 9:28:21 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-825.068
Langevin prior  26/ 40: energy=-825.068
2022-12-29 02:37:38,548 - 9:28:21 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-829.046
Langevin prior  31/ 40: energy=-829.046
2022-12-29 02:37:38,556 - 9:28:21 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-827.454
Langevin prior  36/ 40: energy=-827.454
2022-12-29 02:37:38,564 - 9:28:21 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-855.552
Langevin prior  40/ 40: energy=-855.552
2022-12-29 02:37:41,155 - 9:28:24 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-009-test-greedy.txt
Generation: 188 batches
2022-12-29 02:37:46,871 - 9:28:30 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:37:48,335 - 9:28:31 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:37:48,336 - 9:28:31 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:37:50,728 - 9:28:33 - 2.4s - INFO - root - --- bleu: BLEU = 31.15, 57.0/34.5/28.0/24.8 (BP=0.912, ratio=0.915, hyp_len=155394, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.15, 57.0/34.5/28.0/24.8 (BP=0.912, ratio=0.915, hyp_len=155394, ref_len=169777)
--- bleu: BLEU = 31.15, 57.0/34.5/28.0/24.8 (BP=0.912, ratio=0.915, hyp_len=155394, ref_len=169777)

2022-12-29 02:37:50,728 - 9:28:33 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:37:50,730 - 9:28:33 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:37:59,923 - 9:28:43 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    10 rec:   28.288 mi: 1.99337471 zkl:  118.084 cd:    2.948 pos_prob:   32.014 prob_neg:   29.065 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    10 rec:   28.288 mi: 1.99337471 zkl:  118.084 cd:    2.948 pos_prob:   32.014 prob_neg:   29.065 kl_weight:    0.062 do_ae_train: False
2022-12-29 02:38:09,284 - 9:28:52 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    10 rec:   23.491 mi: 1.79897547 zkl:   87.441 cd:    8.185 pos_prob:   39.611 prob_neg:   31.427 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    10 rec:   23.491 mi: 1.79897547 zkl:   87.441 cd:    8.185 pos_prob:   39.611 prob_neg:   31.427 kl_weight:    0.125 do_ae_train: False
2022-12-29 02:38:18,623 - 9:29:01 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    10 rec:   31.876 mi: 1.97100925 zkl:   76.855 cd:    1.916 pos_prob:   37.076 prob_neg:   35.160 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    10 rec:   31.876 mi: 1.97100925 zkl:   76.855 cd:    1.916 pos_prob:   37.076 prob_neg:   35.160 kl_weight:    0.188 do_ae_train: False
2022-12-29 02:38:28,178 - 9:29:11 - 9.6s - INFO - root - batch/max_batch/ep:   400/   529/    10 rec:   29.337 mi: 1.82217789 zkl:   68.482 cd:    4.742 pos_prob:   40.367 prob_neg:   35.625 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    10 rec:   29.337 mi: 1.82217789 zkl:   68.482 cd:    4.742 pos_prob:   40.367 prob_neg:   35.625 kl_weight:    0.251 do_ae_train: False
2022-12-29 02:38:37,678 - 9:29:20 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    10 rec:   32.886 mi: 2.04884624 zkl:   61.075 cd:    1.926 pos_prob:   42.017 prob_neg:   40.091 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    10 rec:   32.886 mi: 2.04884624 zkl:   61.075 cd:    1.926 pos_prob:   42.017 prob_neg:   40.091 kl_weight:    0.314 do_ae_train: False
2022-12-29 02:38:37,690 - 9:29:20 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-203.876
Langevin prior   1/ 40: energy=-203.876
2022-12-29 02:38:37,696 - 9:29:20 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-495.696
Langevin prior   6/ 40: energy=-495.696
2022-12-29 02:38:37,701 - 9:29:20 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-847.322
Langevin prior  11/ 40: energy=-847.322
2022-12-29 02:38:37,707 - 9:29:20 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1031.573
Langevin prior  16/ 40: energy=-1031.573
2022-12-29 02:38:37,713 - 9:29:20 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1164.247
Langevin prior  21/ 40: energy=-1164.247
2022-12-29 02:38:37,720 - 9:29:20 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1201.008
Langevin prior  26/ 40: energy=-1201.008
2022-12-29 02:38:37,728 - 9:29:20 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1189.410
Langevin prior  31/ 40: energy=-1189.410
2022-12-29 02:38:37,737 - 9:29:20 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1226.367
Langevin prior  36/ 40: energy=-1226.367
2022-12-29 02:38:37,744 - 9:29:20 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1270.740
Langevin prior  40/ 40: energy=-1270.740
2022-12-29 02:38:40,444 - 9:29:23 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-010-test-greedy.txt
Generation: 188 batches
2022-12-29 02:38:46,171 - 9:29:29 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:38:47,681 - 9:29:30 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:38:47,682 - 9:29:30 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:38:50,117 - 9:29:33 - 2.4s - INFO - root - --- bleu: BLEU = 29.88, 52.6/31.4/25.2/22.2 (BP=0.963, ratio=0.964, hyp_len=163648, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.88, 52.6/31.4/25.2/22.2 (BP=0.963, ratio=0.964, hyp_len=163648, ref_len=169777)
--- bleu: BLEU = 29.88, 52.6/31.4/25.2/22.2 (BP=0.963, ratio=0.964, hyp_len=163648, ref_len=169777)

2022-12-29 02:38:50,117 - 9:29:33 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:38:50,119 - 9:29:33 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:38:59,579 - 9:29:42 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    11 rec:   20.583 mi: 1.59560251 zkl:   56.603 cd:    6.138 pos_prob:   49.107 prob_neg:   42.969 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    11 rec:   20.583 mi: 1.59560251 zkl:   56.603 cd:    6.138 pos_prob:   49.107 prob_neg:   42.969 kl_weight:    0.396 do_ae_train: False
2022-12-29 02:39:09,074 - 9:29:52 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    11 rec:   25.407 mi: 2.00755119 zkl:   56.468 cd:    2.237 pos_prob:   46.748 prob_neg:   44.511 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    11 rec:   25.407 mi: 2.00755119 zkl:   56.468 cd:    2.237 pos_prob:   46.748 prob_neg:   44.511 kl_weight:    0.459 do_ae_train: False
2022-12-29 02:39:18,347 - 9:30:01 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    11 rec:   39.999 mi: 1.97954631 zkl:   49.820 cd:    3.011 pos_prob:   49.574 prob_neg:   46.563 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    11 rec:   39.999 mi: 1.97954631 zkl:   49.820 cd:    3.011 pos_prob:   49.574 prob_neg:   46.563 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:39:27,694 - 9:30:10 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    11 rec:   26.734 mi: 1.99885702 zkl:   52.505 cd:    3.112 pos_prob:   51.560 prob_neg:   48.448 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    11 rec:   26.734 mi: 1.99885702 zkl:   52.505 cd:    3.112 pos_prob:   51.560 prob_neg:   48.448 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:39:37,164 - 9:30:20 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    11 rec:   27.073 mi: 2.10172915 zkl:   51.828 cd:   -2.755 pos_prob:   52.128 prob_neg:   54.882 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    11 rec:   27.073 mi: 2.10172915 zkl:   51.828 cd:   -2.755 pos_prob:   52.128 prob_neg:   54.882 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:39:37,177 - 9:30:20 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-242.275
Langevin prior   1/ 40: energy=-242.275
2022-12-29 02:39:37,182 - 9:30:20 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-738.432
Langevin prior   6/ 40: energy=-738.432
2022-12-29 02:39:37,187 - 9:30:20 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1114.960
Langevin prior  11/ 40: energy=-1114.960
2022-12-29 02:39:37,193 - 9:30:20 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1343.466
Langevin prior  16/ 40: energy=-1343.466
2022-12-29 02:39:37,198 - 9:30:20 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1431.306
Langevin prior  21/ 40: energy=-1431.306
2022-12-29 02:39:37,203 - 9:30:20 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1513.949
Langevin prior  26/ 40: energy=-1513.949
2022-12-29 02:39:37,210 - 9:30:20 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1507.004
Langevin prior  31/ 40: energy=-1507.004
2022-12-29 02:39:37,218 - 9:30:20 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1519.490
Langevin prior  36/ 40: energy=-1519.490
2022-12-29 02:39:37,225 - 9:30:20 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1553.761
Langevin prior  40/ 40: energy=-1553.761
2022-12-29 02:49:21,947 - 9:40:05 - 584.7s - INFO - root - Negative Log-likehood -143.762617
200 done. -124.43242000693427
400 done. -119.96068463572395
600 done. -119.50222877802129
800 done. -120.10208990575565
1000 done. -119.11596359116261
1200 done. -119.85767440650162
1400 done. -120.67680925509177
1600 done. -119.33354973569462
1800 done. -118.9086316698875
2000 done. -132.71592914834213
2200 done. -141.77826503081764
2400 done. -151.08083814794216
2600 done. -159.52230642773944
2800 done. -165.19188755220028
3000 done. -172.97338935681253
3200 done. -177.41404507560927
3400 done. -183.03358066017546
3600 done. -186.76714010175925
3800 done. -191.00540080047713
4000 done. -194.1361224765242
4200 done. -186.57638977552202
4400 done. -178.84649286919014
4600 done. -172.01154734872046
4800 done. -165.66361655651536
5000 done. -159.99240034616437
5200 done. -154.65308645301627
5400 done. -149.7714213517425
5600 done. -145.2666223173251
Negative Log-likehood -143.762617
2022-12-29 02:49:21,947 - 9:40:05 - 0.0s - INFO - root - log-likelihood:   -143.763
log-likelihood:   -143.763
2022-12-29 02:50:08,561 - 9:40:51 - 46.6s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-011-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 02:50:08,565 - 9:40:51 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-011-test-greedy.txt
Generation: 188 batches
2022-12-29 02:50:14,299 - 9:40:57 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:50:15,831 - 9:40:59 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 02:50:15,831 - 9:40:59 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 02:50:18,559 - 9:41:01 - 2.7s - INFO - root - --- bleu: BLEU = 29.82, 53.9/32.0/25.8/22.8 (BP=0.938, ratio=0.940, hyp_len=159621, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.82, 53.9/32.0/25.8/22.8 (BP=0.938, ratio=0.940, hyp_len=159621, ref_len=169777)
--- bleu: BLEU = 29.82, 53.9/32.0/25.8/22.8 (BP=0.938, ratio=0.940, hyp_len=159621, ref_len=169777)

2022-12-29 02:50:18,559 - 9:41:01 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 02:50:18,560 - 9:41:01 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 02:50:28,115 - 9:41:11 - 9.6s - INFO - root - batch/max_batch/ep:   100/   529/    12 rec:   22.216 mi: 1.93841481 zkl:   55.715 cd:    5.603 pos_prob:   57.888 prob_neg:   52.285 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    12 rec:   22.216 mi: 1.93841481 zkl:   55.715 cd:    5.603 pos_prob:   57.888 prob_neg:   52.285 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:50:37,440 - 9:41:20 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    12 rec:   30.540 mi: 2.08773899 zkl:   60.120 cd:   -0.707 pos_prob:   56.247 prob_neg:   56.954 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    12 rec:   30.540 mi: 2.08773899 zkl:   60.120 cd:   -0.707 pos_prob:   56.247 prob_neg:   56.954 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:50:46,814 - 9:41:30 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    12 rec:   28.673 mi: 1.98094094 zkl:   56.926 cd:    3.308 pos_prob:   60.689 prob_neg:   57.381 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    12 rec:   28.673 mi: 1.98094094 zkl:   56.926 cd:    3.308 pos_prob:   60.689 prob_neg:   57.381 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:50:56,215 - 9:41:39 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    12 rec:   30.781 mi: 1.83967948 zkl:   55.602 cd:   -0.878 pos_prob:   61.651 prob_neg:   62.529 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    12 rec:   30.781 mi: 1.83967948 zkl:   55.602 cd:   -0.878 pos_prob:   61.651 prob_neg:   62.529 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:51:05,517 - 9:41:48 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    12 rec:   29.929 mi: 1.73761940 zkl:   61.949 cd:    9.429 pos_prob:   74.375 prob_neg:   64.946 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    12 rec:   29.929 mi: 1.73761940 zkl:   61.949 cd:    9.429 pos_prob:   74.375 prob_neg:   64.946 kl_weight:    0.500 do_ae_train: False
2022-12-29 02:51:05,530 - 9:41:48 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-243.196
Langevin prior   1/ 40: energy=-243.196
2022-12-29 02:51:05,536 - 9:41:48 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-961.922
Langevin prior   6/ 40: energy=-961.922
2022-12-29 02:51:05,541 - 9:41:48 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1502.146
Langevin prior  11/ 40: energy=-1502.146
2022-12-29 02:51:05,546 - 9:41:48 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1804.820
Langevin prior  16/ 40: energy=-1804.820
2022-12-29 02:51:05,551 - 9:41:48 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1940.377
Langevin prior  21/ 40: energy=-1940.377
2022-12-29 02:51:05,557 - 9:41:48 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2035.258
Langevin prior  26/ 40: energy=-2035.258
2022-12-29 02:51:05,563 - 9:41:48 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2089.803
Langevin prior  31/ 40: energy=-2089.803
2022-12-29 02:51:05,572 - 9:41:48 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2098.404
Langevin prior  36/ 40: energy=-2098.404
2022-12-29 02:51:05,578 - 9:41:48 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2156.123
Langevin prior  40/ 40: energy=-2156.123
2022-12-29 03:00:50,289 - 9:51:33 - 584.7s - INFO - root - Negative Log-likehood -144.941861
200 done. -125.94815582061972
400 done. -121.64360380600273
600 done. -121.18575299452765
800 done. -121.82648690160059
1000 done. -120.85480727848129
1200 done. -121.56281949031138
1400 done. -122.47856789077181
1600 done. -121.03971009895889
1800 done. -120.61980957572395
2000 done. -134.7615381263059
2200 done. -143.81124510916484
2400 done. -153.0968488153011
2600 done. -161.56282656948522
2800 done. -167.24555263750474
3000 done. -175.0121567040858
3200 done. -179.41920316925066
3400 done. -185.05342548584412
3600 done. -188.80951957287354
3800 done. -193.08613001541934
4000 done. -196.33003738617242
4200 done. -188.66136079764075
4400 done. -180.78430393794838
4600 done. -173.8024897536871
4800 done. -167.328740467705
5000 done. -161.5278564392256
5200 done. -156.07219539607632
5400 done. -151.10607272377788
5600 done. -146.48917292637844
Negative Log-likehood -144.941861
2022-12-29 03:00:50,289 - 9:51:33 - 0.0s - INFO - root - log-likelihood:   -144.942
log-likelihood:   -144.942
2022-12-29 03:01:37,365 - 9:52:20 - 47.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-012-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 03:01:37,368 - 9:52:20 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-012-test-greedy.txt
Generation: 188 batches
2022-12-29 03:01:43,097 - 9:52:26 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:01:44,615 - 9:52:27 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:01:44,616 - 9:52:27 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:01:47,113 - 9:52:30 - 2.5s - INFO - root - --- bleu: BLEU = 29.69, 52.3/31.2/25.2/22.3 (BP=0.960, ratio=0.960, hyp_len=163069, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.69, 52.3/31.2/25.2/22.3 (BP=0.960, ratio=0.960, hyp_len=163069, ref_len=169777)
--- bleu: BLEU = 29.69, 52.3/31.2/25.2/22.3 (BP=0.960, ratio=0.960, hyp_len=163069, ref_len=169777)

2022-12-29 03:01:47,114 - 9:52:30 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:01:47,115 - 9:52:30 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:01:56,158 - 9:52:39 - 9.0s - INFO - root - batch/max_batch/ep:   100/   529/    13 rec:   21.178 mi: 1.75932264 zkl:  136.785 cd:  -14.476 pos_prob:   57.513 prob_neg:   71.989 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    13 rec:   21.178 mi: 1.75932264 zkl:  136.785 cd:  -14.476 pos_prob:   57.513 prob_neg:   71.989 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:02:05,516 - 9:52:48 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    13 rec:   33.220 mi: 1.82895958 zkl:  143.251 cd:  -24.445 pos_prob:   45.217 prob_neg:   69.662 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    13 rec:   33.220 mi: 1.82895958 zkl:  143.251 cd:  -24.445 pos_prob:   45.217 prob_neg:   69.662 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:02:14,642 - 9:52:57 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    13 rec:   24.292 mi: 2.10944605 zkl:  145.654 cd:  -25.995 pos_prob:   44.406 prob_neg:   70.401 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    13 rec:   24.292 mi: 2.10944605 zkl:  145.654 cd:  -25.995 pos_prob:   44.406 prob_neg:   70.401 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:02:23,952 - 9:53:07 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    13 rec:   28.684 mi: 1.74432576 zkl:  143.945 cd:  -24.753 pos_prob:   40.295 prob_neg:   65.048 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    13 rec:   28.684 mi: 1.74432576 zkl:  143.945 cd:  -24.753 pos_prob:   40.295 prob_neg:   65.048 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:02:33,214 - 9:53:16 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    13 rec:   24.540 mi: 1.76104367 zkl:  147.211 cd:  -33.588 pos_prob:   36.058 prob_neg:   69.646 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    13 rec:   24.540 mi: 1.76104367 zkl:  147.211 cd:  -33.588 pos_prob:   36.058 prob_neg:   69.646 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:02:33,226 - 9:53:16 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-278.856
Langevin prior   1/ 40: energy=-278.856
2022-12-29 03:02:33,232 - 9:53:16 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1024.188
Langevin prior   6/ 40: energy=-1024.188
2022-12-29 03:02:33,237 - 9:53:16 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1430.814
Langevin prior  11/ 40: energy=-1430.814
2022-12-29 03:02:33,242 - 9:53:16 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1641.227
Langevin prior  16/ 40: energy=-1641.227
2022-12-29 03:02:33,247 - 9:53:16 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1823.047
Langevin prior  21/ 40: energy=-1823.047
2022-12-29 03:02:33,254 - 9:53:16 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1929.194
Langevin prior  26/ 40: energy=-1929.194
2022-12-29 03:02:33,262 - 9:53:16 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2045.866
Langevin prior  31/ 40: energy=-2045.866
2022-12-29 03:02:33,270 - 9:53:16 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2067.966
Langevin prior  36/ 40: energy=-2067.966
2022-12-29 03:02:33,278 - 9:53:16 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2126.707
Langevin prior  40/ 40: energy=-2126.707
2022-12-29 03:02:35,928 - 9:53:19 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-013-test-greedy.txt
Generation: 188 batches
2022-12-29 03:02:41,638 - 9:53:24 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:02:43,146 - 9:53:26 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:02:43,146 - 9:53:26 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:02:45,569 - 9:53:28 - 2.4s - INFO - root - --- bleu: BLEU = 30.95, 54.8/32.8/26.5/23.5 (BP=0.951, ratio=0.953, hyp_len=161732, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.95, 54.8/32.8/26.5/23.5 (BP=0.951, ratio=0.953, hyp_len=161732, ref_len=169777)
--- bleu: BLEU = 30.95, 54.8/32.8/26.5/23.5 (BP=0.951, ratio=0.953, hyp_len=161732, ref_len=169777)

2022-12-29 03:02:45,570 - 9:53:28 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:02:45,571 - 9:53:28 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:02:54,696 - 9:53:37 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    14 rec:   17.906 mi: 2.02935290 zkl:  155.666 cd:  -28.598 pos_prob:   39.057 prob_neg:   67.655 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    14 rec:   17.906 mi: 2.02935290 zkl:  155.666 cd:  -28.598 pos_prob:   39.057 prob_neg:   67.655 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:03:03,852 - 9:53:47 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    14 rec:   14.724 mi: 2.00766492 zkl:  154.971 cd:  -26.265 pos_prob:   37.792 prob_neg:   64.057 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    14 rec:   14.724 mi: 2.00766492 zkl:  154.971 cd:  -26.265 pos_prob:   37.792 prob_neg:   64.057 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:03:13,088 - 9:53:56 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    14 rec:   24.348 mi: 1.85943651 zkl:  157.396 cd:  -33.736 pos_prob:   38.260 prob_neg:   71.995 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    14 rec:   24.348 mi: 1.85943651 zkl:  157.396 cd:  -33.736 pos_prob:   38.260 prob_neg:   71.995 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:03:22,103 - 9:54:05 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    14 rec:   19.307 mi: 1.90312397 zkl:  156.545 cd:  -31.857 pos_prob:   36.960 prob_neg:   68.818 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    14 rec:   19.307 mi: 1.90312397 zkl:  156.545 cd:  -31.857 pos_prob:   36.960 prob_neg:   68.818 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:03:31,310 - 9:54:14 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    14 rec:   17.708 mi: 1.87185550 zkl:  157.485 cd:  -34.909 pos_prob:   35.725 prob_neg:   70.634 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    14 rec:   17.708 mi: 1.87185550 zkl:  157.485 cd:  -34.909 pos_prob:   35.725 prob_neg:   70.634 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:03:31,323 - 9:54:14 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-226.075
Langevin prior   1/ 40: energy=-226.075
2022-12-29 03:03:31,329 - 9:54:14 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1010.083
Langevin prior   6/ 40: energy=-1010.083
2022-12-29 03:03:31,334 - 9:54:14 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1528.280
Langevin prior  11/ 40: energy=-1528.280
2022-12-29 03:03:31,339 - 9:54:14 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1795.852
Langevin prior  16/ 40: energy=-1795.852
2022-12-29 03:03:31,344 - 9:54:14 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1914.221
Langevin prior  21/ 40: energy=-1914.221
2022-12-29 03:03:31,350 - 9:54:14 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1918.411
Langevin prior  26/ 40: energy=-1918.411
2022-12-29 03:03:31,360 - 9:54:14 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2060.468
Langevin prior  31/ 40: energy=-2060.468
2022-12-29 03:03:31,371 - 9:54:14 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2077.535
Langevin prior  36/ 40: energy=-2077.535
2022-12-29 03:03:31,379 - 9:54:14 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2055.648
Langevin prior  40/ 40: energy=-2055.648
2022-12-29 03:03:33,961 - 9:54:17 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-014-test-greedy.txt
Generation: 188 batches
2022-12-29 03:03:39,659 - 9:54:22 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:03:41,135 - 9:54:24 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:03:41,135 - 9:54:24 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:03:43,524 - 9:54:26 - 2.4s - INFO - root - --- bleu: BLEU = 31.30, 56.4/34.2/28.0/24.8 (BP=0.920, ratio=0.923, hyp_len=156738, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.30, 56.4/34.2/28.0/24.8 (BP=0.920, ratio=0.923, hyp_len=156738, ref_len=169777)
--- bleu: BLEU = 31.30, 56.4/34.2/28.0/24.8 (BP=0.920, ratio=0.923, hyp_len=156738, ref_len=169777)

2022-12-29 03:03:43,525 - 9:54:26 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:03:43,526 - 9:54:26 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:03:52,787 - 9:54:35 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    15 rec:   12.586 mi: 1.88852644 zkl:  158.411 cd:  -27.796 pos_prob:   37.823 prob_neg:   65.618 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    15 rec:   12.586 mi: 1.88852644 zkl:  158.411 cd:  -27.796 pos_prob:   37.823 prob_neg:   65.618 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:04:01,956 - 9:54:45 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    15 rec:   16.300 mi: 2.07840037 zkl:  162.545 cd:  -38.385 pos_prob:   36.256 prob_neg:   74.642 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    15 rec:   16.300 mi: 2.07840037 zkl:  162.545 cd:  -38.385 pos_prob:   36.256 prob_neg:   74.642 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:04:11,227 - 9:54:54 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    15 rec:   13.992 mi: 1.95175731 zkl:  159.826 cd:  -37.334 pos_prob:   33.895 prob_neg:   71.229 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    15 rec:   13.992 mi: 1.95175731 zkl:  159.826 cd:  -37.334 pos_prob:   33.895 prob_neg:   71.229 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:04:20,531 - 9:55:03 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    15 rec:   23.279 mi: 1.94899523 zkl:  164.692 cd:  -45.314 pos_prob:   28.836 prob_neg:   74.150 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    15 rec:   23.279 mi: 1.94899523 zkl:  164.692 cd:  -45.314 pos_prob:   28.836 prob_neg:   74.150 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:04:29,770 - 9:55:12 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    15 rec:   13.632 mi: 1.85395455 zkl:  169.672 cd:  -40.292 pos_prob:   34.311 prob_neg:   74.603 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    15 rec:   13.632 mi: 1.85395455 zkl:  169.672 cd:  -40.292 pos_prob:   34.311 prob_neg:   74.603 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:04:29,782 - 9:55:12 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-277.692
Langevin prior   1/ 40: energy=-277.692
2022-12-29 03:04:29,788 - 9:55:12 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-960.033
Langevin prior   6/ 40: energy=-960.033
2022-12-29 03:04:29,794 - 9:55:12 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1649.584
Langevin prior  11/ 40: energy=-1649.584
2022-12-29 03:04:29,799 - 9:55:12 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1922.743
Langevin prior  16/ 40: energy=-1922.743
2022-12-29 03:04:29,806 - 9:55:13 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1968.493
Langevin prior  21/ 40: energy=-1968.493
2022-12-29 03:04:29,813 - 9:55:13 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2114.529
Langevin prior  26/ 40: energy=-2114.529
2022-12-29 03:04:29,821 - 9:55:13 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2191.368
Langevin prior  31/ 40: energy=-2191.368
2022-12-29 03:04:29,831 - 9:55:13 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2193.102
Langevin prior  36/ 40: energy=-2193.102
2022-12-29 03:04:29,839 - 9:55:13 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2186.340
Langevin prior  40/ 40: energy=-2186.340
2022-12-29 03:04:32,498 - 9:55:15 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-015-test-greedy.txt
Generation: 188 batches
2022-12-29 03:04:38,201 - 9:55:21 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:04:39,714 - 9:55:22 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:04:39,714 - 9:55:22 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:04:42,129 - 9:55:25 - 2.4s - INFO - root - --- bleu: BLEU = 31.66, 55.5/33.9/27.6/24.5 (BP=0.943, ratio=0.945, hyp_len=160405, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.66, 55.5/33.9/27.6/24.5 (BP=0.943, ratio=0.945, hyp_len=160405, ref_len=169777)
--- bleu: BLEU = 31.66, 55.5/33.9/27.6/24.5 (BP=0.943, ratio=0.945, hyp_len=160405, ref_len=169777)

2022-12-29 03:04:42,129 - 9:55:25 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:04:42,131 - 9:55:25 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:04:51,436 - 9:55:34 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    16 rec:   17.328 mi: 2.04978991 zkl:  109.129 cd:  -15.683 pos_prob:   48.657 prob_neg:   64.340 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    16 rec:   17.328 mi: 2.04978991 zkl:  109.129 cd:  -15.683 pos_prob:   48.657 prob_neg:   64.340 kl_weight:    0.062 do_ae_train: False
2022-12-29 03:05:00,676 - 9:55:43 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    16 rec:   21.381 mi: 1.80353212 zkl:   95.596 cd:   -6.080 pos_prob:   54.394 prob_neg:   60.474 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    16 rec:   21.381 mi: 1.80353212 zkl:   95.596 cd:   -6.080 pos_prob:   54.394 prob_neg:   60.474 kl_weight:    0.125 do_ae_train: False
2022-12-29 03:05:09,998 - 9:55:53 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    16 rec:   19.946 mi: 1.78309143 zkl:   82.496 cd:   -0.322 pos_prob:   61.399 prob_neg:   61.720 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    16 rec:   19.946 mi: 1.78309143 zkl:   82.496 cd:   -0.322 pos_prob:   61.399 prob_neg:   61.720 kl_weight:    0.188 do_ae_train: False
2022-12-29 03:05:19,378 - 9:56:02 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    16 rec:   18.862 mi: 2.01384258 zkl:   75.029 cd:   -7.381 pos_prob:   60.111 prob_neg:   67.492 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    16 rec:   18.862 mi: 2.01384258 zkl:   75.029 cd:   -7.381 pos_prob:   60.111 prob_neg:   67.492 kl_weight:    0.251 do_ae_train: False
2022-12-29 03:05:28,753 - 9:56:11 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    16 rec:   27.364 mi: 2.12585258 zkl:   71.848 cd:   -2.362 pos_prob:   62.185 prob_neg:   64.547 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    16 rec:   27.364 mi: 2.12585258 zkl:   71.848 cd:   -2.362 pos_prob:   62.185 prob_neg:   64.547 kl_weight:    0.314 do_ae_train: False
2022-12-29 03:05:28,766 - 9:56:11 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-249.698
Langevin prior   1/ 40: energy=-249.698
2022-12-29 03:05:28,772 - 9:56:11 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-853.563
Langevin prior   6/ 40: energy=-853.563
2022-12-29 03:05:28,777 - 9:56:11 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1339.293
Langevin prior  11/ 40: energy=-1339.293
2022-12-29 03:05:28,782 - 9:56:11 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1632.554
Langevin prior  16/ 40: energy=-1632.554
2022-12-29 03:05:28,788 - 9:56:11 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1763.176
Langevin prior  21/ 40: energy=-1763.176
2022-12-29 03:05:28,794 - 9:56:11 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1817.937
Langevin prior  26/ 40: energy=-1817.937
2022-12-29 03:05:28,800 - 9:56:11 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1874.980
Langevin prior  31/ 40: energy=-1874.980
2022-12-29 03:05:28,808 - 9:56:12 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1980.314
Langevin prior  36/ 40: energy=-1980.314
2022-12-29 03:05:28,815 - 9:56:12 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1887.600
Langevin prior  40/ 40: energy=-1887.600
2022-12-29 03:05:31,624 - 9:56:14 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-016-test-greedy.txt
Generation: 188 batches
2022-12-29 03:05:37,341 - 9:56:20 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:05:38,855 - 9:56:22 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:05:38,855 - 9:56:22 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:05:41,302 - 9:56:24 - 2.4s - INFO - root - --- bleu: BLEU = 30.16, 52.7/31.6/25.6/22.7 (BP=0.961, ratio=0.962, hyp_len=163345, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.16, 52.7/31.6/25.6/22.7 (BP=0.961, ratio=0.962, hyp_len=163345, ref_len=169777)
--- bleu: BLEU = 30.16, 52.7/31.6/25.6/22.7 (BP=0.961, ratio=0.962, hyp_len=163345, ref_len=169777)

2022-12-29 03:05:41,303 - 9:56:24 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:05:41,304 - 9:56:24 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:05:50,775 - 9:56:33 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    17 rec:   21.290 mi: 1.99934793 zkl:   72.625 cd:    1.789 pos_prob:   68.591 prob_neg:   66.802 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    17 rec:   21.290 mi: 1.99934793 zkl:   72.625 cd:    1.789 pos_prob:   68.591 prob_neg:   66.802 kl_weight:    0.396 do_ae_train: False
2022-12-29 03:06:00,257 - 9:56:43 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    17 rec:   26.649 mi: 2.05625296 zkl:   56.884 cd:   -1.658 pos_prob:   64.570 prob_neg:   66.228 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    17 rec:   26.649 mi: 2.05625296 zkl:   56.884 cd:   -1.658 pos_prob:   64.570 prob_neg:   66.228 kl_weight:    0.459 do_ae_train: False
2022-12-29 03:06:09,655 - 9:56:52 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    17 rec:   22.193 mi: 2.06319928 zkl:   63.086 cd:    2.396 pos_prob:   72.619 prob_neg:   70.223 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    17 rec:   22.193 mi: 2.06319928 zkl:   63.086 cd:    2.396 pos_prob:   72.619 prob_neg:   70.223 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:06:19,039 - 9:57:02 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    17 rec:   19.130 mi: 1.97090375 zkl:   61.863 cd:   -1.365 pos_prob:   68.797 prob_neg:   70.162 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    17 rec:   19.130 mi: 1.97090375 zkl:   61.863 cd:   -1.365 pos_prob:   68.797 prob_neg:   70.162 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:06:28,783 - 9:57:11 - 9.7s - INFO - root - batch/max_batch/ep:   500/   529/    17 rec:   22.903 mi: 2.04910684 zkl:   61.743 cd:   -0.483 pos_prob:   67.013 prob_neg:   67.495 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    17 rec:   22.903 mi: 2.04910684 zkl:   61.743 cd:   -0.483 pos_prob:   67.013 prob_neg:   67.495 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:06:28,796 - 9:57:11 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-283.463
Langevin prior   1/ 40: energy=-283.463
2022-12-29 03:06:28,802 - 9:57:11 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1054.591
Langevin prior   6/ 40: energy=-1054.591
2022-12-29 03:06:28,807 - 9:57:12 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1643.765
Langevin prior  11/ 40: energy=-1643.765
2022-12-29 03:06:28,812 - 9:57:12 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1895.228
Langevin prior  16/ 40: energy=-1895.228
2022-12-29 03:06:28,818 - 9:57:12 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1938.242
Langevin prior  21/ 40: energy=-1938.242
2022-12-29 03:06:28,823 - 9:57:12 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1960.580
Langevin prior  26/ 40: energy=-1960.580
2022-12-29 03:06:28,830 - 9:57:12 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2009.181
Langevin prior  31/ 40: energy=-2009.181
2022-12-29 03:06:28,838 - 9:57:12 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2063.184
Langevin prior  36/ 40: energy=-2063.184
2022-12-29 03:06:28,845 - 9:57:12 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2023.550
Langevin prior  40/ 40: energy=-2023.550
2022-12-29 03:16:13,758 - 10:06:56 - 584.9s - INFO - root - Negative Log-likehood -151.694851
200 done. -132.9484234904565
400 done. -127.8874107762851
600 done. -127.60193319898966
800 done. -128.06136189153895
1000 done. -127.10612843491555
1200 done. -127.80902974898167
1400 done. -128.83488738212287
1600 done. -127.36971994975097
1800 done. -126.92488147350865
2000 done. -141.55925661316076
2200 done. -151.04760627485277
2400 done. -161.04202744624106
2600 done. -169.9439020065383
2800 done. -175.72544065683357
3000 done. -183.95810066204945
3200 done. -188.69336855246445
3400 done. -194.60515421619772
3600 done. -198.45179709110363
3800 done. -202.8442204533463
4000 done. -206.33233799675787
4200 done. -198.17749640175185
4400 done. -189.81427130382116
4600 done. -182.37834319301405
4800 done. -175.4914098759464
5000 done. -169.33168077423664
5200 done. -163.53395751038187
5400 done. -158.25629983648912
5600 done. -153.340015775803
Negative Log-likehood -151.694851
2022-12-29 03:16:13,759 - 10:06:56 - 0.0s - INFO - root - log-likelihood:   -151.695
log-likelihood:   -151.695
2022-12-29 03:17:00,889 - 10:07:44 - 47.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-017-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 03:17:00,910 - 10:07:44 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-017-test-greedy.txt
Generation: 188 batches
2022-12-29 03:17:06,631 - 10:07:49 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:17:08,149 - 10:07:51 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:17:08,149 - 10:07:51 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:17:10,587 - 10:07:53 - 2.4s - INFO - root - --- bleu: BLEU = 29.68, 52.4/31.3/25.3/22.4 (BP=0.956, ratio=0.957, hyp_len=162417, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.68, 52.4/31.3/25.3/22.4 (BP=0.956, ratio=0.957, hyp_len=162417, ref_len=169777)
--- bleu: BLEU = 29.68, 52.4/31.3/25.3/22.4 (BP=0.956, ratio=0.957, hyp_len=162417, ref_len=169777)

2022-12-29 03:17:10,588 - 10:07:53 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:17:10,589 - 10:07:53 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:17:19,930 - 10:08:03 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    18 rec:   21.071 mi: 2.05387425 zkl:   67.844 cd:   -4.873 pos_prob:   75.143 prob_neg:   80.017 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    18 rec:   21.071 mi: 2.05387425 zkl:   67.844 cd:   -4.873 pos_prob:   75.143 prob_neg:   80.017 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:17:29,333 - 10:08:12 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    18 rec:   24.740 mi: 2.00219822 zkl:   65.407 cd:   -1.802 pos_prob:   73.433 prob_neg:   75.235 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    18 rec:   24.740 mi: 2.00219822 zkl:   65.407 cd:   -1.802 pos_prob:   73.433 prob_neg:   75.235 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:17:38,826 - 10:08:22 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    18 rec:   29.902 mi: 1.88396525 zkl:   64.796 cd:   -4.194 pos_prob:   75.308 prob_neg:   79.502 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    18 rec:   29.902 mi: 1.88396525 zkl:   64.796 cd:   -4.194 pos_prob:   75.308 prob_neg:   79.502 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:17:48,146 - 10:08:31 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    18 rec:   32.178 mi: 2.05900216 zkl:   65.785 cd:    1.822 pos_prob:   76.535 prob_neg:   74.712 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    18 rec:   32.178 mi: 2.05900216 zkl:   65.785 cd:    1.822 pos_prob:   76.535 prob_neg:   74.712 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:17:57,571 - 10:08:40 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    18 rec:   17.579 mi: 2.05150509 zkl:   59.542 cd:   -7.834 pos_prob:   69.797 prob_neg:   77.632 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    18 rec:   17.579 mi: 2.05150509 zkl:   59.542 cd:   -7.834 pos_prob:   69.797 prob_neg:   77.632 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:17:57,584 - 10:08:40 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-252.769
Langevin prior   1/ 40: energy=-252.769
2022-12-29 03:17:57,590 - 10:08:40 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1169.618
Langevin prior   6/ 40: energy=-1169.618
2022-12-29 03:17:57,596 - 10:08:40 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1757.098
Langevin prior  11/ 40: energy=-1757.098
2022-12-29 03:17:57,604 - 10:08:40 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1964.395
Langevin prior  16/ 40: energy=-1964.395
2022-12-29 03:17:57,613 - 10:08:40 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2135.682
Langevin prior  21/ 40: energy=-2135.682
2022-12-29 03:17:57,622 - 10:08:40 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2121.436
Langevin prior  26/ 40: energy=-2121.436
2022-12-29 03:17:57,633 - 10:08:40 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2146.041
Langevin prior  31/ 40: energy=-2146.041
2022-12-29 03:17:57,643 - 10:08:40 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2223.965
Langevin prior  36/ 40: energy=-2223.965
2022-12-29 03:17:57,653 - 10:08:40 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2190.260
Langevin prior  40/ 40: energy=-2190.260
2022-12-29 03:27:43,258 - 10:18:26 - 585.6s - INFO - root - Negative Log-likehood -152.358640
200 done. -132.99858974514635
400 done. -127.79884375777078
600 done. -127.5037669929819
800 done. -128.08771670767922
1000 done. -127.0336319111371
1200 done. -127.7670948915053
1400 done. -128.74818193349378
1600 done. -127.31505643173615
1800 done. -126.90998691841591
2000 done. -141.6584698153621
2200 done. -151.27915582678528
2400 done. -161.39520036527796
2600 done. -170.3567137847901
2800 done. -176.25284313029425
3000 done. -184.50107778862485
3200 done. -189.2024063852014
3400 done. -195.26557746497346
3600 done. -199.26094175388755
3800 done. -203.7964912366957
4000 done. -207.18041488877697
4200 done. -199.01084888505994
4400 done. -190.61947856220047
4600 done. -183.16639472787807
4800 done. -176.24527508135012
5000 done. -170.07341303208662
5200 done. -164.2421302307843
5400 done. -158.94247417704875
5600 done. -154.00584778698715
Negative Log-likehood -152.358640
2022-12-29 03:27:43,258 - 10:18:26 - 0.0s - INFO - root - log-likelihood:   -152.359
log-likelihood:   -152.359
2022-12-29 03:28:30,825 - 10:19:14 - 47.6s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-018-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 03:28:30,846 - 10:19:14 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-018-test-greedy.txt
Generation: 188 batches
2022-12-29 03:28:36,557 - 10:19:19 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:28:38,065 - 10:19:21 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:28:38,065 - 10:19:21 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:28:40,771 - 10:19:23 - 2.7s - INFO - root - --- bleu: BLEU = 29.48, 52.9/31.7/25.8/22.8 (BP=0.936, ratio=0.938, hyp_len=159279, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.48, 52.9/31.7/25.8/22.8 (BP=0.936, ratio=0.938, hyp_len=159279, ref_len=169777)
--- bleu: BLEU = 29.48, 52.9/31.7/25.8/22.8 (BP=0.936, ratio=0.938, hyp_len=159279, ref_len=169777)

2022-12-29 03:28:40,772 - 10:19:23 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:28:40,773 - 10:19:23 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:28:50,044 - 10:19:33 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    19 rec:   18.286 mi: 1.92522407 zkl:  146.109 cd:   -8.438 pos_prob:   64.587 prob_neg:   73.025 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    19 rec:   18.286 mi: 1.92522407 zkl:  146.109 cd:   -8.438 pos_prob:   64.587 prob_neg:   73.025 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:28:59,224 - 10:19:42 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    19 rec:   15.304 mi: 2.08614111 zkl:  147.588 cd:  -23.074 pos_prob:   59.104 prob_neg:   82.178 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    19 rec:   15.304 mi: 2.08614111 zkl:  147.588 cd:  -23.074 pos_prob:   59.104 prob_neg:   82.178 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:29:08,407 - 10:19:51 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    19 rec:   16.600 mi: 2.02848697 zkl:  150.743 cd:  -22.327 pos_prob:   54.930 prob_neg:   77.258 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    19 rec:   16.600 mi: 2.02848697 zkl:  150.743 cd:  -22.327 pos_prob:   54.930 prob_neg:   77.258 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:29:17,584 - 10:20:00 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    19 rec:   18.032 mi: 2.04468894 zkl:  151.204 cd:  -32.079 pos_prob:   51.382 prob_neg:   83.461 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    19 rec:   18.032 mi: 2.04468894 zkl:  151.204 cd:  -32.079 pos_prob:   51.382 prob_neg:   83.461 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:29:26,835 - 10:20:10 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    19 rec:   18.102 mi: 1.97664845 zkl:  154.288 cd:  -30.282 pos_prob:   46.580 prob_neg:   76.862 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    19 rec:   18.102 mi: 1.97664845 zkl:  154.288 cd:  -30.282 pos_prob:   46.580 prob_neg:   76.862 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:29:26,847 - 10:20:10 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-252.705
Langevin prior   1/ 40: energy=-252.705
2022-12-29 03:29:26,853 - 10:20:10 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1072.084
Langevin prior   6/ 40: energy=-1072.084
2022-12-29 03:29:26,858 - 10:20:10 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1737.209
Langevin prior  11/ 40: energy=-1737.209
2022-12-29 03:29:26,864 - 10:20:10 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2030.555
Langevin prior  16/ 40: energy=-2030.555
2022-12-29 03:29:26,870 - 10:20:10 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2243.037
Langevin prior  21/ 40: energy=-2243.037
2022-12-29 03:29:26,878 - 10:20:10 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2281.525
Langevin prior  26/ 40: energy=-2281.525
2022-12-29 03:29:26,887 - 10:20:10 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2265.951
Langevin prior  31/ 40: energy=-2265.951
2022-12-29 03:29:26,896 - 10:20:10 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2331.015
Langevin prior  36/ 40: energy=-2331.015
2022-12-29 03:29:26,904 - 10:20:10 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2340.198
Langevin prior  40/ 40: energy=-2340.198
2022-12-29 03:29:29,459 - 10:20:12 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-019-test-greedy.txt
Generation: 188 batches
2022-12-29 03:29:35,160 - 10:20:18 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:29:36,669 - 10:20:19 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:29:36,669 - 10:20:19 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:29:39,096 - 10:20:22 - 2.4s - INFO - root - --- bleu: BLEU = 31.15, 54.7/33.1/26.9/23.8 (BP=0.950, ratio=0.951, hyp_len=161424, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.15, 54.7/33.1/26.9/23.8 (BP=0.950, ratio=0.951, hyp_len=161424, ref_len=169777)
--- bleu: BLEU = 31.15, 54.7/33.1/26.9/23.8 (BP=0.950, ratio=0.951, hyp_len=161424, ref_len=169777)

2022-12-29 03:29:39,097 - 10:20:22 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:29:39,098 - 10:20:22 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:29:48,422 - 10:20:31 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    20 rec:   14.286 mi: 2.06353831 zkl:  152.259 cd:  -33.067 pos_prob:   44.359 prob_neg:   77.426 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    20 rec:   14.286 mi: 2.06353831 zkl:  152.259 cd:  -33.067 pos_prob:   44.359 prob_neg:   77.426 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:29:57,588 - 10:20:40 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    20 rec:   13.768 mi: 1.85758519 zkl:  158.242 cd:  -27.167 pos_prob:   46.983 prob_neg:   74.150 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    20 rec:   13.768 mi: 1.85758519 zkl:  158.242 cd:  -27.167 pos_prob:   46.983 prob_neg:   74.150 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:30:06,675 - 10:20:49 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    20 rec:   13.745 mi: 2.08324504 zkl:  157.312 cd:  -34.614 pos_prob:   45.444 prob_neg:   80.058 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    20 rec:   13.745 mi: 2.08324504 zkl:  157.312 cd:  -34.614 pos_prob:   45.444 prob_neg:   80.058 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:30:15,768 - 10:20:58 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    20 rec:   18.240 mi: 1.99060953 zkl:  159.902 cd:  -34.956 pos_prob:   43.346 prob_neg:   78.302 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    20 rec:   18.240 mi: 1.99060953 zkl:  159.902 cd:  -34.956 pos_prob:   43.346 prob_neg:   78.302 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:30:24,912 - 10:21:08 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    20 rec:   17.225 mi: 1.92413259 zkl:  163.399 cd:  -29.798 pos_prob:   41.795 prob_neg:   71.593 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    20 rec:   17.225 mi: 1.92413259 zkl:  163.399 cd:  -29.798 pos_prob:   41.795 prob_neg:   71.593 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:30:24,924 - 10:21:08 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-244.163
Langevin prior   1/ 40: energy=-244.163
2022-12-29 03:30:24,929 - 10:21:08 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1160.799
Langevin prior   6/ 40: energy=-1160.799
2022-12-29 03:30:24,934 - 10:21:08 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1907.248
Langevin prior  11/ 40: energy=-1907.248
2022-12-29 03:30:24,939 - 10:21:08 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2254.378
Langevin prior  16/ 40: energy=-2254.378
2022-12-29 03:30:24,944 - 10:21:08 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2356.698
Langevin prior  21/ 40: energy=-2356.698
2022-12-29 03:30:24,950 - 10:21:08 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2393.925
Langevin prior  26/ 40: energy=-2393.925
2022-12-29 03:30:24,955 - 10:21:08 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2470.257
Langevin prior  31/ 40: energy=-2470.257
2022-12-29 03:30:24,963 - 10:21:08 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2478.475
Langevin prior  36/ 40: energy=-2478.475
2022-12-29 03:30:24,969 - 10:21:08 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2461.527
Langevin prior  40/ 40: energy=-2461.527
2022-12-29 03:30:27,528 - 10:21:10 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-020-test-greedy.txt
Generation: 188 batches
2022-12-29 03:30:33,211 - 10:21:16 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:30:34,714 - 10:21:17 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:30:34,714 - 10:21:17 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:30:37,191 - 10:21:20 - 2.5s - INFO - root - --- bleu: BLEU = 31.50, 54.6/33.1/27.1/24.1 (BP=0.955, ratio=0.956, hyp_len=162275, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.50, 54.6/33.1/27.1/24.1 (BP=0.955, ratio=0.956, hyp_len=162275, ref_len=169777)
--- bleu: BLEU = 31.50, 54.6/33.1/27.1/24.1 (BP=0.955, ratio=0.956, hyp_len=162275, ref_len=169777)

2022-12-29 03:30:37,192 - 10:21:20 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:30:37,193 - 10:21:20 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:30:46,481 - 10:21:29 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    21 rec:   17.737 mi: 2.03355384 zkl:  159.599 cd:  -34.886 pos_prob:   39.638 prob_neg:   74.524 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    21 rec:   17.737 mi: 2.03355384 zkl:  159.599 cd:  -34.886 pos_prob:   39.638 prob_neg:   74.524 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:30:55,627 - 10:21:38 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    21 rec:   11.120 mi: 1.96620548 zkl:  161.283 cd:  -38.887 pos_prob:   42.283 prob_neg:   81.171 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    21 rec:   11.120 mi: 1.96620548 zkl:  161.283 cd:  -38.887 pos_prob:   42.283 prob_neg:   81.171 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:31:04,832 - 10:21:48 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    21 rec:   13.756 mi: 1.86176765 zkl:  165.423 cd:  -35.830 pos_prob:   39.612 prob_neg:   75.442 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    21 rec:   13.756 mi: 1.86176765 zkl:  165.423 cd:  -35.830 pos_prob:   39.612 prob_neg:   75.442 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:31:14,078 - 10:21:57 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    21 rec:   14.720 mi: 1.91299868 zkl:  167.745 cd:  -34.321 pos_prob:   39.363 prob_neg:   73.684 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    21 rec:   14.720 mi: 1.91299868 zkl:  167.745 cd:  -34.321 pos_prob:   39.363 prob_neg:   73.684 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:31:23,177 - 10:22:06 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    21 rec:   16.189 mi: 1.95765185 zkl:  163.190 cd:  -41.993 pos_prob:   37.360 prob_neg:   79.353 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    21 rec:   16.189 mi: 1.95765185 zkl:  163.190 cd:  -41.993 pos_prob:   37.360 prob_neg:   79.353 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:31:23,189 - 10:22:06 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-239.174
Langevin prior   1/ 40: energy=-239.174
2022-12-29 03:31:23,194 - 10:22:06 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1138.138
Langevin prior   6/ 40: energy=-1138.138
2022-12-29 03:31:23,199 - 10:22:06 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1827.398
Langevin prior  11/ 40: energy=-1827.398
2022-12-29 03:31:23,204 - 10:22:06 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2083.998
Langevin prior  16/ 40: energy=-2083.998
2022-12-29 03:31:23,209 - 10:22:06 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2402.729
Langevin prior  21/ 40: energy=-2402.729
2022-12-29 03:31:23,215 - 10:22:06 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2483.158
Langevin prior  26/ 40: energy=-2483.158
2022-12-29 03:31:23,220 - 10:22:06 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2577.057
Langevin prior  31/ 40: energy=-2577.057
2022-12-29 03:31:23,227 - 10:22:06 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2551.766
Langevin prior  36/ 40: energy=-2551.766
2022-12-29 03:31:23,233 - 10:22:06 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2579.428
Langevin prior  40/ 40: energy=-2579.428
2022-12-29 03:31:25,731 - 10:22:08 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-021-test-greedy.txt
Generation: 188 batches
2022-12-29 03:31:31,421 - 10:22:14 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:31:32,930 - 10:22:16 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:31:32,930 - 10:22:16 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:31:35,368 - 10:22:18 - 2.4s - INFO - root - --- bleu: BLEU = 31.67, 55.3/33.6/27.4/24.2 (BP=0.951, ratio=0.952, hyp_len=161661, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.67, 55.3/33.6/27.4/24.2 (BP=0.951, ratio=0.952, hyp_len=161661, ref_len=169777)
--- bleu: BLEU = 31.67, 55.3/33.6/27.4/24.2 (BP=0.951, ratio=0.952, hyp_len=161661, ref_len=169777)

2022-12-29 03:31:35,368 - 10:22:18 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:31:35,370 - 10:22:18 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:31:44,657 - 10:22:27 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    22 rec:   15.495 mi: 1.98284674 zkl:  100.130 cd:  -10.826 pos_prob:   63.150 prob_neg:   73.975 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    22 rec:   15.495 mi: 1.98284674 zkl:  100.130 cd:  -10.826 pos_prob:   63.150 prob_neg:   73.975 kl_weight:    0.062 do_ae_train: False
2022-12-29 03:31:54,042 - 10:22:37 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    22 rec:   13.172 mi: 2.02416110 zkl:   85.953 cd:   -9.394 pos_prob:   61.088 prob_neg:   70.482 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    22 rec:   13.172 mi: 2.02416110 zkl:   85.953 cd:   -9.394 pos_prob:   61.088 prob_neg:   70.482 kl_weight:    0.125 do_ae_train: False
2022-12-29 03:32:03,442 - 10:22:46 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    22 rec:   17.078 mi: 2.03223300 zkl:   80.781 cd:   -7.184 pos_prob:   63.178 prob_neg:   70.362 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    22 rec:   17.078 mi: 2.03223300 zkl:   80.781 cd:   -7.184 pos_prob:   63.178 prob_neg:   70.362 kl_weight:    0.188 do_ae_train: False
2022-12-29 03:32:12,922 - 10:22:56 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    22 rec:   20.373 mi: 1.56919563 zkl:   77.567 cd:   -3.589 pos_prob:   68.082 prob_neg:   71.671 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    22 rec:   20.373 mi: 1.56919563 zkl:   77.567 cd:   -3.589 pos_prob:   68.082 prob_neg:   71.671 kl_weight:    0.251 do_ae_train: False
2022-12-29 03:32:22,168 - 10:23:05 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    22 rec:   19.189 mi: 2.02416015 zkl:   67.999 cd:   -4.196 pos_prob:   62.950 prob_neg:   67.146 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    22 rec:   19.189 mi: 2.02416015 zkl:   67.999 cd:   -4.196 pos_prob:   62.950 prob_neg:   67.146 kl_weight:    0.314 do_ae_train: False
2022-12-29 03:32:22,180 - 10:23:05 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-262.894
Langevin prior   1/ 40: energy=-262.894
2022-12-29 03:32:22,185 - 10:23:05 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1030.479
Langevin prior   6/ 40: energy=-1030.479
2022-12-29 03:32:22,191 - 10:23:05 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1501.412
Langevin prior  11/ 40: energy=-1501.412
2022-12-29 03:32:22,196 - 10:23:05 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1774.959
Langevin prior  16/ 40: energy=-1774.959
2022-12-29 03:32:22,202 - 10:23:05 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1875.448
Langevin prior  21/ 40: energy=-1875.448
2022-12-29 03:32:22,208 - 10:23:05 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1976.675
Langevin prior  26/ 40: energy=-1976.675
2022-12-29 03:32:22,216 - 10:23:05 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1910.302
Langevin prior  31/ 40: energy=-1910.302
2022-12-29 03:32:22,225 - 10:23:05 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2013.301
Langevin prior  36/ 40: energy=-2013.301
2022-12-29 03:32:22,232 - 10:23:05 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1934.103
Langevin prior  40/ 40: energy=-1934.103
2022-12-29 03:32:25,025 - 10:23:08 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-022-test-greedy.txt
Generation: 188 batches
2022-12-29 03:32:30,699 - 10:23:13 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:32:32,213 - 10:23:15 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:32:32,214 - 10:23:15 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:32:34,648 - 10:23:17 - 2.4s - INFO - root - --- bleu: BLEU = 30.72, 53.9/32.6/26.7/23.7 (BP=0.946, ratio=0.948, hyp_len=160874, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.72, 53.9/32.6/26.7/23.7 (BP=0.946, ratio=0.948, hyp_len=160874, ref_len=169777)
--- bleu: BLEU = 30.72, 53.9/32.6/26.7/23.7 (BP=0.946, ratio=0.948, hyp_len=160874, ref_len=169777)

2022-12-29 03:32:34,649 - 10:23:17 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:32:34,650 - 10:23:17 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:32:44,061 - 10:23:27 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    23 rec:   21.277 mi: 1.89848804 zkl:   66.860 cd:   -9.709 pos_prob:   61.654 prob_neg:   71.363 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    23 rec:   21.277 mi: 1.89848804 zkl:   66.860 cd:   -9.709 pos_prob:   61.654 prob_neg:   71.363 kl_weight:    0.396 do_ae_train: False
2022-12-29 03:32:53,479 - 10:23:36 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    23 rec:   18.985 mi: 2.08461189 zkl:   61.251 cd:   -2.541 pos_prob:   63.401 prob_neg:   65.941 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    23 rec:   18.985 mi: 2.08461189 zkl:   61.251 cd:   -2.541 pos_prob:   63.401 prob_neg:   65.941 kl_weight:    0.459 do_ae_train: False
2022-12-29 03:33:02,863 - 10:23:46 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    23 rec:   19.335 mi: 2.07098579 zkl:   59.267 cd:   -4.629 pos_prob:   62.504 prob_neg:   67.133 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    23 rec:   19.335 mi: 2.07098579 zkl:   59.267 cd:   -4.629 pos_prob:   62.504 prob_neg:   67.133 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:33:12,345 - 10:23:55 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    23 rec:   19.417 mi: 2.05600429 zkl:   64.010 cd:    3.436 pos_prob:   71.932 prob_neg:   68.496 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    23 rec:   19.417 mi: 2.05600429 zkl:   64.010 cd:    3.436 pos_prob:   71.932 prob_neg:   68.496 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:33:21,709 - 10:24:04 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    23 rec:   26.762 mi: 1.97794521 zkl:   64.382 cd:   -5.772 pos_prob:   69.069 prob_neg:   74.841 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    23 rec:   26.762 mi: 1.97794521 zkl:   64.382 cd:   -5.772 pos_prob:   69.069 prob_neg:   74.841 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:33:21,722 - 10:24:04 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-245.354
Langevin prior   1/ 40: energy=-245.354
2022-12-29 03:33:21,727 - 10:24:04 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1133.831
Langevin prior   6/ 40: energy=-1133.831
2022-12-29 03:33:21,732 - 10:24:04 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1730.549
Langevin prior  11/ 40: energy=-1730.549
2022-12-29 03:33:21,737 - 10:24:04 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1961.224
Langevin prior  16/ 40: energy=-1961.224
2022-12-29 03:33:21,742 - 10:24:04 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2026.892
Langevin prior  21/ 40: energy=-2026.892
2022-12-29 03:33:21,748 - 10:24:04 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2053.427
Langevin prior  26/ 40: energy=-2053.427
2022-12-29 03:33:21,754 - 10:24:04 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2110.694
Langevin prior  31/ 40: energy=-2110.694
2022-12-29 03:33:21,761 - 10:24:04 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2114.347
Langevin prior  36/ 40: energy=-2114.347
2022-12-29 03:33:21,768 - 10:24:04 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2090.951
Langevin prior  40/ 40: energy=-2090.951
2022-12-29 03:43:06,983 - 10:33:50 - 585.2s - INFO - root - Negative Log-likehood -158.559458
200 done. -137.75808139172668
400 done. -132.44369567067983
600 done. -132.19318321337076
800 done. -132.78306618419654
1000 done. -131.8030189709638
1200 done. -132.55566814421857
1400 done. -133.53671880123915
1600 done. -132.01125753512568
1800 done. -131.53293309716912
2000 done. -146.89404843362382
2200 done. -156.995419304328
2400 done. -167.54505108482755
2600 done. -177.00886252678353
2800 done. -183.19584606194974
3000 done. -191.91962862459715
3200 done. -196.8138847212123
3400 done. -203.22401906400123
3600 done. -207.33632650757616
3800 done. -212.0604450499414
4000 done. -215.76120332462816
4200 done. -207.23807382330216
4400 done. -198.49617516985776
4600 done. -190.72176359899038
4800 done. -183.50883548012905
5000 done. -177.04151949576922
5200 done. -170.95901402090365
5400 done. -165.41587235691546
5600 done. -160.27482481326155
Negative Log-likehood -158.559458
2022-12-29 03:43:06,984 - 10:33:50 - 0.0s - INFO - root - log-likelihood:   -158.559
log-likelihood:   -158.559
2022-12-29 03:43:54,121 - 10:34:37 - 47.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-023-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 03:43:54,142 - 10:34:37 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-023-test-greedy.txt
Generation: 188 batches
2022-12-29 03:43:59,832 - 10:34:43 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:44:01,349 - 10:34:44 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:44:01,349 - 10:34:44 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:44:03,756 - 10:34:46 - 2.4s - INFO - root - --- bleu: BLEU = 29.76, 53.2/32.0/26.3/23.3 (BP=0.931, ratio=0.933, hyp_len=158438, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.76, 53.2/32.0/26.3/23.3 (BP=0.931, ratio=0.933, hyp_len=158438, ref_len=169777)
--- bleu: BLEU = 29.76, 53.2/32.0/26.3/23.3 (BP=0.931, ratio=0.933, hyp_len=158438, ref_len=169777)

2022-12-29 03:44:03,756 - 10:34:46 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:44:03,758 - 10:34:46 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:44:13,158 - 10:34:56 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    24 rec:   21.502 mi: 2.03938794 zkl:   64.411 cd:   -1.964 pos_prob:   71.194 prob_neg:   73.158 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    24 rec:   21.502 mi: 2.03938794 zkl:   64.411 cd:   -1.964 pos_prob:   71.194 prob_neg:   73.158 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:44:22,627 - 10:35:05 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    24 rec:   17.550 mi: 1.85872400 zkl:   59.819 cd:  -13.188 pos_prob:   63.897 prob_neg:   77.085 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    24 rec:   17.550 mi: 1.85872400 zkl:   59.819 cd:  -13.188 pos_prob:   63.897 prob_neg:   77.085 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:44:32,007 - 10:35:15 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    24 rec:   15.878 mi: 2.07283306 zkl:   63.518 cd:   -1.769 pos_prob:   75.282 prob_neg:   77.051 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    24 rec:   15.878 mi: 2.07283306 zkl:   63.518 cd:   -1.769 pos_prob:   75.282 prob_neg:   77.051 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:44:41,538 - 10:35:24 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    24 rec:   24.952 mi: 2.09232950 zkl:   64.370 cd:    0.683 pos_prob:   75.786 prob_neg:   75.103 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    24 rec:   24.952 mi: 2.09232950 zkl:   64.370 cd:    0.683 pos_prob:   75.786 prob_neg:   75.103 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:44:50,968 - 10:35:34 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    24 rec:   19.099 mi: 1.91965508 zkl:   65.491 cd:   -0.831 pos_prob:   77.582 prob_neg:   78.413 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    24 rec:   19.099 mi: 1.91965508 zkl:   65.491 cd:   -0.831 pos_prob:   77.582 prob_neg:   78.413 kl_weight:    0.500 do_ae_train: False
2022-12-29 03:44:50,980 - 10:35:34 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-220.526
Langevin prior   1/ 40: energy=-220.526
2022-12-29 03:44:50,986 - 10:35:34 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1017.983
Langevin prior   6/ 40: energy=-1017.983
2022-12-29 03:44:50,991 - 10:35:34 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1627.493
Langevin prior  11/ 40: energy=-1627.493
2022-12-29 03:44:50,996 - 10:35:34 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1959.732
Langevin prior  16/ 40: energy=-1959.732
2022-12-29 03:44:51,001 - 10:35:34 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2107.882
Langevin prior  21/ 40: energy=-2107.882
2022-12-29 03:44:51,007 - 10:35:34 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2113.332
Langevin prior  26/ 40: energy=-2113.332
2022-12-29 03:44:51,015 - 10:35:34 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2177.184
Langevin prior  31/ 40: energy=-2177.184
2022-12-29 03:44:51,023 - 10:35:34 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2274.950
Langevin prior  36/ 40: energy=-2274.950
2022-12-29 03:44:51,030 - 10:35:34 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2254.984
Langevin prior  40/ 40: energy=-2254.984
2022-12-29 03:54:35,884 - 10:45:19 - 584.9s - INFO - root - Negative Log-likehood -158.749871
200 done. -137.82030753950056
400 done. -132.69890816112462
600 done. -132.36491344671072
800 done. -133.11042942607733
1000 done. -132.0317446732134
1200 done. -132.84303250551577
1400 done. -133.80158835859882
1600 done. -132.21349833445674
1800 done. -131.6401975706263
2000 done. -147.33977246696955
2200 done. -157.5873142186289
2400 done. -168.17321048198724
2600 done. -177.62695426119257
2800 done. -183.9687645725809
3000 done. -192.5227983506174
3200 done. -197.4483611094575
3400 done. -203.78859719183325
3600 done. -207.99027237644373
3800 done. -212.78609363921328
4000 done. -216.3804157974069
4200 done. -207.79312671555516
4400 done. -198.98564322284483
4600 done. -191.152774749849
4800 done. -183.8815624671103
5000 done. -177.37575741602302
5200 done. -171.25018141810776
5400 done. -165.66214088519254
5600 done. -160.47988556187005
Negative Log-likehood -158.749871
2022-12-29 03:54:35,884 - 10:45:19 - 0.0s - INFO - root - log-likelihood:   -158.750
log-likelihood:   -158.750
2022-12-29 03:55:22,750 - 10:46:05 - 46.9s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-024-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 03:55:22,771 - 10:46:05 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-024-test-greedy.txt
Generation: 188 batches
2022-12-29 03:55:28,486 - 10:46:11 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:55:30,016 - 10:46:13 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:55:30,016 - 10:46:13 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:55:32,479 - 10:46:15 - 2.5s - INFO - root - --- bleu: BLEU = 30.00, 52.2/31.4/25.6/22.7 (BP=0.960, ratio=0.961, hyp_len=163199, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.00, 52.2/31.4/25.6/22.7 (BP=0.960, ratio=0.961, hyp_len=163199, ref_len=169777)
--- bleu: BLEU = 30.00, 52.2/31.4/25.6/22.7 (BP=0.960, ratio=0.961, hyp_len=163199, ref_len=169777)

2022-12-29 03:55:32,479 - 10:46:15 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:55:32,481 - 10:46:15 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:55:41,723 - 10:46:24 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    25 rec:   11.964 mi: 2.03192163 zkl:  148.727 cd:   -8.824 pos_prob:   65.803 prob_neg:   74.627 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    25 rec:   11.964 mi: 2.03192163 zkl:  148.727 cd:   -8.824 pos_prob:   65.803 prob_neg:   74.627 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:55:50,919 - 10:46:34 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    25 rec:   20.448 mi: 1.96016955 zkl:  155.606 cd:  -14.514 pos_prob:   59.846 prob_neg:   74.361 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    25 rec:   20.448 mi: 1.96016955 zkl:  155.606 cd:  -14.514 pos_prob:   59.846 prob_neg:   74.361 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:56:00,143 - 10:46:43 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    25 rec:    8.107 mi: 1.98279488 zkl:  154.710 cd:  -13.277 pos_prob:   62.506 prob_neg:   75.783 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    25 rec:    8.107 mi: 1.98279488 zkl:  154.710 cd:  -13.277 pos_prob:   62.506 prob_neg:   75.783 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:56:09,408 - 10:46:52 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    25 rec:   14.386 mi: 2.02734637 zkl:  156.668 cd:  -25.764 pos_prob:   52.957 prob_neg:   78.721 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    25 rec:   14.386 mi: 2.02734637 zkl:  156.668 cd:  -25.764 pos_prob:   52.957 prob_neg:   78.721 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:56:18,526 - 10:47:01 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    25 rec:   15.893 mi: 1.98304677 zkl:  154.675 cd:  -29.955 pos_prob:   49.501 prob_neg:   79.456 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    25 rec:   15.893 mi: 1.98304677 zkl:  154.675 cd:  -29.955 pos_prob:   49.501 prob_neg:   79.456 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:56:18,539 - 10:47:01 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-260.674
Langevin prior   1/ 40: energy=-260.674
2022-12-29 03:56:18,544 - 10:47:01 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1126.915
Langevin prior   6/ 40: energy=-1126.915
2022-12-29 03:56:18,548 - 10:47:01 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1812.535
Langevin prior  11/ 40: energy=-1812.535
2022-12-29 03:56:18,553 - 10:47:01 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2178.364
Langevin prior  16/ 40: energy=-2178.364
2022-12-29 03:56:18,559 - 10:47:01 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2261.008
Langevin prior  21/ 40: energy=-2261.008
2022-12-29 03:56:18,564 - 10:47:01 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2344.987
Langevin prior  26/ 40: energy=-2344.987
2022-12-29 03:56:18,570 - 10:47:01 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2295.276
Langevin prior  31/ 40: energy=-2295.276
2022-12-29 03:56:18,576 - 10:47:01 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2377.214
Langevin prior  36/ 40: energy=-2377.214
2022-12-29 03:56:18,583 - 10:47:01 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2329.709
Langevin prior  40/ 40: energy=-2329.709
2022-12-29 03:56:21,114 - 10:47:04 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-025-test-greedy.txt
Generation: 188 batches
2022-12-29 03:56:26,823 - 10:47:10 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:56:28,601 - 10:47:11 - 1.8s - INFO - root - Generation Done
Generation Done
2022-12-29 03:56:28,602 - 10:47:11 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:56:31,068 - 10:47:14 - 2.5s - INFO - root - --- bleu: BLEU = 31.41, 53.7/32.8/26.8/23.7 (BP=0.966, ratio=0.966, hyp_len=164025, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.41, 53.7/32.8/26.8/23.7 (BP=0.966, ratio=0.966, hyp_len=164025, ref_len=169777)
--- bleu: BLEU = 31.41, 53.7/32.8/26.8/23.7 (BP=0.966, ratio=0.966, hyp_len=164025, ref_len=169777)

2022-12-29 03:56:31,068 - 10:47:14 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:56:31,070 - 10:47:14 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:56:40,198 - 10:47:23 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    26 rec:   11.677 mi: 2.01980543 zkl:  162.970 cd:  -20.096 pos_prob:   53.193 prob_neg:   73.289 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    26 rec:   11.677 mi: 2.01980543 zkl:  162.970 cd:  -20.096 pos_prob:   53.193 prob_neg:   73.289 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:56:49,385 - 10:47:32 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    26 rec:   14.577 mi: 2.03680110 zkl:  155.474 cd:  -35.382 pos_prob:   42.659 prob_neg:   78.041 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    26 rec:   14.577 mi: 2.03680110 zkl:  155.474 cd:  -35.382 pos_prob:   42.659 prob_neg:   78.041 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:56:58,533 - 10:47:41 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    26 rec:   14.425 mi: 1.99849689 zkl:  161.986 cd:  -30.027 pos_prob:   41.865 prob_neg:   71.892 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    26 rec:   14.425 mi: 1.99849689 zkl:  161.986 cd:  -30.027 pos_prob:   41.865 prob_neg:   71.892 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:57:07,708 - 10:47:50 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    26 rec:   10.843 mi: 1.88373625 zkl:  161.464 cd:  -31.087 pos_prob:   45.541 prob_neg:   76.628 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    26 rec:   10.843 mi: 1.88373625 zkl:  161.464 cd:  -31.087 pos_prob:   45.541 prob_neg:   76.628 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:57:16,940 - 10:48:00 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    26 rec:   13.128 mi: 1.93354344 zkl:  162.296 cd:  -29.214 pos_prob:   47.106 prob_neg:   76.320 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    26 rec:   13.128 mi: 1.93354344 zkl:  162.296 cd:  -29.214 pos_prob:   47.106 prob_neg:   76.320 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:57:16,953 - 10:48:00 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-251.510
Langevin prior   1/ 40: energy=-251.510
2022-12-29 03:57:16,958 - 10:48:00 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1175.753
Langevin prior   6/ 40: energy=-1175.753
2022-12-29 03:57:16,964 - 10:48:00 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1700.005
Langevin prior  11/ 40: energy=-1700.005
2022-12-29 03:57:16,969 - 10:48:00 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2003.279
Langevin prior  16/ 40: energy=-2003.279
2022-12-29 03:57:16,975 - 10:48:00 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2143.317
Langevin prior  21/ 40: energy=-2143.317
2022-12-29 03:57:16,982 - 10:48:00 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2220.615
Langevin prior  26/ 40: energy=-2220.615
2022-12-29 03:57:16,991 - 10:48:00 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2303.085
Langevin prior  31/ 40: energy=-2303.085
2022-12-29 03:57:16,999 - 10:48:00 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2388.526
Langevin prior  36/ 40: energy=-2388.526
2022-12-29 03:57:17,007 - 10:48:00 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2372.902
Langevin prior  40/ 40: energy=-2372.902
2022-12-29 03:57:19,614 - 10:48:02 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-026-test-greedy.txt
Generation: 188 batches
2022-12-29 03:57:25,314 - 10:48:08 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:57:26,812 - 10:48:10 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:57:26,813 - 10:48:10 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:57:29,237 - 10:48:12 - 2.4s - INFO - root - --- bleu: BLEU = 31.28, 54.8/33.3/27.4/24.3 (BP=0.943, ratio=0.944, hyp_len=160293, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.28, 54.8/33.3/27.4/24.3 (BP=0.943, ratio=0.944, hyp_len=160293, ref_len=169777)
--- bleu: BLEU = 31.28, 54.8/33.3/27.4/24.3 (BP=0.943, ratio=0.944, hyp_len=160293, ref_len=169777)

2022-12-29 03:57:29,237 - 10:48:12 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:57:29,239 - 10:48:12 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:57:38,485 - 10:48:21 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    27 rec:   10.470 mi: 1.99750733 zkl:  163.181 cd:  -34.452 pos_prob:   43.670 prob_neg:   78.122 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    27 rec:   10.470 mi: 1.99750733 zkl:  163.181 cd:  -34.452 pos_prob:   43.670 prob_neg:   78.122 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:57:47,622 - 10:48:30 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    27 rec:    9.475 mi: 2.03185964 zkl:  162.504 cd:  -33.257 pos_prob:   42.352 prob_neg:   75.609 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    27 rec:    9.475 mi: 2.03185964 zkl:  162.504 cd:  -33.257 pos_prob:   42.352 prob_neg:   75.609 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:57:57,031 - 10:48:40 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    27 rec:    9.453 mi: 1.92797959 zkl:  164.299 cd:  -31.735 pos_prob:   44.752 prob_neg:   76.487 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    27 rec:    9.453 mi: 1.92797959 zkl:  164.299 cd:  -31.735 pos_prob:   44.752 prob_neg:   76.487 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:58:06,158 - 10:48:49 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    27 rec:   14.784 mi: 1.82474518 zkl:  164.164 cd:  -37.953 pos_prob:   35.383 prob_neg:   73.336 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    27 rec:   14.784 mi: 1.82474518 zkl:  164.164 cd:  -37.953 pos_prob:   35.383 prob_neg:   73.336 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:58:15,355 - 10:48:58 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    27 rec:   12.129 mi: 1.93911493 zkl:  167.740 cd:  -35.661 pos_prob:   41.779 prob_neg:   77.440 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    27 rec:   12.129 mi: 1.93911493 zkl:  167.740 cd:  -35.661 pos_prob:   41.779 prob_neg:   77.440 kl_weight:    0.000 do_ae_train: True
2022-12-29 03:58:15,367 - 10:48:58 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-321.797
Langevin prior   1/ 40: energy=-321.797
2022-12-29 03:58:15,373 - 10:48:58 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1221.590
Langevin prior   6/ 40: energy=-1221.590
2022-12-29 03:58:15,378 - 10:48:58 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1794.102
Langevin prior  11/ 40: energy=-1794.102
2022-12-29 03:58:15,383 - 10:48:58 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2121.070
Langevin prior  16/ 40: energy=-2121.070
2022-12-29 03:58:15,389 - 10:48:58 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2198.889
Langevin prior  21/ 40: energy=-2198.889
2022-12-29 03:58:15,395 - 10:48:58 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2342.349
Langevin prior  26/ 40: energy=-2342.349
2022-12-29 03:58:15,403 - 10:48:58 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2314.672
Langevin prior  31/ 40: energy=-2314.672
2022-12-29 03:58:15,411 - 10:48:58 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2307.159
Langevin prior  36/ 40: energy=-2307.159
2022-12-29 03:58:15,419 - 10:48:58 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2352.699
Langevin prior  40/ 40: energy=-2352.699
2022-12-29 03:58:18,034 - 10:49:01 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-027-test-greedy.txt
Generation: 188 batches
2022-12-29 03:58:23,737 - 10:49:06 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:58:25,239 - 10:49:08 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:58:25,239 - 10:49:08 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:58:27,672 - 10:49:10 - 2.4s - INFO - root - --- bleu: BLEU = 31.52, 55.0/33.6/27.4/24.3 (BP=0.946, ratio=0.948, hyp_len=160911, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.52, 55.0/33.6/27.4/24.3 (BP=0.946, ratio=0.948, hyp_len=160911, ref_len=169777)
--- bleu: BLEU = 31.52, 55.0/33.6/27.4/24.3 (BP=0.946, ratio=0.948, hyp_len=160911, ref_len=169777)

2022-12-29 03:58:27,672 - 10:49:10 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:58:27,674 - 10:49:10 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:58:37,029 - 10:49:20 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    28 rec:   15.263 mi: 1.95909357 zkl:   99.922 cd:  -12.030 pos_prob:   63.001 prob_neg:   75.030 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    28 rec:   15.263 mi: 1.95909357 zkl:   99.922 cd:  -12.030 pos_prob:   63.001 prob_neg:   75.030 kl_weight:    0.062 do_ae_train: False
2022-12-29 03:58:46,404 - 10:49:29 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    28 rec:   16.871 mi: 2.05261040 zkl:   83.160 cd:   -6.226 pos_prob:   67.802 prob_neg:   74.027 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    28 rec:   16.871 mi: 2.05261040 zkl:   83.160 cd:   -6.226 pos_prob:   67.802 prob_neg:   74.027 kl_weight:    0.125 do_ae_train: False
2022-12-29 03:58:55,733 - 10:49:38 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    28 rec:   12.322 mi: 2.04917288 zkl:   75.074 cd:   -8.835 pos_prob:   63.381 prob_neg:   72.216 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    28 rec:   12.322 mi: 2.04917288 zkl:   75.074 cd:   -8.835 pos_prob:   63.381 prob_neg:   72.216 kl_weight:    0.188 do_ae_train: False
2022-12-29 03:59:05,030 - 10:49:48 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    28 rec:   16.269 mi: 1.93191063 zkl:   79.934 cd:    0.092 pos_prob:   68.198 prob_neg:   68.106 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    28 rec:   16.269 mi: 1.93191063 zkl:   79.934 cd:    0.092 pos_prob:   68.198 prob_neg:   68.106 kl_weight:    0.251 do_ae_train: False
2022-12-29 03:59:14,395 - 10:49:57 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    28 rec:   17.060 mi: 2.03124094 zkl:   68.086 cd:   -8.091 pos_prob:   67.635 prob_neg:   75.726 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    28 rec:   17.060 mi: 2.03124094 zkl:   68.086 cd:   -8.091 pos_prob:   67.635 prob_neg:   75.726 kl_weight:    0.314 do_ae_train: False
2022-12-29 03:59:14,408 - 10:49:57 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-277.830
Langevin prior   1/ 40: energy=-277.830
2022-12-29 03:59:14,413 - 10:49:57 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1129.757
Langevin prior   6/ 40: energy=-1129.757
2022-12-29 03:59:14,418 - 10:49:57 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1758.524
Langevin prior  11/ 40: energy=-1758.524
2022-12-29 03:59:14,423 - 10:49:57 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2124.727
Langevin prior  16/ 40: energy=-2124.727
2022-12-29 03:59:14,429 - 10:49:57 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2289.424
Langevin prior  21/ 40: energy=-2289.424
2022-12-29 03:59:14,436 - 10:49:57 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2229.138
Langevin prior  26/ 40: energy=-2229.138
2022-12-29 03:59:14,445 - 10:49:57 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2264.740
Langevin prior  31/ 40: energy=-2264.740
2022-12-29 03:59:14,454 - 10:49:57 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2141.323
Langevin prior  36/ 40: energy=-2141.323
2022-12-29 03:59:14,461 - 10:49:57 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2206.594
Langevin prior  40/ 40: energy=-2206.594
2022-12-29 03:59:17,082 - 10:50:00 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-028-test-greedy.txt
Generation: 188 batches
2022-12-29 03:59:22,774 - 10:50:05 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:59:24,291 - 10:50:07 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 03:59:24,291 - 10:50:07 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 03:59:26,728 - 10:50:09 - 2.4s - INFO - root - --- bleu: BLEU = 30.45, 53.5/32.5/26.5/23.5 (BP=0.944, ratio=0.945, hyp_len=160505, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.45, 53.5/32.5/26.5/23.5 (BP=0.944, ratio=0.945, hyp_len=160505, ref_len=169777)
--- bleu: BLEU = 30.45, 53.5/32.5/26.5/23.5 (BP=0.944, ratio=0.945, hyp_len=160505, ref_len=169777)

2022-12-29 03:59:26,729 - 10:50:09 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 03:59:26,730 - 10:50:09 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 03:59:36,094 - 10:50:19 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    29 rec:   19.379 mi: 1.97912180 zkl:   64.706 cd:   -9.032 pos_prob:   68.224 prob_neg:   77.256 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    29 rec:   19.379 mi: 1.97912180 zkl:   64.706 cd:   -9.032 pos_prob:   68.224 prob_neg:   77.256 kl_weight:    0.396 do_ae_train: False
2022-12-29 03:59:45,542 - 10:50:28 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    29 rec:   15.448 mi: 2.08569670 zkl:   63.235 cd:    2.283 pos_prob:   72.494 prob_neg:   70.211 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    29 rec:   15.448 mi: 2.08569670 zkl:   63.235 cd:    2.283 pos_prob:   72.494 prob_neg:   70.211 kl_weight:    0.459 do_ae_train: False
2022-12-29 03:59:54,884 - 10:50:38 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    29 rec:   18.386 mi: 1.94604790 zkl:   67.024 cd:   -4.792 pos_prob:   77.162 prob_neg:   81.955 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    29 rec:   18.386 mi: 1.94604790 zkl:   67.024 cd:   -4.792 pos_prob:   77.162 prob_neg:   81.955 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:00:04,175 - 10:50:47 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    29 rec:   19.217 mi: 1.94842052 zkl:   69.117 cd:   -0.754 pos_prob:   76.155 prob_neg:   76.908 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    29 rec:   19.217 mi: 1.94842052 zkl:   69.117 cd:   -0.754 pos_prob:   76.155 prob_neg:   76.908 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:00:13,471 - 10:50:56 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    29 rec:   31.530 mi: 1.95668280 zkl:   65.910 cd:   -7.969 pos_prob:   68.615 prob_neg:   76.584 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    29 rec:   31.530 mi: 1.95668280 zkl:   65.910 cd:   -7.969 pos_prob:   68.615 prob_neg:   76.584 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:00:13,484 - 10:50:56 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-268.997
Langevin prior   1/ 40: energy=-268.997
2022-12-29 04:00:13,490 - 10:50:56 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1153.832
Langevin prior   6/ 40: energy=-1153.832
2022-12-29 04:00:13,495 - 10:50:56 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1761.756
Langevin prior  11/ 40: energy=-1761.756
2022-12-29 04:00:13,501 - 10:50:56 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2033.171
Langevin prior  16/ 40: energy=-2033.171
2022-12-29 04:00:13,507 - 10:50:56 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2112.848
Langevin prior  21/ 40: energy=-2112.848
2022-12-29 04:00:13,515 - 10:50:56 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2226.114
Langevin prior  26/ 40: energy=-2226.114
2022-12-29 04:00:13,523 - 10:50:56 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2215.002
Langevin prior  31/ 40: energy=-2215.002
2022-12-29 04:00:13,532 - 10:50:56 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2245.954
Langevin prior  36/ 40: energy=-2245.954
2022-12-29 04:00:13,540 - 10:50:56 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2253.650
Langevin prior  40/ 40: energy=-2253.650
2022-12-29 04:09:58,185 - 11:00:41 - 584.6s - INFO - root - Negative Log-likehood -163.820387
200 done. -142.1972816262783
400 done. -137.05861708792764
600 done. -136.8191189868001
800 done. -137.43279074863494
1000 done. -136.41267131630013
1200 done. -137.07239672876966
1400 done. -138.12099123399693
1600 done. -136.52506209456647
1800 done. -135.9846527120657
2000 done. -152.13900635954738
2200 done. -162.4442926337501
2400 done. -173.6381996758172
2600 done. -183.4162588017236
2800 done. -189.90361113687837
3000 done. -198.89498111866925
3200 done. -204.11312203976854
3400 done. -210.63172142162594
3600 done. -214.81111247620242
3800 done. -219.76047765695208
4000 done. -223.5872032082009
4200 done. -214.69878403588342
4400 done. -205.57192598270368
4600 done. -197.44232912490818
4800 done. -189.904008685767
5000 done. -183.14993543545353
5200 done. -176.79745012883214
5400 done. -171.00741722333143
5600 done. -165.61858104507715
Negative Log-likehood -163.820387
2022-12-29 04:09:58,185 - 11:00:41 - 0.0s - INFO - root - log-likelihood:   -163.820
log-likelihood:   -163.820
2022-12-29 04:10:45,248 - 11:01:28 - 47.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-029-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 04:10:45,257 - 11:01:28 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-029-test-greedy.txt
Generation: 188 batches
2022-12-29 04:10:50,927 - 11:01:34 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:10:52,436 - 11:01:35 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:10:52,436 - 11:01:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:10:54,895 - 11:01:38 - 2.5s - INFO - root - --- bleu: BLEU = 29.83, 52.9/31.9/26.1/23.1 (BP=0.939, ratio=0.941, hyp_len=159781, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.83, 52.9/31.9/26.1/23.1 (BP=0.939, ratio=0.941, hyp_len=159781, ref_len=169777)
--- bleu: BLEU = 29.83, 52.9/31.9/26.1/23.1 (BP=0.939, ratio=0.941, hyp_len=159781, ref_len=169777)

2022-12-29 04:10:54,897 - 11:01:38 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:10:54,900 - 11:01:38 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:11:04,392 - 11:01:47 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    30 rec:   19.309 mi: 2.05494142 zkl:   65.263 cd:   -0.188 pos_prob:   75.019 prob_neg:   75.207 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    30 rec:   19.309 mi: 2.05494142 zkl:   65.263 cd:   -0.188 pos_prob:   75.019 prob_neg:   75.207 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:11:13,823 - 11:01:57 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    30 rec:   20.082 mi: 2.07061076 zkl:   65.871 cd:   -1.910 pos_prob:   75.279 prob_neg:   77.189 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    30 rec:   20.082 mi: 2.07061076 zkl:   65.871 cd:   -1.910 pos_prob:   75.279 prob_neg:   77.189 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:11:23,204 - 11:02:06 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    30 rec:   15.656 mi: 2.03853965 zkl:   64.131 cd:   -2.469 pos_prob:   75.963 prob_neg:   78.433 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    30 rec:   15.656 mi: 2.03853965 zkl:   64.131 cd:   -2.469 pos_prob:   75.963 prob_neg:   78.433 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:11:32,517 - 11:02:15 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    30 rec:   14.588 mi: 1.89474380 zkl:   68.043 cd:    2.108 pos_prob:   84.686 prob_neg:   82.577 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    30 rec:   14.588 mi: 1.89474380 zkl:   68.043 cd:    2.108 pos_prob:   84.686 prob_neg:   82.577 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:11:41,826 - 11:02:25 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    30 rec:   18.449 mi: 2.04171109 zkl:   61.540 cd:   -3.445 pos_prob:   76.834 prob_neg:   80.279 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    30 rec:   18.449 mi: 2.04171109 zkl:   61.540 cd:   -3.445 pos_prob:   76.834 prob_neg:   80.279 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:11:41,838 - 11:02:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-289.887
Langevin prior   1/ 40: energy=-289.887
2022-12-29 04:11:41,843 - 11:02:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1262.087
Langevin prior   6/ 40: energy=-1262.087
2022-12-29 04:11:41,848 - 11:02:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1803.719
Langevin prior  11/ 40: energy=-1803.719
2022-12-29 04:11:41,854 - 11:02:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2197.944
Langevin prior  16/ 40: energy=-2197.944
2022-12-29 04:11:41,859 - 11:02:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2335.431
Langevin prior  21/ 40: energy=-2335.431
2022-12-29 04:11:41,866 - 11:02:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2364.490
Langevin prior  26/ 40: energy=-2364.490
2022-12-29 04:11:41,873 - 11:02:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2352.332
Langevin prior  31/ 40: energy=-2352.332
2022-12-29 04:11:41,882 - 11:02:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2354.881
Langevin prior  36/ 40: energy=-2354.881
2022-12-29 04:11:41,889 - 11:02:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2443.042
Langevin prior  40/ 40: energy=-2443.042
2022-12-29 04:21:26,239 - 11:12:09 - 584.3s - INFO - root - Negative Log-likehood -165.374547
200 done. -144.24113099554003
400 done. -139.24132192596502
600 done. -139.0179256604773
800 done. -139.70241825894362
1000 done. -138.62538421939118
1200 done. -139.35434245035825
1400 done. -140.46033542947583
1600 done. -138.86506397085304
1800 done. -138.31617244440372
2000 done. -154.58888153706314
2200 done. -164.9170150421128
2400 done. -175.8231182493356
2600 done. -185.49335614724527
2800 done. -191.9097805071194
3000 done. -200.75504846917318
3200 done. -205.88721411693135
3400 done. -212.43933686540632
3600 done. -216.69978752083483
3800 done. -221.66316837151234
4000 done. -225.28316219168065
4200 done. -216.36669337399522
4400 done. -207.21186560658188
4600 done. -199.06315951318425
4800 done. -191.51008868012948
5000 done. -184.74253431027088
5200 done. -178.37254018225408
5400 done. -172.5621721340969
5600 done. -167.17256591473793
Negative Log-likehood -165.374547
2022-12-29 04:21:26,239 - 11:12:09 - 0.0s - INFO - root - log-likelihood:   -165.375
log-likelihood:   -165.375
2022-12-29 04:22:13,362 - 11:12:56 - 47.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-030-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 04:22:13,383 - 11:12:56 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-030-test-greedy.txt
Generation: 188 batches
2022-12-29 04:22:19,083 - 11:13:02 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:22:20,622 - 11:13:03 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:22:20,622 - 11:13:03 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:22:23,106 - 11:13:06 - 2.5s - INFO - root - --- bleu: BLEU = 29.79, 51.7/31.2/25.5/22.6 (BP=0.961, ratio=0.961, hyp_len=163201, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.79, 51.7/31.2/25.5/22.6 (BP=0.961, ratio=0.961, hyp_len=163201, ref_len=169777)
--- bleu: BLEU = 29.79, 51.7/31.2/25.5/22.6 (BP=0.961, ratio=0.961, hyp_len=163201, ref_len=169777)

2022-12-29 04:22:23,107 - 11:13:06 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:22:23,108 - 11:13:06 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:22:32,263 - 11:13:15 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    31 rec:   13.816 mi: 2.00550389 zkl:  150.158 cd:  -14.486 pos_prob:   64.726 prob_neg:   79.211 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    31 rec:   13.816 mi: 2.00550389 zkl:  150.158 cd:  -14.486 pos_prob:   64.726 prob_neg:   79.211 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:22:41,479 - 11:13:24 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    31 rec:   14.658 mi: 1.90668726 zkl:  152.460 cd:  -22.981 pos_prob:   53.038 prob_neg:   76.020 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    31 rec:   14.658 mi: 1.90668726 zkl:  152.460 cd:  -22.981 pos_prob:   53.038 prob_neg:   76.020 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:22:50,604 - 11:13:33 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    31 rec:    9.684 mi: 1.90555203 zkl:  155.884 cd:  -17.363 pos_prob:   60.350 prob_neg:   77.713 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    31 rec:    9.684 mi: 1.90555203 zkl:  155.884 cd:  -17.363 pos_prob:   60.350 prob_neg:   77.713 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:22:59,616 - 11:13:42 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    31 rec:   12.231 mi: 1.98731601 zkl:  157.773 cd:  -25.796 pos_prob:   51.568 prob_neg:   77.365 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    31 rec:   12.231 mi: 1.98731601 zkl:  157.773 cd:  -25.796 pos_prob:   51.568 prob_neg:   77.365 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:23:08,635 - 11:13:51 - 9.0s - INFO - root - batch/max_batch/ep:   500/   529/    31 rec:   14.681 mi: 2.00075340 zkl:  158.551 cd:  -33.388 pos_prob:   49.451 prob_neg:   82.838 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    31 rec:   14.681 mi: 2.00075340 zkl:  158.551 cd:  -33.388 pos_prob:   49.451 prob_neg:   82.838 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:23:08,648 - 11:13:51 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-332.786
Langevin prior   1/ 40: energy=-332.786
2022-12-29 04:23:08,653 - 11:13:51 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1202.350
Langevin prior   6/ 40: energy=-1202.350
2022-12-29 04:23:08,658 - 11:13:51 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1787.481
Langevin prior  11/ 40: energy=-1787.481
2022-12-29 04:23:08,664 - 11:13:51 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2044.221
Langevin prior  16/ 40: energy=-2044.221
2022-12-29 04:23:08,669 - 11:13:51 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2169.699
Langevin prior  21/ 40: energy=-2169.699
2022-12-29 04:23:08,677 - 11:13:51 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2278.593
Langevin prior  26/ 40: energy=-2278.593
2022-12-29 04:23:08,685 - 11:13:51 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2386.963
Langevin prior  31/ 40: energy=-2386.963
2022-12-29 04:23:08,694 - 11:13:51 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2408.660
Langevin prior  36/ 40: energy=-2408.660
2022-12-29 04:23:08,701 - 11:13:51 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2396.464
Langevin prior  40/ 40: energy=-2396.464
2022-12-29 04:23:11,256 - 11:13:54 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-031-test-greedy.txt
Generation: 188 batches
2022-12-29 04:23:16,920 - 11:14:00 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:23:18,411 - 11:14:01 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:23:18,411 - 11:14:01 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:23:21,087 - 11:14:04 - 2.7s - INFO - root - --- bleu: BLEU = 31.24, 55.0/33.7/27.7/24.7 (BP=0.931, ratio=0.934, hyp_len=158490, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.24, 55.0/33.7/27.7/24.7 (BP=0.931, ratio=0.934, hyp_len=158490, ref_len=169777)
--- bleu: BLEU = 31.24, 55.0/33.7/27.7/24.7 (BP=0.931, ratio=0.934, hyp_len=158490, ref_len=169777)

2022-12-29 04:23:21,087 - 11:14:04 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:23:21,089 - 11:14:04 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:23:30,265 - 11:14:13 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    32 rec:    9.752 mi: 2.02319956 zkl:  159.680 cd:  -36.733 pos_prob:   49.754 prob_neg:   86.487 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    32 rec:    9.752 mi: 2.02319956 zkl:  159.680 cd:  -36.733 pos_prob:   49.754 prob_neg:   86.487 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:23:39,403 - 11:14:22 - 9.1s - INFO - root - batch/max_batch/ep:   200/   529/    32 rec:   12.996 mi: 1.93103266 zkl:  160.286 cd:  -27.615 pos_prob:   51.345 prob_neg:   78.960 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    32 rec:   12.996 mi: 1.93103266 zkl:  160.286 cd:  -27.615 pos_prob:   51.345 prob_neg:   78.960 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:23:48,540 - 11:14:31 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    32 rec:   13.263 mi: 1.92528009 zkl:  158.899 cd:  -29.232 pos_prob:   47.363 prob_neg:   76.594 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    32 rec:   13.263 mi: 1.92528009 zkl:  158.899 cd:  -29.232 pos_prob:   47.363 prob_neg:   76.594 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:23:57,802 - 11:14:40 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    32 rec:   11.783 mi: 1.93876386 zkl:  161.900 cd:  -34.152 pos_prob:   46.677 prob_neg:   80.829 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    32 rec:   11.783 mi: 1.93876386 zkl:  161.900 cd:  -34.152 pos_prob:   46.677 prob_neg:   80.829 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:24:06,999 - 11:14:50 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    32 rec:   13.505 mi: 1.90915728 zkl:  166.300 cd:  -34.836 pos_prob:   44.370 prob_neg:   79.206 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    32 rec:   13.505 mi: 1.90915728 zkl:  166.300 cd:  -34.836 pos_prob:   44.370 prob_neg:   79.206 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:24:07,011 - 11:14:50 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-238.427
Langevin prior   1/ 40: energy=-238.427
2022-12-29 04:24:07,017 - 11:14:50 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1127.560
Langevin prior   6/ 40: energy=-1127.560
2022-12-29 04:24:07,022 - 11:14:50 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1774.998
Langevin prior  11/ 40: energy=-1774.998
2022-12-29 04:24:07,027 - 11:14:50 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2109.328
Langevin prior  16/ 40: energy=-2109.328
2022-12-29 04:24:07,032 - 11:14:50 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2287.504
Langevin prior  21/ 40: energy=-2287.504
2022-12-29 04:24:07,039 - 11:14:50 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2392.032
Langevin prior  26/ 40: energy=-2392.032
2022-12-29 04:24:07,048 - 11:14:50 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2425.092
Langevin prior  31/ 40: energy=-2425.092
2022-12-29 04:24:07,056 - 11:14:50 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2411.725
Langevin prior  36/ 40: energy=-2411.725
2022-12-29 04:24:07,064 - 11:14:50 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2439.813
Langevin prior  40/ 40: energy=-2439.813
2022-12-29 04:24:09,600 - 11:14:52 - 2.5s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-032-test-greedy.txt
Generation: 188 batches
2022-12-29 04:24:15,300 - 11:14:58 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:24:16,833 - 11:15:00 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:24:16,834 - 11:15:00 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:24:19,303 - 11:15:02 - 2.5s - INFO - root - --- bleu: BLEU = 31.37, 53.4/32.2/26.3/23.3 (BP=0.978, ratio=0.978, hyp_len=166116, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.37, 53.4/32.2/26.3/23.3 (BP=0.978, ratio=0.978, hyp_len=166116, ref_len=169777)
--- bleu: BLEU = 31.37, 53.4/32.2/26.3/23.3 (BP=0.978, ratio=0.978, hyp_len=166116, ref_len=169777)

2022-12-29 04:24:19,303 - 11:15:02 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:24:19,305 - 11:15:02 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:24:28,368 - 11:15:11 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    33 rec:    7.604 mi: 1.97810435 zkl:  162.885 cd:  -30.161 pos_prob:   47.956 prob_neg:   78.117 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    33 rec:    7.604 mi: 1.97810435 zkl:  162.885 cd:  -30.161 pos_prob:   47.956 prob_neg:   78.117 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:24:37,587 - 11:15:20 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    33 rec:   14.616 mi: 1.91647017 zkl:  163.353 cd:  -33.134 pos_prob:   45.534 prob_neg:   78.668 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    33 rec:   14.616 mi: 1.91647017 zkl:  163.353 cd:  -33.134 pos_prob:   45.534 prob_neg:   78.668 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:24:46,872 - 11:15:30 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    33 rec:    9.180 mi: 1.93399858 zkl:  164.552 cd:  -37.385 pos_prob:   42.131 prob_neg:   79.516 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    33 rec:    9.180 mi: 1.93399858 zkl:  164.552 cd:  -37.385 pos_prob:   42.131 prob_neg:   79.516 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:24:55,963 - 11:15:39 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    33 rec:   12.627 mi: 1.81327975 zkl:  165.268 cd:  -34.832 pos_prob:   41.788 prob_neg:   76.620 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    33 rec:   12.627 mi: 1.81327975 zkl:  165.268 cd:  -34.832 pos_prob:   41.788 prob_neg:   76.620 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:25:05,169 - 11:15:48 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    33 rec:   12.415 mi: 1.87432063 zkl:  163.123 cd:  -41.092 pos_prob:   38.492 prob_neg:   79.584 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    33 rec:   12.415 mi: 1.87432063 zkl:  163.123 cd:  -41.092 pos_prob:   38.492 prob_neg:   79.584 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:25:05,182 - 11:15:48 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-314.112
Langevin prior   1/ 40: energy=-314.112
2022-12-29 04:25:05,187 - 11:15:48 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1304.869
Langevin prior   6/ 40: energy=-1304.869
2022-12-29 04:25:05,192 - 11:15:48 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1888.204
Langevin prior  11/ 40: energy=-1888.204
2022-12-29 04:25:05,198 - 11:15:48 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2226.921
Langevin prior  16/ 40: energy=-2226.921
2022-12-29 04:25:05,203 - 11:15:48 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2405.244
Langevin prior  21/ 40: energy=-2405.244
2022-12-29 04:25:05,209 - 11:15:48 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2432.743
Langevin prior  26/ 40: energy=-2432.743
2022-12-29 04:25:05,217 - 11:15:48 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2484.349
Langevin prior  31/ 40: energy=-2484.349
2022-12-29 04:25:05,226 - 11:15:48 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2476.210
Langevin prior  36/ 40: energy=-2476.210
2022-12-29 04:25:05,233 - 11:15:48 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2513.748
Langevin prior  40/ 40: energy=-2513.748
2022-12-29 04:25:07,840 - 11:15:51 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-033-test-greedy.txt
Generation: 188 batches
2022-12-29 04:25:13,520 - 11:15:56 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:25:15,038 - 11:15:58 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:25:15,038 - 11:15:58 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:25:17,488 - 11:16:00 - 2.4s - INFO - root - --- bleu: BLEU = 31.38, 54.0/32.6/26.7/23.7 (BP=0.966, ratio=0.966, hyp_len=164048, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.38, 54.0/32.6/26.7/23.7 (BP=0.966, ratio=0.966, hyp_len=164048, ref_len=169777)
--- bleu: BLEU = 31.38, 54.0/32.6/26.7/23.7 (BP=0.966, ratio=0.966, hyp_len=164048, ref_len=169777)

2022-12-29 04:25:17,488 - 11:16:00 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:25:17,490 - 11:16:00 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:25:26,945 - 11:16:10 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    34 rec:   11.205 mi: 2.08194399 zkl:   97.254 cd:  -19.309 pos_prob:   54.226 prob_neg:   73.535 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    34 rec:   11.205 mi: 2.08194399 zkl:   97.254 cd:  -19.309 pos_prob:   54.226 prob_neg:   73.535 kl_weight:    0.062 do_ae_train: False
2022-12-29 04:25:36,276 - 11:16:19 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    34 rec:   10.421 mi: 1.97746491 zkl:   77.462 cd:   -4.276 pos_prob:   63.307 prob_neg:   67.583 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    34 rec:   10.421 mi: 1.97746491 zkl:   77.462 cd:   -4.276 pos_prob:   63.307 prob_neg:   67.583 kl_weight:    0.125 do_ae_train: False
2022-12-29 04:25:45,669 - 11:16:28 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    34 rec:   17.513 mi: 1.93083823 zkl:   80.720 cd:   -0.772 pos_prob:   66.644 prob_neg:   67.416 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    34 rec:   17.513 mi: 1.93083823 zkl:   80.720 cd:   -0.772 pos_prob:   66.644 prob_neg:   67.416 kl_weight:    0.188 do_ae_train: False
2022-12-29 04:25:54,998 - 11:16:38 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    34 rec:   18.906 mi: 1.70046246 zkl:   70.302 cd:    5.467 pos_prob:   67.874 prob_neg:   62.407 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    34 rec:   18.906 mi: 1.70046246 zkl:   70.302 cd:    5.467 pos_prob:   67.874 prob_neg:   62.407 kl_weight:    0.251 do_ae_train: False
2022-12-29 04:26:04,430 - 11:16:47 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    34 rec:   14.426 mi: 1.92030621 zkl:   64.257 cd:   -2.331 pos_prob:   64.667 prob_neg:   66.999 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    34 rec:   14.426 mi: 1.92030621 zkl:   64.257 cd:   -2.331 pos_prob:   64.667 prob_neg:   66.999 kl_weight:    0.314 do_ae_train: False
2022-12-29 04:26:04,443 - 11:16:47 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-290.764
Langevin prior   1/ 40: energy=-290.764
2022-12-29 04:26:04,448 - 11:16:47 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1173.813
Langevin prior   6/ 40: energy=-1173.813
2022-12-29 04:26:04,454 - 11:16:47 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1656.347
Langevin prior  11/ 40: energy=-1656.347
2022-12-29 04:26:04,460 - 11:16:47 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1951.750
Langevin prior  16/ 40: energy=-1951.750
2022-12-29 04:26:04,466 - 11:16:47 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2051.237
Langevin prior  21/ 40: energy=-2051.237
2022-12-29 04:26:04,474 - 11:16:47 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2041.469
Langevin prior  26/ 40: energy=-2041.469
2022-12-29 04:26:04,483 - 11:16:47 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2101.552
Langevin prior  31/ 40: energy=-2101.552
2022-12-29 04:26:04,493 - 11:16:47 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2083.368
Langevin prior  36/ 40: energy=-2083.368
2022-12-29 04:26:04,501 - 11:16:47 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2113.650
Langevin prior  40/ 40: energy=-2113.650
2022-12-29 04:26:07,142 - 11:16:50 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-034-test-greedy.txt
Generation: 188 batches
2022-12-29 04:26:12,813 - 11:16:56 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:26:14,320 - 11:16:57 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:26:14,321 - 11:16:57 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:26:16,748 - 11:16:59 - 2.4s - INFO - root - --- bleu: BLEU = 30.61, 53.5/32.4/26.6/23.7 (BP=0.947, ratio=0.949, hyp_len=161057, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.61, 53.5/32.4/26.6/23.7 (BP=0.947, ratio=0.949, hyp_len=161057, ref_len=169777)
--- bleu: BLEU = 30.61, 53.5/32.4/26.6/23.7 (BP=0.947, ratio=0.949, hyp_len=161057, ref_len=169777)

2022-12-29 04:26:16,749 - 11:16:59 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:26:16,750 - 11:16:59 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:26:26,048 - 11:17:09 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    35 rec:   12.136 mi: 1.87531137 zkl:   66.335 cd:    4.308 pos_prob:   73.679 prob_neg:   69.371 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    35 rec:   12.136 mi: 1.87531137 zkl:   66.335 cd:    4.308 pos_prob:   73.679 prob_neg:   69.371 kl_weight:    0.396 do_ae_train: False
2022-12-29 04:26:35,474 - 11:17:18 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    35 rec:   17.451 mi: 1.97458279 zkl:   62.721 cd:   -4.180 pos_prob:   63.709 prob_neg:   67.889 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    35 rec:   17.451 mi: 1.97458279 zkl:   62.721 cd:   -4.180 pos_prob:   63.709 prob_neg:   67.889 kl_weight:    0.459 do_ae_train: False
2022-12-29 04:26:44,765 - 11:17:27 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    35 rec:   19.110 mi: 2.09879446 zkl:   63.494 cd:    0.577 pos_prob:   68.671 prob_neg:   68.093 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    35 rec:   19.110 mi: 2.09879446 zkl:   63.494 cd:    0.577 pos_prob:   68.671 prob_neg:   68.093 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:26:54,141 - 11:17:37 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    35 rec:   16.629 mi: 2.12920427 zkl:   63.632 cd:   -2.365 pos_prob:   71.490 prob_neg:   73.855 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    35 rec:   16.629 mi: 2.12920427 zkl:   63.632 cd:   -2.365 pos_prob:   71.490 prob_neg:   73.855 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:27:03,365 - 11:17:46 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    35 rec:   21.053 mi: 1.97725797 zkl:   61.445 cd:   -3.256 pos_prob:   65.972 prob_neg:   69.228 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    35 rec:   21.053 mi: 1.97725797 zkl:   61.445 cd:   -3.256 pos_prob:   65.972 prob_neg:   69.228 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:27:03,378 - 11:17:46 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-224.295
Langevin prior   1/ 40: energy=-224.295
2022-12-29 04:27:03,384 - 11:17:46 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1028.043
Langevin prior   6/ 40: energy=-1028.043
2022-12-29 04:27:03,389 - 11:17:46 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1657.488
Langevin prior  11/ 40: energy=-1657.488
2022-12-29 04:27:03,394 - 11:17:46 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2002.580
Langevin prior  16/ 40: energy=-2002.580
2022-12-29 04:27:03,400 - 11:17:46 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2017.585
Langevin prior  21/ 40: energy=-2017.585
2022-12-29 04:27:03,406 - 11:17:46 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2067.035
Langevin prior  26/ 40: energy=-2067.035
2022-12-29 04:27:03,413 - 11:17:46 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2135.839
Langevin prior  31/ 40: energy=-2135.839
2022-12-29 04:27:03,421 - 11:17:46 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2155.766
Langevin prior  36/ 40: energy=-2155.766
2022-12-29 04:27:03,429 - 11:17:46 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2144.794
Langevin prior  40/ 40: energy=-2144.794
2022-12-29 04:36:48,384 - 11:27:31 - 585.0s - INFO - root - Negative Log-likehood -169.816514
200 done. -147.7694190262814
400 done. -142.2209626342424
600 done. -141.9155302207312
800 done. -142.58240679022276
1000 done. -141.32299939831938
1200 done. -142.10800189431416
1400 done. -143.26611336387913
1600 done. -141.6087374842973
1800 done. -141.0401307493118
2000 done. -157.94010019932824
2200 done. -168.61822819300065
2400 done. -180.1390892108134
2600 done. -190.55521856196714
2800 done. -196.98927229677594
3000 done. -205.93994256978857
3200 done. -211.3234016188906
3400 done. -218.1197963028418
3600 done. -222.51887912992106
3800 done. -227.59407362428976
4000 done. -231.4472228090202
4200 done. -222.27005715500988
4400 done. -212.86285130991365
4600 done. -204.47360466713153
4800 done. -196.7141684403109
5000 done. -189.7368431915554
5200 done. -183.1771719350882
5400 done. -177.2120987562783
5600 done. -171.67099646053262
Negative Log-likehood -169.816514
2022-12-29 04:36:48,385 - 11:27:31 - 0.0s - INFO - root - log-likelihood:   -169.817
log-likelihood:   -169.817
2022-12-29 04:37:35,957 - 11:28:19 - 47.6s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-035-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 04:37:35,978 - 11:28:19 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-035-test-greedy.txt
Generation: 188 batches
2022-12-29 04:37:41,646 - 11:28:24 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:37:43,185 - 11:28:26 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:37:43,185 - 11:28:26 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:37:45,657 - 11:28:28 - 2.5s - INFO - root - --- bleu: BLEU = 30.13, 52.4/31.8/26.1/23.2 (BP=0.951, ratio=0.952, hyp_len=161643, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.13, 52.4/31.8/26.1/23.2 (BP=0.951, ratio=0.952, hyp_len=161643, ref_len=169777)
--- bleu: BLEU = 30.13, 52.4/31.8/26.1/23.2 (BP=0.951, ratio=0.952, hyp_len=161643, ref_len=169777)

2022-12-29 04:37:45,657 - 11:28:28 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:37:45,659 - 11:28:28 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:37:54,961 - 11:28:38 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    36 rec:   20.636 mi: 1.99553621 zkl:   60.359 cd:   -1.009 pos_prob:   69.867 prob_neg:   70.876 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    36 rec:   20.636 mi: 1.99553621 zkl:   60.359 cd:   -1.009 pos_prob:   69.867 prob_neg:   70.876 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:38:04,498 - 11:28:47 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    36 rec:   16.995 mi: 1.89661586 zkl:   61.374 cd:    5.103 pos_prob:   77.451 prob_neg:   72.348 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    36 rec:   16.995 mi: 1.89661586 zkl:   61.374 cd:    5.103 pos_prob:   77.451 prob_neg:   72.348 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:38:13,943 - 11:28:57 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    36 rec:   23.726 mi: 2.03469563 zkl:   65.193 cd:   -9.844 pos_prob:   68.710 prob_neg:   78.554 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    36 rec:   23.726 mi: 2.03469563 zkl:   65.193 cd:   -9.844 pos_prob:   68.710 prob_neg:   78.554 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:38:23,243 - 11:29:06 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    36 rec:   17.361 mi: 2.03259397 zkl:   64.727 cd:    0.974 pos_prob:   75.786 prob_neg:   74.812 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    36 rec:   17.361 mi: 2.03259397 zkl:   64.727 cd:    0.974 pos_prob:   75.786 prob_neg:   74.812 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:38:32,557 - 11:29:15 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    36 rec:   15.978 mi: 2.07723165 zkl:   61.302 cd:    1.321 pos_prob:   76.790 prob_neg:   75.469 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    36 rec:   15.978 mi: 2.07723165 zkl:   61.302 cd:    1.321 pos_prob:   76.790 prob_neg:   75.469 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:38:32,569 - 11:29:15 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-312.777
Langevin prior   1/ 40: energy=-312.777
2022-12-29 04:38:32,575 - 11:29:15 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1249.299
Langevin prior   6/ 40: energy=-1249.299
2022-12-29 04:38:32,580 - 11:29:15 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1790.464
Langevin prior  11/ 40: energy=-1790.464
2022-12-29 04:38:32,585 - 11:29:15 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2147.936
Langevin prior  16/ 40: energy=-2147.936
2022-12-29 04:38:32,591 - 11:29:15 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2251.655
Langevin prior  21/ 40: energy=-2251.655
2022-12-29 04:38:32,596 - 11:29:15 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2293.744
Langevin prior  26/ 40: energy=-2293.744
2022-12-29 04:38:32,604 - 11:29:15 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2349.295
Langevin prior  31/ 40: energy=-2349.295
2022-12-29 04:38:32,612 - 11:29:15 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2356.776
Langevin prior  36/ 40: energy=-2356.776
2022-12-29 04:38:32,619 - 11:29:15 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2336.494
Langevin prior  40/ 40: energy=-2336.494
2022-12-29 04:48:17,376 - 11:39:00 - 584.8s - INFO - root - Negative Log-likehood -168.642396
200 done. -146.6726250767281
400 done. -141.61078961001724
600 done. -141.4247352337546
800 done. -142.17930108409846
1000 done. -140.9986034040036
1200 done. -141.78021856236393
1400 done. -142.9344868359048
1600 done. -141.20365278053097
1800 done. -140.6274368617522
2000 done. -157.53280178340407
2200 done. -168.14167521682307
2400 done. -179.49902554039286
2600 done. -189.73911966349795
2800 done. -196.05289542182
3000 done. -204.79482889170566
3200 done. -210.06220044899052
3400 done. -216.7170357044546
3600 done. -221.0806809416144
3800 done. -226.11414634094947
4000 done. -229.8598303353272
4200 done. -220.76811776700194
4400 done. -211.42371276451337
4600 done. -203.09675675548073
4800 done. -195.37351795525342
5000 done. -188.4321074001887
5200 done. -181.9161921489656
5400 done. -176.00051187621054
5600 done. -170.48439080852071
Negative Log-likehood -168.642396
2022-12-29 04:48:17,376 - 11:39:00 - 0.0s - INFO - root - log-likelihood:   -168.642
log-likelihood:   -168.642
2022-12-29 04:49:04,337 - 11:39:47 - 47.0s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-036-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 04:49:04,339 - 11:39:47 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-036-test-greedy.txt
Generation: 188 batches
2022-12-29 04:49:10,018 - 11:39:53 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:49:11,570 - 11:39:54 - 1.6s - INFO - root - Generation Done
Generation Done
2022-12-29 04:49:11,570 - 11:39:54 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:49:14,085 - 11:39:57 - 2.5s - INFO - root - --- bleu: BLEU = 30.26, 51.8/31.4/25.7/22.8 (BP=0.969, ratio=0.969, hyp_len=164558, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.26, 51.8/31.4/25.7/22.8 (BP=0.969, ratio=0.969, hyp_len=164558, ref_len=169777)
--- bleu: BLEU = 30.26, 51.8/31.4/25.7/22.8 (BP=0.969, ratio=0.969, hyp_len=164558, ref_len=169777)

2022-12-29 04:49:14,086 - 11:39:57 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:49:14,087 - 11:39:57 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:49:23,196 - 11:40:06 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    37 rec:    7.859 mi: 1.88454235 zkl:  153.716 cd:   -4.395 pos_prob:   69.068 prob_neg:   73.464 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    37 rec:    7.859 mi: 1.88454235 zkl:  153.716 cd:   -4.395 pos_prob:   69.068 prob_neg:   73.464 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:49:32,592 - 11:40:15 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    37 rec:   10.989 mi: 2.10009575 zkl:  155.918 cd:  -26.438 pos_prob:   56.875 prob_neg:   83.313 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    37 rec:   10.989 mi: 2.10009575 zkl:  155.918 cd:  -26.438 pos_prob:   56.875 prob_neg:   83.313 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:49:41,766 - 11:40:24 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    37 rec:   11.633 mi: 1.90249932 zkl:  153.665 cd:  -25.782 pos_prob:   52.189 prob_neg:   77.971 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    37 rec:   11.633 mi: 1.90249932 zkl:  153.665 cd:  -25.782 pos_prob:   52.189 prob_neg:   77.971 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:49:50,885 - 11:40:34 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    37 rec:   14.232 mi: 1.86391294 zkl:  153.941 cd:  -17.331 pos_prob:   51.654 prob_neg:   68.985 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    37 rec:   14.232 mi: 1.86391294 zkl:  153.941 cd:  -17.331 pos_prob:   51.654 prob_neg:   68.985 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:50:00,097 - 11:40:43 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    37 rec:   11.649 mi: 2.12920141 zkl:  156.185 cd:  -24.272 pos_prob:   47.943 prob_neg:   72.215 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    37 rec:   11.649 mi: 2.12920141 zkl:  156.185 cd:  -24.272 pos_prob:   47.943 prob_neg:   72.215 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:50:00,109 - 11:40:43 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-279.192
Langevin prior   1/ 40: energy=-279.192
2022-12-29 04:50:00,115 - 11:40:43 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1240.020
Langevin prior   6/ 40: energy=-1240.020
2022-12-29 04:50:00,119 - 11:40:43 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1799.480
Langevin prior  11/ 40: energy=-1799.480
2022-12-29 04:50:00,125 - 11:40:43 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2061.312
Langevin prior  16/ 40: energy=-2061.312
2022-12-29 04:50:00,130 - 11:40:43 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2180.952
Langevin prior  21/ 40: energy=-2180.952
2022-12-29 04:50:00,136 - 11:40:43 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2159.399
Langevin prior  26/ 40: energy=-2159.399
2022-12-29 04:50:00,143 - 11:40:43 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2168.965
Langevin prior  31/ 40: energy=-2168.965
2022-12-29 04:50:00,152 - 11:40:43 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2111.243
Langevin prior  36/ 40: energy=-2111.243
2022-12-29 04:50:00,159 - 11:40:43 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2191.997
Langevin prior  40/ 40: energy=-2191.997
2022-12-29 04:50:02,790 - 11:40:45 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-037-test-greedy.txt
Generation: 188 batches
2022-12-29 04:50:08,447 - 11:40:51 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:50:09,968 - 11:40:53 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:50:09,968 - 11:40:53 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:50:12,416 - 11:40:55 - 2.4s - INFO - root - --- bleu: BLEU = 31.07, 53.8/32.7/26.8/23.7 (BP=0.956, ratio=0.957, hyp_len=162523, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.07, 53.8/32.7/26.8/23.7 (BP=0.956, ratio=0.957, hyp_len=162523, ref_len=169777)
--- bleu: BLEU = 31.07, 53.8/32.7/26.8/23.7 (BP=0.956, ratio=0.957, hyp_len=162523, ref_len=169777)

2022-12-29 04:50:12,416 - 11:40:55 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:50:12,417 - 11:40:55 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:50:21,546 - 11:41:04 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    38 rec:    9.171 mi: 2.05506492 zkl:  160.535 cd:  -25.285 pos_prob:   50.285 prob_neg:   75.571 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    38 rec:    9.171 mi: 2.05506492 zkl:  160.535 cd:  -25.285 pos_prob:   50.285 prob_neg:   75.571 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:50:30,878 - 11:41:14 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    38 rec:    9.275 mi: 1.74190140 zkl:  155.199 cd:  -31.831 pos_prob:   47.531 prob_neg:   79.362 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    38 rec:    9.275 mi: 1.74190140 zkl:  155.199 cd:  -31.831 pos_prob:   47.531 prob_neg:   79.362 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:50:40,020 - 11:41:23 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    38 rec:   12.566 mi: 2.07736230 zkl:  159.040 cd:  -36.962 pos_prob:   42.762 prob_neg:   79.724 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    38 rec:   12.566 mi: 2.07736230 zkl:  159.040 cd:  -36.962 pos_prob:   42.762 prob_neg:   79.724 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:50:49,164 - 11:41:32 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    38 rec:   12.845 mi: 1.92288375 zkl:  159.785 cd:  -26.977 pos_prob:   43.536 prob_neg:   70.514 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    38 rec:   12.845 mi: 1.92288375 zkl:  159.785 cd:  -26.977 pos_prob:   43.536 prob_neg:   70.514 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:50:58,336 - 11:41:41 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    38 rec:   11.967 mi: 1.99708796 zkl:  156.621 cd:  -32.004 pos_prob:   42.315 prob_neg:   74.319 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    38 rec:   11.967 mi: 1.99708796 zkl:  156.621 cd:  -32.004 pos_prob:   42.315 prob_neg:   74.319 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:50:58,349 - 11:41:41 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-260.547
Langevin prior   1/ 40: energy=-260.547
2022-12-29 04:50:58,354 - 11:41:41 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1156.410
Langevin prior   6/ 40: energy=-1156.410
2022-12-29 04:50:58,359 - 11:41:41 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1737.328
Langevin prior  11/ 40: energy=-1737.328
2022-12-29 04:50:58,364 - 11:41:41 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2038.225
Langevin prior  16/ 40: energy=-2038.225
2022-12-29 04:50:58,369 - 11:41:41 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2203.490
Langevin prior  21/ 40: energy=-2203.490
2022-12-29 04:50:58,376 - 11:41:41 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2172.289
Langevin prior  26/ 40: energy=-2172.289
2022-12-29 04:50:58,384 - 11:41:41 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2173.966
Langevin prior  31/ 40: energy=-2173.966
2022-12-29 04:50:58,393 - 11:41:41 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2180.315
Langevin prior  36/ 40: energy=-2180.315
2022-12-29 04:50:58,400 - 11:41:41 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2232.389
Langevin prior  40/ 40: energy=-2232.389
2022-12-29 04:51:01,203 - 11:41:44 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-038-test-greedy.txt
Generation: 188 batches
2022-12-29 04:51:06,872 - 11:41:50 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:51:08,394 - 11:41:51 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:51:08,394 - 11:41:51 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:51:11,173 - 11:41:54 - 2.8s - INFO - root - --- bleu: BLEU = 31.61, 54.1/33.3/27.3/24.2 (BP=0.958, ratio=0.959, hyp_len=162769, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.61, 54.1/33.3/27.3/24.2 (BP=0.958, ratio=0.959, hyp_len=162769, ref_len=169777)
--- bleu: BLEU = 31.61, 54.1/33.3/27.3/24.2 (BP=0.958, ratio=0.959, hyp_len=162769, ref_len=169777)

2022-12-29 04:51:11,174 - 11:41:54 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:51:11,175 - 11:41:54 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:51:20,673 - 11:42:03 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    39 rec:    8.015 mi: 1.94776249 zkl:  162.431 cd:  -27.193 pos_prob:   45.138 prob_neg:   72.331 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    39 rec:    8.015 mi: 1.94776249 zkl:  162.431 cd:  -27.193 pos_prob:   45.138 prob_neg:   72.331 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:51:29,836 - 11:42:13 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    39 rec:   10.239 mi: 2.00019097 zkl:  166.184 cd:  -34.579 pos_prob:   37.970 prob_neg:   72.549 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    39 rec:   10.239 mi: 2.00019097 zkl:  166.184 cd:  -34.579 pos_prob:   37.970 prob_neg:   72.549 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:51:38,905 - 11:42:22 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    39 rec:   10.477 mi: 1.91287696 zkl:  162.071 cd:  -30.307 pos_prob:   39.349 prob_neg:   69.655 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    39 rec:   10.477 mi: 1.91287696 zkl:  162.071 cd:  -30.307 pos_prob:   39.349 prob_neg:   69.655 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:51:48,124 - 11:42:31 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    39 rec:    9.490 mi: 1.91595483 zkl:  162.549 cd:  -30.210 pos_prob:   40.984 prob_neg:   71.194 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    39 rec:    9.490 mi: 1.91595483 zkl:  162.549 cd:  -30.210 pos_prob:   40.984 prob_neg:   71.194 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:51:57,451 - 11:42:40 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    39 rec:    9.970 mi: 2.04093146 zkl:  164.366 cd:  -34.740 pos_prob:   44.204 prob_neg:   78.944 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    39 rec:    9.970 mi: 2.04093146 zkl:  164.366 cd:  -34.740 pos_prob:   44.204 prob_neg:   78.944 kl_weight:    0.000 do_ae_train: True
2022-12-29 04:51:57,463 - 11:42:40 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-314.693
Langevin prior   1/ 40: energy=-314.693
2022-12-29 04:51:57,469 - 11:42:40 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1160.113
Langevin prior   6/ 40: energy=-1160.113
2022-12-29 04:51:57,474 - 11:42:40 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1751.657
Langevin prior  11/ 40: energy=-1751.657
2022-12-29 04:51:57,479 - 11:42:40 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2018.831
Langevin prior  16/ 40: energy=-2018.831
2022-12-29 04:51:57,484 - 11:42:40 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2069.457
Langevin prior  21/ 40: energy=-2069.457
2022-12-29 04:51:57,491 - 11:42:40 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2145.982
Langevin prior  26/ 40: energy=-2145.982
2022-12-29 04:51:57,502 - 11:42:40 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2186.683
Langevin prior  31/ 40: energy=-2186.683
2022-12-29 04:51:57,513 - 11:42:40 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2222.747
Langevin prior  36/ 40: energy=-2222.747
2022-12-29 04:51:57,522 - 11:42:40 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2228.463
Langevin prior  40/ 40: energy=-2228.463
2022-12-29 04:52:00,202 - 11:42:43 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-039-test-greedy.txt
Generation: 188 batches
2022-12-29 04:52:05,883 - 11:42:49 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:52:07,401 - 11:42:50 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:52:07,401 - 11:42:50 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:52:09,831 - 11:42:53 - 2.4s - INFO - root - --- bleu: BLEU = 31.50, 54.5/33.4/27.4/24.4 (BP=0.948, ratio=0.950, hyp_len=161231, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.50, 54.5/33.4/27.4/24.4 (BP=0.948, ratio=0.950, hyp_len=161231, ref_len=169777)
--- bleu: BLEU = 31.50, 54.5/33.4/27.4/24.4 (BP=0.948, ratio=0.950, hyp_len=161231, ref_len=169777)

2022-12-29 04:52:09,832 - 11:42:53 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:52:09,833 - 11:42:53 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:52:19,629 - 11:43:02 - 9.8s - INFO - root - batch/max_batch/ep:   100/   529/    40 rec:   13.666 mi: 2.07277036 zkl:   95.338 cd:  -18.457 pos_prob:   57.151 prob_neg:   75.607 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    40 rec:   13.666 mi: 2.07277036 zkl:   95.338 cd:  -18.457 pos_prob:   57.151 prob_neg:   75.607 kl_weight:    0.062 do_ae_train: False
2022-12-29 04:52:28,976 - 11:43:12 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    40 rec:   13.304 mi: 2.06847358 zkl:   85.113 cd:  -10.444 pos_prob:   58.132 prob_neg:   68.576 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    40 rec:   13.304 mi: 2.06847358 zkl:   85.113 cd:  -10.444 pos_prob:   58.132 prob_neg:   68.576 kl_weight:    0.125 do_ae_train: False
2022-12-29 04:52:38,437 - 11:43:21 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    40 rec:    8.532 mi: 1.96831381 zkl:   77.103 cd:   -5.670 pos_prob:   62.852 prob_neg:   68.523 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    40 rec:    8.532 mi: 1.96831381 zkl:   77.103 cd:   -5.670 pos_prob:   62.852 prob_neg:   68.523 kl_weight:    0.188 do_ae_train: False
2022-12-29 04:52:47,801 - 11:43:30 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    40 rec:    9.441 mi: 2.00836682 zkl:   64.904 cd:   -4.753 pos_prob:   62.992 prob_neg:   67.745 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    40 rec:    9.441 mi: 2.00836682 zkl:   64.904 cd:   -4.753 pos_prob:   62.992 prob_neg:   67.745 kl_weight:    0.251 do_ae_train: False
2022-12-29 04:52:57,140 - 11:43:40 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    40 rec:   12.424 mi: 1.82104576 zkl:   61.353 cd:    1.784 pos_prob:   63.619 prob_neg:   61.836 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    40 rec:   12.424 mi: 1.82104576 zkl:   61.353 cd:    1.784 pos_prob:   63.619 prob_neg:   61.836 kl_weight:    0.314 do_ae_train: False
2022-12-29 04:52:57,153 - 11:43:40 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-246.847
Langevin prior   1/ 40: energy=-246.847
2022-12-29 04:52:57,159 - 11:43:40 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1012.862
Langevin prior   6/ 40: energy=-1012.862
2022-12-29 04:52:57,164 - 11:43:40 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1473.376
Langevin prior  11/ 40: energy=-1473.376
2022-12-29 04:52:57,169 - 11:43:40 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1664.313
Langevin prior  16/ 40: energy=-1664.313
2022-12-29 04:52:57,174 - 11:43:40 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1759.778
Langevin prior  21/ 40: energy=-1759.778
2022-12-29 04:52:57,180 - 11:43:40 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1881.208
Langevin prior  26/ 40: energy=-1881.208
2022-12-29 04:52:57,188 - 11:43:40 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1929.285
Langevin prior  31/ 40: energy=-1929.285
2022-12-29 04:52:57,196 - 11:43:40 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1902.476
Langevin prior  36/ 40: energy=-1902.476
2022-12-29 04:52:57,203 - 11:43:40 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1937.250
Langevin prior  40/ 40: energy=-1937.250
2022-12-29 04:52:59,864 - 11:43:43 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-040-test-greedy.txt
Generation: 188 batches
2022-12-29 04:53:05,515 - 11:43:48 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:53:06,997 - 11:43:50 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 04:53:06,997 - 11:43:50 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 04:53:09,393 - 11:43:52 - 2.4s - INFO - root - --- bleu: BLEU = 29.71, 53.9/32.5/26.6/23.6 (BP=0.918, ratio=0.921, hyp_len=156321, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.71, 53.9/32.5/26.6/23.6 (BP=0.918, ratio=0.921, hyp_len=156321, ref_len=169777)
--- bleu: BLEU = 29.71, 53.9/32.5/26.6/23.6 (BP=0.918, ratio=0.921, hyp_len=156321, ref_len=169777)

2022-12-29 04:53:09,393 - 11:43:52 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 04:53:09,395 - 11:43:52 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 04:53:18,740 - 11:44:01 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    41 rec:   17.057 mi: 1.94472492 zkl:   64.171 cd:   -1.267 pos_prob:   63.129 prob_neg:   64.396 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    41 rec:   17.057 mi: 1.94472492 zkl:   64.171 cd:   -1.267 pos_prob:   63.129 prob_neg:   64.396 kl_weight:    0.396 do_ae_train: False
2022-12-29 04:53:28,217 - 11:44:11 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    41 rec:   16.726 mi: 2.02892852 zkl:   60.638 cd:    2.625 pos_prob:   69.402 prob_neg:   66.777 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    41 rec:   16.726 mi: 2.02892852 zkl:   60.638 cd:    2.625 pos_prob:   69.402 prob_neg:   66.777 kl_weight:    0.459 do_ae_train: False
2022-12-29 04:53:37,543 - 11:44:20 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    41 rec:   16.304 mi: 1.96530032 zkl:   57.717 cd:   -8.951 pos_prob:   65.191 prob_neg:   74.142 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    41 rec:   16.304 mi: 1.96530032 zkl:   57.717 cd:   -8.951 pos_prob:   65.191 prob_neg:   74.142 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:53:46,963 - 11:44:30 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    41 rec:   19.641 mi: 2.11407161 zkl:   59.914 cd:   -3.645 pos_prob:   65.668 prob_neg:   69.313 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    41 rec:   19.641 mi: 2.11407161 zkl:   59.914 cd:   -3.645 pos_prob:   65.668 prob_neg:   69.313 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:53:56,361 - 11:44:39 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    41 rec:   20.413 mi: 2.00607443 zkl:   61.479 cd:    2.764 pos_prob:   69.367 prob_neg:   66.603 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    41 rec:   20.413 mi: 2.00607443 zkl:   61.479 cd:    2.764 pos_prob:   69.367 prob_neg:   66.603 kl_weight:    0.500 do_ae_train: False
2022-12-29 04:53:56,374 - 11:44:39 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-245.252
Langevin prior   1/ 40: energy=-245.252
2022-12-29 04:53:56,385 - 11:44:39 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-996.360
Langevin prior   6/ 40: energy=-996.360
2022-12-29 04:53:56,394 - 11:44:39 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1554.634
Langevin prior  11/ 40: energy=-1554.634
2022-12-29 04:53:56,405 - 11:44:39 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1751.179
Langevin prior  16/ 40: energy=-1751.179
2022-12-29 04:53:56,416 - 11:44:39 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1850.679
Langevin prior  21/ 40: energy=-1850.679
2022-12-29 04:53:56,427 - 11:44:39 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1853.895
Langevin prior  26/ 40: energy=-1853.895
2022-12-29 04:53:56,438 - 11:44:39 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-1884.308
Langevin prior  31/ 40: energy=-1884.308
2022-12-29 04:53:56,450 - 11:44:39 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1978.295
Langevin prior  36/ 40: energy=-1978.295
2022-12-29 04:53:56,460 - 11:44:39 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1997.585
Langevin prior  40/ 40: energy=-1997.585
2022-12-29 05:03:40,429 - 11:54:23 - 584.0s - INFO - root - Negative Log-likehood -174.520021
200 done. -152.42602108592197
400 done. -147.06862264046558
600 done. -146.84390062645448
800 done. -147.85384436246264
1000 done. -146.80398634477567
1200 done. -147.58571794800662
1400 done. -148.6157391693574
1600 done. -146.84760466868568
1800 done. -146.27675419587425
2000 done. -163.29564672137835
2200 done. -174.3023480851977
2400 done. -185.8638497110055
2600 done. -196.40717413372178
2800 done. -203.058617708043
3000 done. -212.12384770351363
3200 done. -217.5465174268831
3400 done. -224.54777118151785
3600 done. -228.9495549057795
3800 done. -234.28039688011717
4000 done. -238.0692520754136
4200 done. -228.67198013405175
4400 done. -218.97260045754902
4600 done. -210.32227384568827
4800 done. -202.2988946181517
5000 done. -195.09511982440412
5200 done. -188.31771851700452
5400 done. -182.1673436715517
5600 done. -176.43952211261197
Negative Log-likehood -174.520021
2022-12-29 05:03:40,429 - 11:54:23 - 0.0s - INFO - root - log-likelihood:   -174.520
log-likelihood:   -174.520
2022-12-29 05:04:27,518 - 11:55:10 - 47.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-041-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 05:04:27,521 - 11:55:10 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-041-test-greedy.txt
Generation: 188 batches
2022-12-29 05:04:33,215 - 11:55:16 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:04:34,744 - 11:55:17 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:04:34,744 - 11:55:17 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:04:37,238 - 11:55:20 - 2.5s - INFO - root - --- bleu: BLEU = 29.59, 51.4/30.8/25.3/22.5 (BP=0.961, ratio=0.961, hyp_len=163201, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.59, 51.4/30.8/25.3/22.5 (BP=0.961, ratio=0.961, hyp_len=163201, ref_len=169777)
--- bleu: BLEU = 29.59, 51.4/30.8/25.3/22.5 (BP=0.961, ratio=0.961, hyp_len=163201, ref_len=169777)

2022-12-29 05:04:37,238 - 11:55:20 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:04:37,240 - 11:55:20 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:04:46,849 - 11:55:30 - 9.6s - INFO - root - batch/max_batch/ep:   100/   529/    42 rec:   13.042 mi: 1.95028389 zkl:   58.582 cd:    6.083 pos_prob:   76.146 prob_neg:   70.063 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    42 rec:   13.042 mi: 1.95028389 zkl:   58.582 cd:    6.083 pos_prob:   76.146 prob_neg:   70.063 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:04:56,955 - 11:55:40 - 10.1s - INFO - root - batch/max_batch/ep:   200/   529/    42 rec:   16.557 mi: 2.03945899 zkl:   64.415 cd:   -3.954 pos_prob:   69.559 prob_neg:   73.514 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    42 rec:   16.557 mi: 2.03945899 zkl:   64.415 cd:   -3.954 pos_prob:   69.559 prob_neg:   73.514 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:05:06,421 - 11:55:49 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    42 rec:   19.119 mi: 1.94695401 zkl:   64.581 cd:    1.856 pos_prob:   74.320 prob_neg:   72.463 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    42 rec:   19.119 mi: 1.94695401 zkl:   64.581 cd:    1.856 pos_prob:   74.320 prob_neg:   72.463 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:05:15,893 - 11:55:59 - 9.5s - INFO - root - batch/max_batch/ep:   400/   529/    42 rec:   20.435 mi: 2.01481867 zkl:   68.445 cd:    2.539 pos_prob:   74.660 prob_neg:   72.121 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    42 rec:   20.435 mi: 2.01481867 zkl:   68.445 cd:    2.539 pos_prob:   74.660 prob_neg:   72.121 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:05:25,288 - 11:56:08 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    42 rec:   14.307 mi: 1.98113179 zkl:   69.894 cd:    7.197 pos_prob:   83.866 prob_neg:   76.669 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    42 rec:   14.307 mi: 1.98113179 zkl:   69.894 cd:    7.197 pos_prob:   83.866 prob_neg:   76.669 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:05:25,300 - 11:56:08 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-282.210
Langevin prior   1/ 40: energy=-282.210
2022-12-29 05:05:25,306 - 11:56:08 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1146.969
Langevin prior   6/ 40: energy=-1146.969
2022-12-29 05:05:25,311 - 11:56:08 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1727.798
Langevin prior  11/ 40: energy=-1727.798
2022-12-29 05:05:25,317 - 11:56:08 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1965.905
Langevin prior  16/ 40: energy=-1965.905
2022-12-29 05:05:25,323 - 11:56:08 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2054.777
Langevin prior  21/ 40: energy=-2054.777
2022-12-29 05:05:25,331 - 11:56:08 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2185.141
Langevin prior  26/ 40: energy=-2185.141
2022-12-29 05:05:25,339 - 11:56:08 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2217.557
Langevin prior  31/ 40: energy=-2217.557
2022-12-29 05:05:25,348 - 11:56:08 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2188.228
Langevin prior  36/ 40: energy=-2188.228
2022-12-29 05:05:25,356 - 11:56:08 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2165.397
Langevin prior  40/ 40: energy=-2165.397
2022-12-29 05:15:10,022 - 12:05:53 - 584.7s - INFO - root - Negative Log-likehood -173.280881
200 done. -151.42984653552347
400 done. -145.7602977808258
600 done. -145.4428562299257
800 done. -146.24595143578793
1000 done. -145.09398204704468
1200 done. -145.89748823735704
1400 done. -147.02854792561968
1600 done. -145.22357401295932
1800 done. -144.62554173240449
2000 done. -161.60389126212297
2200 done. -172.6187587374513
2400 done. -184.27605595830457
2600 done. -194.64809388028723
2800 done. -201.43775995508713
3000 done. -210.52455927322237
3200 done. -215.91314446117207
3400 done. -222.95142976594485
3600 done. -227.4976987927337
3800 done. -232.73303691900608
4000 done. -236.6683376035734
4200 done. -227.2995879912818
4400 done. -217.63955652145935
4600 done. -209.00027825883416
4800 done. -200.9936840102889
5000 done. -193.80324232120017
5200 done. -187.06144762890287
5400 done. -180.90426114663924
5600 done. -175.188219214472
Negative Log-likehood -173.280881
2022-12-29 05:15:10,022 - 12:05:53 - 0.0s - INFO - root - log-likelihood:   -173.281
log-likelihood:   -173.281
2022-12-29 05:15:57,280 - 12:06:40 - 47.3s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-042-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 05:15:57,282 - 12:06:40 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-042-test-greedy.txt
Generation: 188 batches
2022-12-29 05:16:02,928 - 12:06:46 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:16:04,443 - 12:06:47 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:16:04,443 - 12:06:47 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:16:06,884 - 12:06:50 - 2.4s - INFO - root - --- bleu: BLEU = 30.02, 52.4/31.9/26.2/23.2 (BP=0.946, ratio=0.947, hyp_len=160800, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.02, 52.4/31.9/26.2/23.2 (BP=0.946, ratio=0.947, hyp_len=160800, ref_len=169777)
--- bleu: BLEU = 30.02, 52.4/31.9/26.2/23.2 (BP=0.946, ratio=0.947, hyp_len=160800, ref_len=169777)

2022-12-29 05:16:06,885 - 12:06:50 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:16:06,886 - 12:06:50 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:16:16,182 - 12:06:59 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    43 rec:    6.989 mi: 1.94095314 zkl:  157.217 cd:  -10.468 pos_prob:   71.812 prob_neg:   82.280 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    43 rec:    6.989 mi: 1.94095314 zkl:  157.217 cd:  -10.468 pos_prob:   71.812 prob_neg:   82.280 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:16:25,503 - 12:07:08 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    43 rec:   14.911 mi: 2.00585628 zkl:  155.534 cd:  -21.485 pos_prob:   53.719 prob_neg:   75.204 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    43 rec:   14.911 mi: 2.00585628 zkl:  155.534 cd:  -21.485 pos_prob:   53.719 prob_neg:   75.204 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:16:34,873 - 12:07:18 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    43 rec:   11.230 mi: 2.10802937 zkl:  155.770 cd:  -23.045 pos_prob:   54.540 prob_neg:   77.585 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    43 rec:   11.230 mi: 2.10802937 zkl:  155.770 cd:  -23.045 pos_prob:   54.540 prob_neg:   77.585 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:16:43,949 - 12:07:27 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    43 rec:   13.675 mi: 2.07440281 zkl:  158.781 cd:  -20.370 pos_prob:   50.251 prob_neg:   70.621 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    43 rec:   13.675 mi: 2.07440281 zkl:  158.781 cd:  -20.370 pos_prob:   50.251 prob_neg:   70.621 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:16:53,196 - 12:07:36 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    43 rec:   11.322 mi: 2.02252698 zkl:  156.451 cd:  -28.714 pos_prob:   46.845 prob_neg:   75.559 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    43 rec:   11.322 mi: 2.02252698 zkl:  156.451 cd:  -28.714 pos_prob:   46.845 prob_neg:   75.559 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:16:53,208 - 12:07:36 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-266.066
Langevin prior   1/ 40: energy=-266.066
2022-12-29 05:16:53,214 - 12:07:36 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1080.956
Langevin prior   6/ 40: energy=-1080.956
2022-12-29 05:16:53,219 - 12:07:36 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1633.687
Langevin prior  11/ 40: energy=-1633.687
2022-12-29 05:16:53,225 - 12:07:36 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2009.151
Langevin prior  16/ 40: energy=-2009.151
2022-12-29 05:16:53,232 - 12:07:36 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2163.985
Langevin prior  21/ 40: energy=-2163.985
2022-12-29 05:16:53,240 - 12:07:36 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2244.950
Langevin prior  26/ 40: energy=-2244.950
2022-12-29 05:16:53,249 - 12:07:36 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2305.187
Langevin prior  31/ 40: energy=-2305.187
2022-12-29 05:16:53,258 - 12:07:36 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2306.174
Langevin prior  36/ 40: energy=-2306.174
2022-12-29 05:16:53,266 - 12:07:36 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2344.586
Langevin prior  40/ 40: energy=-2344.586
2022-12-29 05:16:55,852 - 12:07:39 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-043-test-greedy.txt
Generation: 188 batches
2022-12-29 05:17:01,499 - 12:07:44 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:17:03,013 - 12:07:46 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:17:03,014 - 12:07:46 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:17:05,463 - 12:07:48 - 2.4s - INFO - root - --- bleu: BLEU = 31.11, 53.8/32.8/26.8/23.8 (BP=0.955, ratio=0.956, hyp_len=162356, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.11, 53.8/32.8/26.8/23.8 (BP=0.955, ratio=0.956, hyp_len=162356, ref_len=169777)
--- bleu: BLEU = 31.11, 53.8/32.8/26.8/23.8 (BP=0.955, ratio=0.956, hyp_len=162356, ref_len=169777)

2022-12-29 05:17:05,463 - 12:07:48 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:17:05,465 - 12:07:48 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:17:14,623 - 12:07:57 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    44 rec:   11.339 mi: 1.92761850 zkl:  156.882 cd:  -27.991 pos_prob:   50.667 prob_neg:   78.658 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    44 rec:   11.339 mi: 1.92761850 zkl:  156.882 cd:  -27.991 pos_prob:   50.667 prob_neg:   78.658 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:17:23,874 - 12:08:07 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    44 rec:   12.594 mi: 1.87675953 zkl:  157.561 cd:  -32.435 pos_prob:   41.881 prob_neg:   74.316 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    44 rec:   12.594 mi: 1.87675953 zkl:  157.561 cd:  -32.435 pos_prob:   41.881 prob_neg:   74.316 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:17:33,055 - 12:08:16 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    44 rec:    9.894 mi: 2.04670310 zkl:  157.409 cd:  -34.308 pos_prob:   41.332 prob_neg:   75.640 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    44 rec:    9.894 mi: 2.04670310 zkl:  157.409 cd:  -34.308 pos_prob:   41.332 prob_neg:   75.640 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:17:42,172 - 12:08:25 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    44 rec:   14.968 mi: 2.07704020 zkl:  159.274 cd:  -35.362 pos_prob:   41.402 prob_neg:   76.765 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    44 rec:   14.968 mi: 2.07704020 zkl:  159.274 cd:  -35.362 pos_prob:   41.402 prob_neg:   76.765 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:17:51,517 - 12:08:34 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    44 rec:   10.816 mi: 2.16201830 zkl:  161.767 cd:  -31.071 pos_prob:   42.874 prob_neg:   73.945 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    44 rec:   10.816 mi: 2.16201830 zkl:  161.767 cd:  -31.071 pos_prob:   42.874 prob_neg:   73.945 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:17:51,530 - 12:08:34 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-314.717
Langevin prior   1/ 40: energy=-314.717
2022-12-29 05:17:51,535 - 12:08:34 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1065.165
Langevin prior   6/ 40: energy=-1065.165
2022-12-29 05:17:51,541 - 12:08:34 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1688.850
Langevin prior  11/ 40: energy=-1688.850
2022-12-29 05:17:51,547 - 12:08:34 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1956.368
Langevin prior  16/ 40: energy=-1956.368
2022-12-29 05:17:51,553 - 12:08:34 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2093.010
Langevin prior  21/ 40: energy=-2093.010
2022-12-29 05:17:51,561 - 12:08:34 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2097.380
Langevin prior  26/ 40: energy=-2097.380
2022-12-29 05:17:51,569 - 12:08:34 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2049.789
Langevin prior  31/ 40: energy=-2049.789
2022-12-29 05:17:51,579 - 12:08:34 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2142.598
Langevin prior  36/ 40: energy=-2142.598
2022-12-29 05:17:51,587 - 12:08:34 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2191.047
Langevin prior  40/ 40: energy=-2191.047
2022-12-29 05:17:54,210 - 12:08:37 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-044-test-greedy.txt
Generation: 188 batches
2022-12-29 05:17:59,851 - 12:08:43 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:18:01,362 - 12:08:44 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:18:01,362 - 12:08:44 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:18:03,790 - 12:08:46 - 2.4s - INFO - root - --- bleu: BLEU = 31.28, 54.3/33.2/27.3/24.3 (BP=0.947, ratio=0.948, hyp_len=160937, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.28, 54.3/33.2/27.3/24.3 (BP=0.947, ratio=0.948, hyp_len=160937, ref_len=169777)
--- bleu: BLEU = 31.28, 54.3/33.2/27.3/24.3 (BP=0.947, ratio=0.948, hyp_len=160937, ref_len=169777)

2022-12-29 05:18:03,791 - 12:08:46 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:18:03,792 - 12:08:46 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:18:13,052 - 12:08:56 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    45 rec:    7.367 mi: 1.99294019 zkl:  162.844 cd:  -28.623 pos_prob:   43.110 prob_neg:   71.733 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    45 rec:    7.367 mi: 1.99294019 zkl:  162.844 cd:  -28.623 pos_prob:   43.110 prob_neg:   71.733 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:18:22,234 - 12:09:05 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    45 rec:    7.984 mi: 1.88237774 zkl:  164.427 cd:  -29.948 pos_prob:   46.308 prob_neg:   76.256 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    45 rec:    7.984 mi: 1.88237774 zkl:  164.427 cd:  -29.948 pos_prob:   46.308 prob_neg:   76.256 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:18:31,523 - 12:09:14 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    45 rec:    7.428 mi: 1.98255181 zkl:  161.362 cd:  -37.113 pos_prob:   38.551 prob_neg:   75.664 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    45 rec:    7.428 mi: 1.98255181 zkl:  161.362 cd:  -37.113 pos_prob:   38.551 prob_neg:   75.664 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:18:40,564 - 12:09:23 - 9.0s - INFO - root - batch/max_batch/ep:   400/   529/    45 rec:   11.982 mi: 1.90135813 zkl:  162.980 cd:  -35.062 pos_prob:   40.638 prob_neg:   75.699 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    45 rec:   11.982 mi: 1.90135813 zkl:  162.980 cd:  -35.062 pos_prob:   40.638 prob_neg:   75.699 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:18:49,667 - 12:09:32 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    45 rec:    9.705 mi: 1.69669497 zkl:  163.919 cd:  -43.667 pos_prob:   34.224 prob_neg:   77.892 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    45 rec:    9.705 mi: 1.69669497 zkl:  163.919 cd:  -43.667 pos_prob:   34.224 prob_neg:   77.892 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:18:49,680 - 12:09:32 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-303.855
Langevin prior   1/ 40: energy=-303.855
2022-12-29 05:18:49,686 - 12:09:32 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1205.294
Langevin prior   6/ 40: energy=-1205.294
2022-12-29 05:18:49,691 - 12:09:32 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1780.173
Langevin prior  11/ 40: energy=-1780.173
2022-12-29 05:18:49,697 - 12:09:32 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2034.664
Langevin prior  16/ 40: energy=-2034.664
2022-12-29 05:18:49,702 - 12:09:32 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2212.336
Langevin prior  21/ 40: energy=-2212.336
2022-12-29 05:18:49,708 - 12:09:32 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2219.626
Langevin prior  26/ 40: energy=-2219.626
2022-12-29 05:18:49,716 - 12:09:32 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2232.527
Langevin prior  31/ 40: energy=-2232.527
2022-12-29 05:18:49,725 - 12:09:32 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2288.696
Langevin prior  36/ 40: energy=-2288.696
2022-12-29 05:18:49,732 - 12:09:32 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2279.105
Langevin prior  40/ 40: energy=-2279.105
2022-12-29 05:18:52,298 - 12:09:35 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-045-test-greedy.txt
Generation: 188 batches
2022-12-29 05:18:57,936 - 12:09:41 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:18:59,702 - 12:09:42 - 1.8s - INFO - root - Generation Done
Generation Done
2022-12-29 05:18:59,702 - 12:09:42 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:19:02,184 - 12:09:45 - 2.5s - INFO - root - --- bleu: BLEU = 31.18, 54.0/32.9/27.0/24.0 (BP=0.951, ratio=0.952, hyp_len=161685, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.18, 54.0/32.9/27.0/24.0 (BP=0.951, ratio=0.952, hyp_len=161685, ref_len=169777)
--- bleu: BLEU = 31.18, 54.0/32.9/27.0/24.0 (BP=0.951, ratio=0.952, hyp_len=161685, ref_len=169777)

2022-12-29 05:19:02,184 - 12:09:45 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:19:02,186 - 12:09:45 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:19:11,563 - 12:09:54 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    46 rec:   12.923 mi: 1.87267017 zkl:   95.688 cd:  -12.570 pos_prob:   56.479 prob_neg:   69.049 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    46 rec:   12.923 mi: 1.87267017 zkl:   95.688 cd:  -12.570 pos_prob:   56.479 prob_neg:   69.049 kl_weight:    0.062 do_ae_train: False
2022-12-29 05:19:20,851 - 12:10:04 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    46 rec:   11.033 mi: 1.96025562 zkl:   95.475 cd:   -3.595 pos_prob:   66.162 prob_neg:   69.757 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    46 rec:   11.033 mi: 1.96025562 zkl:   95.475 cd:   -3.595 pos_prob:   66.162 prob_neg:   69.757 kl_weight:    0.125 do_ae_train: False
2022-12-29 05:19:30,324 - 12:10:13 - 9.5s - INFO - root - batch/max_batch/ep:   300/   529/    46 rec:   13.598 mi: 1.99703646 zkl:   80.414 cd:   -4.650 pos_prob:   71.898 prob_neg:   76.548 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    46 rec:   13.598 mi: 1.99703646 zkl:   80.414 cd:   -4.650 pos_prob:   71.898 prob_neg:   76.548 kl_weight:    0.188 do_ae_train: False
2022-12-29 05:19:39,539 - 12:10:22 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    46 rec:   13.161 mi: 1.92072225 zkl:   77.846 cd:    2.199 pos_prob:   70.429 prob_neg:   68.230 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    46 rec:   13.161 mi: 1.92072225 zkl:   77.846 cd:    2.199 pos_prob:   70.429 prob_neg:   68.230 kl_weight:    0.251 do_ae_train: False
2022-12-29 05:19:48,807 - 12:10:32 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    46 rec:   16.014 mi: 2.01088905 zkl:   68.882 cd:   -8.929 pos_prob:   58.882 prob_neg:   67.811 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    46 rec:   16.014 mi: 2.01088905 zkl:   68.882 cd:   -8.929 pos_prob:   58.882 prob_neg:   67.811 kl_weight:    0.314 do_ae_train: False
2022-12-29 05:19:48,820 - 12:10:32 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-294.078
Langevin prior   1/ 40: energy=-294.078
2022-12-29 05:19:48,826 - 12:10:32 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1075.316
Langevin prior   6/ 40: energy=-1075.316
2022-12-29 05:19:48,831 - 12:10:32 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1512.323
Langevin prior  11/ 40: energy=-1512.323
2022-12-29 05:19:48,836 - 12:10:32 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-1849.826
Langevin prior  16/ 40: energy=-1849.826
2022-12-29 05:19:48,842 - 12:10:32 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-1963.729
Langevin prior  21/ 40: energy=-1963.729
2022-12-29 05:19:48,848 - 12:10:32 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-1990.966
Langevin prior  26/ 40: energy=-1990.966
2022-12-29 05:19:48,855 - 12:10:32 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2011.374
Langevin prior  31/ 40: energy=-2011.374
2022-12-29 05:19:48,863 - 12:10:32 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-1973.470
Langevin prior  36/ 40: energy=-1973.470
2022-12-29 05:19:48,870 - 12:10:32 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-1926.582
Langevin prior  40/ 40: energy=-1926.582
2022-12-29 05:19:51,516 - 12:10:34 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-046-test-greedy.txt
Generation: 188 batches
2022-12-29 05:19:57,159 - 12:10:40 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:19:58,680 - 12:10:41 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:19:58,681 - 12:10:41 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:20:01,142 - 12:10:44 - 2.5s - INFO - root - --- bleu: BLEU = 30.57, 52.8/32.1/26.4/23.5 (BP=0.955, ratio=0.956, hyp_len=162312, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.57, 52.8/32.1/26.4/23.5 (BP=0.955, ratio=0.956, hyp_len=162312, ref_len=169777)
--- bleu: BLEU = 30.57, 52.8/32.1/26.4/23.5 (BP=0.955, ratio=0.956, hyp_len=162312, ref_len=169777)

2022-12-29 05:20:01,143 - 12:10:44 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:20:01,144 - 12:10:44 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:20:10,563 - 12:10:53 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    47 rec:   14.520 mi: 2.12268567 zkl:   59.554 cd:   -6.415 pos_prob:   64.018 prob_neg:   70.433 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    47 rec:   14.520 mi: 2.12268567 zkl:   59.554 cd:   -6.415 pos_prob:   64.018 prob_neg:   70.433 kl_weight:    0.396 do_ae_train: False
2022-12-29 05:20:19,930 - 12:11:03 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    47 rec:   11.576 mi: 1.96389604 zkl:   63.355 cd:   -2.976 pos_prob:   70.336 prob_neg:   73.312 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    47 rec:   11.576 mi: 1.96389604 zkl:   63.355 cd:   -2.976 pos_prob:   70.336 prob_neg:   73.312 kl_weight:    0.459 do_ae_train: False
2022-12-29 05:20:29,181 - 12:11:12 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    47 rec:   15.435 mi: 2.22799349 zkl:   63.717 cd:   -2.603 pos_prob:   67.926 prob_neg:   70.529 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    47 rec:   15.435 mi: 2.22799349 zkl:   63.717 cd:   -2.603 pos_prob:   67.926 prob_neg:   70.529 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:20:38,586 - 12:11:21 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    47 rec:   15.853 mi: 2.03759503 zkl:   66.803 cd:   -5.647 pos_prob:   76.191 prob_neg:   81.837 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    47 rec:   15.853 mi: 2.03759503 zkl:   66.803 cd:   -5.647 pos_prob:   76.191 prob_neg:   81.837 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:20:48,006 - 12:11:31 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    47 rec:   14.984 mi: 2.03076291 zkl:   71.070 cd:    1.284 pos_prob:   84.007 prob_neg:   82.723 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    47 rec:   14.984 mi: 2.03076291 zkl:   71.070 cd:    1.284 pos_prob:   84.007 prob_neg:   82.723 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:20:48,019 - 12:11:31 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-341.922
Langevin prior   1/ 40: energy=-341.922
2022-12-29 05:20:48,024 - 12:11:31 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1339.918
Langevin prior   6/ 40: energy=-1339.918
2022-12-29 05:20:48,029 - 12:11:31 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2037.812
Langevin prior  11/ 40: energy=-2037.812
2022-12-29 05:20:48,034 - 12:11:31 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2349.201
Langevin prior  16/ 40: energy=-2349.201
2022-12-29 05:20:48,039 - 12:11:31 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2523.160
Langevin prior  21/ 40: energy=-2523.160
2022-12-29 05:20:48,045 - 12:11:31 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2520.935
Langevin prior  26/ 40: energy=-2520.935
2022-12-29 05:20:48,050 - 12:11:31 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2521.359
Langevin prior  31/ 40: energy=-2521.359
2022-12-29 05:20:48,058 - 12:11:31 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2608.270
Langevin prior  36/ 40: energy=-2608.270
2022-12-29 05:20:48,064 - 12:11:31 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2580.992
Langevin prior  40/ 40: energy=-2580.992
2022-12-29 05:30:32,811 - 12:21:16 - 584.7s - INFO - root - Negative Log-likehood -180.951960
200 done. -158.62626696964378
400 done. -153.11399323485185
600 done. -153.13943802245907
800 done. -153.9469377222944
1000 done. -152.84860115353436
1200 done. -153.6712047641673
1400 done. -154.7587965776501
1600 done. -152.93441848208144
1800 done. -152.3287168536952
2000 done. -169.83931631694168
2200 done. -180.88754178307573
2400 done. -192.77228969085277
2600 done. -203.64175805343987
2800 done. -210.66949096478658
3000 done. -219.9928592349277
3200 done. -225.50346490521542
3400 done. -232.5199227963582
3600 done. -236.94074380444823
3800 done. -242.41688064026366
4000 done. -246.2592474042942
4200 done. -236.5726124267218
4400 done. -226.60695226405252
4600 done. -217.7244197085519
4800 done. -209.48231890181495
5000 done. -202.082487236403
5200 done. -195.13000415353193
5400 done. -188.79853259186706
5600 done. -182.91514615606678
Negative Log-likehood -180.951960
2022-12-29 05:30:32,811 - 12:21:16 - 0.0s - INFO - root - log-likelihood:   -180.952
log-likelihood:   -180.952
2022-12-29 05:31:19,911 - 12:22:03 - 47.1s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-047-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 05:31:19,914 - 12:22:03 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-047-test-greedy.txt
Generation: 188 batches
2022-12-29 05:31:25,569 - 12:22:08 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:31:27,111 - 12:22:10 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:31:27,111 - 12:22:10 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:31:29,580 - 12:22:12 - 2.5s - INFO - root - --- bleu: BLEU = 30.15, 51.3/30.8/25.2/22.3 (BP=0.982, ratio=0.982, hyp_len=166679, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.15, 51.3/30.8/25.2/22.3 (BP=0.982, ratio=0.982, hyp_len=166679, ref_len=169777)
--- bleu: BLEU = 30.15, 51.3/30.8/25.2/22.3 (BP=0.982, ratio=0.982, hyp_len=166679, ref_len=169777)

2022-12-29 05:31:29,580 - 12:22:12 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:31:29,582 - 12:22:12 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:31:38,857 - 12:22:22 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    48 rec:   13.695 mi: 2.08388019 zkl:   69.988 cd:    1.975 pos_prob:   84.943 prob_neg:   82.968 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    48 rec:   13.695 mi: 2.08388019 zkl:   69.988 cd:    1.975 pos_prob:   84.943 prob_neg:   82.968 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:31:48,464 - 12:22:31 - 9.6s - INFO - root - batch/max_batch/ep:   200/   529/    48 rec:   13.371 mi: 2.17900229 zkl:   67.312 cd:   -4.933 pos_prob:   78.435 prob_neg:   83.369 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    48 rec:   13.371 mi: 2.17900229 zkl:   67.312 cd:   -4.933 pos_prob:   78.435 prob_neg:   83.369 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:31:57,774 - 12:22:40 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    48 rec:   12.476 mi: 2.10122371 zkl:   62.971 cd:    5.818 pos_prob:   86.689 prob_neg:   80.870 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    48 rec:   12.476 mi: 2.10122371 zkl:   62.971 cd:    5.818 pos_prob:   86.689 prob_neg:   80.870 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:32:07,098 - 12:22:50 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    48 rec:   17.954 mi: 2.23733377 zkl:   69.372 cd:   -4.668 pos_prob:   80.746 prob_neg:   85.414 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    48 rec:   17.954 mi: 2.23733377 zkl:   69.372 cd:   -4.668 pos_prob:   80.746 prob_neg:   85.414 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:32:16,489 - 12:22:59 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    48 rec:   12.649 mi: 2.05384898 zkl:   66.777 cd:    7.509 pos_prob:   88.231 prob_neg:   80.722 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    48 rec:   12.649 mi: 2.05384898 zkl:   66.777 cd:    7.509 pos_prob:   88.231 prob_neg:   80.722 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:32:16,501 - 12:22:59 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-331.029
Langevin prior   1/ 40: energy=-331.029
2022-12-29 05:32:16,507 - 12:22:59 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1387.394
Langevin prior   6/ 40: energy=-1387.394
2022-12-29 05:32:16,512 - 12:22:59 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2007.596
Langevin prior  11/ 40: energy=-2007.596
2022-12-29 05:32:16,517 - 12:22:59 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2173.962
Langevin prior  16/ 40: energy=-2173.962
2022-12-29 05:32:16,524 - 12:22:59 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2375.083
Langevin prior  21/ 40: energy=-2375.083
2022-12-29 05:32:16,531 - 12:22:59 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2518.051
Langevin prior  26/ 40: energy=-2518.051
2022-12-29 05:32:16,540 - 12:22:59 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2596.979
Langevin prior  31/ 40: energy=-2596.979
2022-12-29 05:32:16,549 - 12:22:59 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2543.328
Langevin prior  36/ 40: energy=-2543.328
2022-12-29 05:32:16,557 - 12:22:59 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2506.637
Langevin prior  40/ 40: energy=-2506.637
2022-12-29 05:42:01,905 - 12:32:45 - 585.3s - INFO - root - Negative Log-likehood -181.728019
200 done. -158.7558070823718
400 done. -152.9547648445707
600 done. -152.77600184632283
800 done. -153.7431007151394
1000 done. -152.67122776098915
1200 done. -153.39824649664948
1400 done. -154.56943971886218
1600 done. -152.78283288580715
1800 done. -152.211439052261
2000 done. -169.74379747686706
2200 done. -180.86433377340424
2400 done. -192.9148704412
2600 done. -204.0212384112917
2800 done. -211.17208923297306
3000 done. -220.5664641524007
3200 done. -226.0872682868403
3400 done. -233.14284707757278
3600 done. -237.65894502780267
3800 done. -243.1462341259999
4000 done. -247.13163649464386
4200 done. -237.46314029848196
4400 done. -227.48204492768215
4600 done. -218.5788313543008
4800 done. -210.3232938017712
5000 done. -202.90095999510643
5200 done. -195.9286397780795
5400 done. -189.5826598543452
5600 done. -183.69279780374933
Negative Log-likehood -181.728019
2022-12-29 05:42:01,905 - 12:32:45 - 0.0s - INFO - root - log-likelihood:   -181.728
log-likelihood:   -181.728
2022-12-29 05:42:48,752 - 12:33:31 - 46.8s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-048-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 05:42:48,754 - 12:33:31 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-048-test-greedy.txt
Generation: 188 batches
2022-12-29 05:42:54,395 - 12:33:37 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:42:55,930 - 12:33:39 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:42:55,930 - 12:33:39 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:42:58,367 - 12:33:41 - 2.4s - INFO - root - --- bleu: BLEU = 29.62, 52.0/31.4/25.7/22.8 (BP=0.947, ratio=0.948, hyp_len=160939, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.62, 52.0/31.4/25.7/22.8 (BP=0.947, ratio=0.948, hyp_len=160939, ref_len=169777)
--- bleu: BLEU = 29.62, 52.0/31.4/25.7/22.8 (BP=0.947, ratio=0.948, hyp_len=160939, ref_len=169777)

2022-12-29 05:42:58,367 - 12:33:41 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:42:58,369 - 12:33:41 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:43:07,441 - 12:33:50 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    49 rec:    7.822 mi: 2.11736417 zkl:  158.009 cd:  -14.283 pos_prob:   69.499 prob_neg:   83.783 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    49 rec:    7.822 mi: 2.11736417 zkl:  158.009 cd:  -14.283 pos_prob:   69.499 prob_neg:   83.783 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:43:16,775 - 12:33:59 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    49 rec:   10.370 mi: 2.26377678 zkl:  160.839 cd:  -20.704 pos_prob:   62.696 prob_neg:   83.400 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    49 rec:   10.370 mi: 2.26377678 zkl:  160.839 cd:  -20.704 pos_prob:   62.696 prob_neg:   83.400 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:43:25,903 - 12:34:09 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    49 rec:    9.764 mi: 1.98939633 zkl:  156.265 cd:  -28.413 pos_prob:   55.949 prob_neg:   84.362 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    49 rec:    9.764 mi: 1.98939633 zkl:  156.265 cd:  -28.413 pos_prob:   55.949 prob_neg:   84.362 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:43:35,510 - 12:34:18 - 9.6s - INFO - root - batch/max_batch/ep:   400/   529/    49 rec:   14.470 mi: 2.34854650 zkl:  159.546 cd:  -31.860 pos_prob:   50.728 prob_neg:   82.588 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    49 rec:   14.470 mi: 2.34854650 zkl:  159.546 cd:  -31.860 pos_prob:   50.728 prob_neg:   82.588 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:43:44,925 - 12:34:28 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    49 rec:   12.734 mi: 2.10847449 zkl:  160.295 cd:  -29.814 pos_prob:   50.852 prob_neg:   80.667 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    49 rec:   12.734 mi: 2.10847449 zkl:  160.295 cd:  -29.814 pos_prob:   50.852 prob_neg:   80.667 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:43:44,938 - 12:34:28 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-305.085
Langevin prior   1/ 40: energy=-305.085
2022-12-29 05:43:44,943 - 12:34:28 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1245.234
Langevin prior   6/ 40: energy=-1245.234
2022-12-29 05:43:44,948 - 12:34:28 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1803.736
Langevin prior  11/ 40: energy=-1803.736
2022-12-29 05:43:44,954 - 12:34:28 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2106.473
Langevin prior  16/ 40: energy=-2106.473
2022-12-29 05:43:44,959 - 12:34:28 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2179.332
Langevin prior  21/ 40: energy=-2179.332
2022-12-29 05:43:44,965 - 12:34:28 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2315.222
Langevin prior  26/ 40: energy=-2315.222
2022-12-29 05:43:44,973 - 12:34:28 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2369.625
Langevin prior  31/ 40: energy=-2369.625
2022-12-29 05:43:44,981 - 12:34:28 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2393.320
Langevin prior  36/ 40: energy=-2393.320
2022-12-29 05:43:44,988 - 12:34:28 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2397.766
Langevin prior  40/ 40: energy=-2397.766
2022-12-29 05:43:47,685 - 12:34:30 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-049-test-greedy.txt
Generation: 188 batches
2022-12-29 05:43:53,337 - 12:34:36 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:43:54,853 - 12:34:38 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:43:54,853 - 12:34:38 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:43:57,316 - 12:34:40 - 2.5s - INFO - root - --- bleu: BLEU = 31.27, 53.4/32.8/26.9/23.8 (BP=0.961, ratio=0.962, hyp_len=163260, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.27, 53.4/32.8/26.9/23.8 (BP=0.961, ratio=0.962, hyp_len=163260, ref_len=169777)
--- bleu: BLEU = 31.27, 53.4/32.8/26.9/23.8 (BP=0.961, ratio=0.962, hyp_len=163260, ref_len=169777)

2022-12-29 05:43:57,316 - 12:34:40 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:43:57,318 - 12:34:40 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:44:06,396 - 12:34:49 - 9.1s - INFO - root - batch/max_batch/ep:   100/   529/    50 rec:    8.267 mi: 2.11973858 zkl:  162.889 cd:  -30.392 pos_prob:   54.044 prob_neg:   84.436 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    50 rec:    8.267 mi: 2.11973858 zkl:  162.889 cd:  -30.392 pos_prob:   54.044 prob_neg:   84.436 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:44:15,664 - 12:34:58 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    50 rec:   12.396 mi: 2.19981670 zkl:  162.608 cd:  -35.868 pos_prob:   50.643 prob_neg:   86.511 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    50 rec:   12.396 mi: 2.19981670 zkl:  162.608 cd:  -35.868 pos_prob:   50.643 prob_neg:   86.511 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:44:24,711 - 12:35:07 - 9.0s - INFO - root - batch/max_batch/ep:   300/   529/    50 rec:    6.135 mi: 1.94259501 zkl:  163.657 cd:  -21.681 pos_prob:   59.265 prob_neg:   80.945 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    50 rec:    6.135 mi: 1.94259501 zkl:  163.657 cd:  -21.681 pos_prob:   59.265 prob_neg:   80.945 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:44:34,059 - 12:35:17 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    50 rec:   14.652 mi: 2.00615048 zkl:  161.231 cd:  -41.313 pos_prob:   42.396 prob_neg:   83.709 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    50 rec:   14.652 mi: 2.00615048 zkl:  161.231 cd:  -41.313 pos_prob:   42.396 prob_neg:   83.709 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:44:43,384 - 12:35:26 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    50 rec:   10.351 mi: 2.08847523 zkl:  163.391 cd:  -33.058 pos_prob:   48.797 prob_neg:   81.855 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    50 rec:   10.351 mi: 2.08847523 zkl:  163.391 cd:  -33.058 pos_prob:   48.797 prob_neg:   81.855 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:44:43,400 - 12:35:26 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-296.213
Langevin prior   1/ 40: energy=-296.213
2022-12-29 05:44:43,409 - 12:35:26 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1363.986
Langevin prior   6/ 40: energy=-1363.986
2022-12-29 05:44:43,419 - 12:35:26 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1991.479
Langevin prior  11/ 40: energy=-1991.479
2022-12-29 05:44:43,430 - 12:35:26 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2323.352
Langevin prior  16/ 40: energy=-2323.352
2022-12-29 05:44:43,441 - 12:35:26 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2452.854
Langevin prior  21/ 40: energy=-2452.854
2022-12-29 05:44:43,453 - 12:35:26 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2516.285
Langevin prior  26/ 40: energy=-2516.285
2022-12-29 05:44:43,466 - 12:35:26 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2574.321
Langevin prior  31/ 40: energy=-2574.321
2022-12-29 05:44:43,478 - 12:35:26 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2599.589
Langevin prior  36/ 40: energy=-2599.589
2022-12-29 05:44:43,489 - 12:35:26 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2500.622
Langevin prior  40/ 40: energy=-2500.622
2022-12-29 05:44:46,100 - 12:35:29 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-050-test-greedy.txt
Generation: 188 batches
2022-12-29 05:44:51,747 - 12:35:34 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:44:53,243 - 12:35:36 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:44:53,243 - 12:35:36 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:44:55,678 - 12:35:38 - 2.4s - INFO - root - --- bleu: BLEU = 31.29, 54.7/33.5/27.5/24.5 (BP=0.939, ratio=0.941, hyp_len=159750, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.29, 54.7/33.5/27.5/24.5 (BP=0.939, ratio=0.941, hyp_len=159750, ref_len=169777)
--- bleu: BLEU = 31.29, 54.7/33.5/27.5/24.5 (BP=0.939, ratio=0.941, hyp_len=159750, ref_len=169777)

2022-12-29 05:44:55,678 - 12:35:38 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:44:55,680 - 12:35:38 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:45:04,952 - 12:35:48 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    51 rec:   11.760 mi: 1.99067903 zkl:  164.163 cd:  -38.838 pos_prob:   45.415 prob_neg:   84.253 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    51 rec:   11.760 mi: 1.99067903 zkl:  164.163 cd:  -38.838 pos_prob:   45.415 prob_neg:   84.253 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:45:14,240 - 12:35:57 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    51 rec:    6.070 mi: 1.97572696 zkl:  161.809 cd:  -35.120 pos_prob:   46.415 prob_neg:   81.536 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    51 rec:    6.070 mi: 1.97572696 zkl:  161.809 cd:  -35.120 pos_prob:   46.415 prob_neg:   81.536 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:45:23,427 - 12:36:06 - 9.2s - INFO - root - batch/max_batch/ep:   300/   529/    51 rec:   10.424 mi: 1.93010104 zkl:  163.132 cd:  -45.933 pos_prob:   39.873 prob_neg:   85.806 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    51 rec:   10.424 mi: 1.93010104 zkl:  163.132 cd:  -45.933 pos_prob:   39.873 prob_neg:   85.806 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:45:32,657 - 12:36:15 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    51 rec:    7.944 mi: 2.04191113 zkl:  162.216 cd:  -45.188 pos_prob:   44.362 prob_neg:   89.550 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    51 rec:    7.944 mi: 2.04191113 zkl:  162.216 cd:  -45.188 pos_prob:   44.362 prob_neg:   89.550 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:45:41,878 - 12:36:25 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    51 rec:    9.527 mi: 2.20932269 zkl:  164.321 cd:  -53.364 pos_prob:   40.744 prob_neg:   94.108 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    51 rec:    9.527 mi: 2.20932269 zkl:  164.321 cd:  -53.364 pos_prob:   40.744 prob_neg:   94.108 kl_weight:    0.000 do_ae_train: True
2022-12-29 05:45:41,890 - 12:36:25 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-322.077
Langevin prior   1/ 40: energy=-322.077
2022-12-29 05:45:41,895 - 12:36:25 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1300.644
Langevin prior   6/ 40: energy=-1300.644
2022-12-29 05:45:41,902 - 12:36:25 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1898.669
Langevin prior  11/ 40: energy=-1898.669
2022-12-29 05:45:41,910 - 12:36:25 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2252.760
Langevin prior  16/ 40: energy=-2252.760
2022-12-29 05:45:41,918 - 12:36:25 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2389.693
Langevin prior  21/ 40: energy=-2389.693
2022-12-29 05:45:41,927 - 12:36:25 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2487.008
Langevin prior  26/ 40: energy=-2487.008
2022-12-29 05:45:41,936 - 12:36:25 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2510.481
Langevin prior  31/ 40: energy=-2510.481
2022-12-29 05:45:41,948 - 12:36:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2524.007
Langevin prior  36/ 40: energy=-2524.007
2022-12-29 05:45:41,957 - 12:36:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2595.101
Langevin prior  40/ 40: energy=-2595.101
2022-12-29 05:45:44,624 - 12:36:27 - 2.7s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-051-test-greedy.txt
Generation: 188 batches
2022-12-29 05:45:50,257 - 12:36:33 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:45:51,750 - 12:36:34 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:45:51,750 - 12:36:34 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:45:54,159 - 12:36:37 - 2.4s - INFO - root - --- bleu: BLEU = 31.22, 54.7/33.4/27.5/24.4 (BP=0.938, ratio=0.940, hyp_len=159634, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.22, 54.7/33.4/27.5/24.4 (BP=0.938, ratio=0.940, hyp_len=159634, ref_len=169777)
--- bleu: BLEU = 31.22, 54.7/33.4/27.5/24.4 (BP=0.938, ratio=0.940, hyp_len=159634, ref_len=169777)

2022-12-29 05:45:54,160 - 12:36:37 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:45:54,161 - 12:36:37 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:46:03,690 - 12:36:46 - 9.5s - INFO - root - batch/max_batch/ep:   100/   529/    52 rec:   11.306 mi: 2.12111950 zkl:   93.328 cd:  -18.840 pos_prob:   66.620 prob_neg:   85.460 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    52 rec:   11.306 mi: 2.12111950 zkl:   93.328 cd:  -18.840 pos_prob:   66.620 prob_neg:   85.460 kl_weight:    0.062 do_ae_train: False
2022-12-29 05:46:13,186 - 12:36:56 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    52 rec:    9.819 mi: 2.07749176 zkl:   87.862 cd:   -6.788 pos_prob:   74.921 prob_neg:   81.709 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    52 rec:    9.819 mi: 2.07749176 zkl:   87.862 cd:   -6.788 pos_prob:   74.921 prob_neg:   81.709 kl_weight:    0.125 do_ae_train: False
2022-12-29 05:46:22,863 - 12:37:06 - 9.7s - INFO - root - batch/max_batch/ep:   300/   529/    52 rec:   14.777 mi: 2.03580403 zkl:   79.877 cd:  -12.535 pos_prob:   67.697 prob_neg:   80.232 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    52 rec:   14.777 mi: 2.03580403 zkl:   79.877 cd:  -12.535 pos_prob:   67.697 prob_neg:   80.232 kl_weight:    0.188 do_ae_train: False
2022-12-29 05:46:32,291 - 12:37:15 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    52 rec:   12.975 mi: 2.22501063 zkl:   76.815 cd:   -3.159 pos_prob:   76.587 prob_neg:   79.747 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    52 rec:   12.975 mi: 2.22501063 zkl:   76.815 cd:   -3.159 pos_prob:   76.587 prob_neg:   79.747 kl_weight:    0.251 do_ae_train: False
2022-12-29 05:46:41,744 - 12:37:24 - 9.5s - INFO - root - batch/max_batch/ep:   500/   529/    52 rec:   14.270 mi: 2.26758337 zkl:   71.711 cd:   -4.537 pos_prob:   72.464 prob_neg:   77.001 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    52 rec:   14.270 mi: 2.26758337 zkl:   71.711 cd:   -4.537 pos_prob:   72.464 prob_neg:   77.001 kl_weight:    0.314 do_ae_train: False
2022-12-29 05:46:41,756 - 12:37:24 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-297.509
Langevin prior   1/ 40: energy=-297.509
2022-12-29 05:46:41,762 - 12:37:24 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1211.528
Langevin prior   6/ 40: energy=-1211.528
2022-12-29 05:46:41,767 - 12:37:24 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1905.787
Langevin prior  11/ 40: energy=-1905.787
2022-12-29 05:46:41,772 - 12:37:24 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2181.743
Langevin prior  16/ 40: energy=-2181.743
2022-12-29 05:46:41,779 - 12:37:24 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2309.113
Langevin prior  21/ 40: energy=-2309.113
2022-12-29 05:46:41,786 - 12:37:24 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2394.427
Langevin prior  26/ 40: energy=-2394.427
2022-12-29 05:46:41,795 - 12:37:24 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2468.936
Langevin prior  31/ 40: energy=-2468.936
2022-12-29 05:46:41,804 - 12:37:25 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2515.441
Langevin prior  36/ 40: energy=-2515.441
2022-12-29 05:46:41,813 - 12:37:25 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2473.637
Langevin prior  40/ 40: energy=-2473.637
2022-12-29 05:46:44,580 - 12:37:27 - 2.8s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-052-test-greedy.txt
Generation: 188 batches
2022-12-29 05:46:50,228 - 12:37:33 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:46:51,978 - 12:37:35 - 1.8s - INFO - root - Generation Done
Generation Done
2022-12-29 05:46:51,978 - 12:37:35 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:46:54,390 - 12:37:37 - 2.4s - INFO - root - --- bleu: BLEU = 30.72, 54.1/33.2/27.4/24.4 (BP=0.929, ratio=0.931, hyp_len=158131, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.72, 54.1/33.2/27.4/24.4 (BP=0.929, ratio=0.931, hyp_len=158131, ref_len=169777)
--- bleu: BLEU = 30.72, 54.1/33.2/27.4/24.4 (BP=0.929, ratio=0.931, hyp_len=158131, ref_len=169777)

2022-12-29 05:46:54,390 - 12:37:37 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:46:54,392 - 12:37:37 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:47:03,744 - 12:37:46 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    53 rec:   11.824 mi: 1.95391619 zkl:   74.081 cd:    0.408 pos_prob:   84.303 prob_neg:   83.896 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    53 rec:   11.824 mi: 1.95391619 zkl:   74.081 cd:    0.408 pos_prob:   84.303 prob_neg:   83.896 kl_weight:    0.396 do_ae_train: False
2022-12-29 05:47:13,023 - 12:37:56 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    53 rec:   14.152 mi: 2.04376435 zkl:   71.901 cd:    4.772 pos_prob:   90.661 prob_neg:   85.889 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    53 rec:   14.152 mi: 2.04376435 zkl:   71.901 cd:    4.772 pos_prob:   90.661 prob_neg:   85.889 kl_weight:    0.459 do_ae_train: False
2022-12-29 05:47:22,467 - 12:38:05 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    53 rec:   15.471 mi: 1.80585694 zkl:   64.391 cd:    2.972 pos_prob:   82.994 prob_neg:   80.022 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    53 rec:   15.471 mi: 1.80585694 zkl:   64.391 cd:    2.972 pos_prob:   82.994 prob_neg:   80.022 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:47:31,874 - 12:38:15 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    53 rec:   16.333 mi: 2.06827331 zkl:   71.180 cd:    6.208 pos_prob:   90.855 prob_neg:   84.647 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    53 rec:   16.333 mi: 2.06827331 zkl:   71.180 cd:    6.208 pos_prob:   90.855 prob_neg:   84.647 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:47:41,121 - 12:38:24 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    53 rec:   18.597 mi: 2.19012189 zkl:   71.974 cd:   -1.521 pos_prob:   84.262 prob_neg:   85.782 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    53 rec:   18.597 mi: 2.19012189 zkl:   71.974 cd:   -1.521 pos_prob:   84.262 prob_neg:   85.782 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:47:41,133 - 12:38:24 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-344.767
Langevin prior   1/ 40: energy=-344.767
2022-12-29 05:47:41,139 - 12:38:24 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1462.526
Langevin prior   6/ 40: energy=-1462.526
2022-12-29 05:47:41,144 - 12:38:24 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2205.078
Langevin prior  11/ 40: energy=-2205.078
2022-12-29 05:47:41,149 - 12:38:24 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2504.662
Langevin prior  16/ 40: energy=-2504.662
2022-12-29 05:47:41,154 - 12:38:24 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2754.434
Langevin prior  21/ 40: energy=-2754.434
2022-12-29 05:47:41,160 - 12:38:24 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2785.063
Langevin prior  26/ 40: energy=-2785.063
2022-12-29 05:47:41,168 - 12:38:24 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2775.619
Langevin prior  31/ 40: energy=-2775.619
2022-12-29 05:47:41,176 - 12:38:24 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2737.637
Langevin prior  36/ 40: energy=-2737.637
2022-12-29 05:47:41,184 - 12:38:24 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2731.457
Langevin prior  40/ 40: energy=-2731.457
2022-12-29 05:57:26,347 - 12:48:09 - 585.2s - INFO - root - Negative Log-likehood -183.934728
200 done. -161.25120346021706
400 done. -155.28816023231505
600 done. -154.97485150719183
800 done. -155.61709928202893
1000 done. -154.47265391774016
1200 done. -155.3113164197485
1400 done. -156.4813878872071
1600 done. -154.6098910280771
1800 done. -153.95162929290368
2000 done. -171.7158677337295
2200 done. -182.92970327378632
2400 done. -194.82509601520889
2600 done. -205.98329169703277
2800 done. -213.1414322474981
3000 done. -223.00016926697495
3200 done. -228.77430800066747
3400 done. -236.11074362138447
3600 done. -240.62444514796653
3800 done. -246.3996603727942
4000 done. -250.61539807061138
4200 done. -240.7488968006807
4400 done. -230.58076156065866
4600 done. -221.50653929239735
4800 done. -213.09223477209773
5000 done. -205.53172700867347
5200 done. -198.4253147340128
5400 done. -191.94450893330853
5600 done. -185.94104917311668
Negative Log-likehood -183.934728
2022-12-29 05:57:26,348 - 12:48:09 - 0.0s - INFO - root - log-likelihood:   -183.935
log-likelihood:   -183.935
2022-12-29 05:58:12,736 - 12:48:55 - 46.4s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-053-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 05:58:12,739 - 12:48:55 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-053-test-greedy.txt
Generation: 188 batches
2022-12-29 05:58:18,363 - 12:49:01 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:58:19,869 - 12:49:03 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 05:58:19,869 - 12:49:03 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 05:58:22,295 - 12:49:05 - 2.4s - INFO - root - --- bleu: BLEU = 29.86, 52.5/32.0/26.5/23.6 (BP=0.933, ratio=0.935, hyp_len=158701, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.86, 52.5/32.0/26.5/23.6 (BP=0.933, ratio=0.935, hyp_len=158701, ref_len=169777)
--- bleu: BLEU = 29.86, 52.5/32.0/26.5/23.6 (BP=0.933, ratio=0.935, hyp_len=158701, ref_len=169777)

2022-12-29 05:58:22,296 - 12:49:05 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 05:58:22,297 - 12:49:05 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 05:58:31,553 - 12:49:14 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    54 rec:   14.582 mi: 2.24989462 zkl:   63.665 cd:  -18.699 pos_prob:   79.611 prob_neg:   98.310 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    54 rec:   14.582 mi: 2.24989462 zkl:   63.665 cd:  -18.699 pos_prob:   79.611 prob_neg:   98.310 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:58:40,868 - 12:49:24 - 9.3s - INFO - root - batch/max_batch/ep:   200/   529/    54 rec:   17.288 mi: 2.20328116 zkl:   69.707 cd:   -1.907 pos_prob:   89.018 prob_neg:   90.925 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    54 rec:   17.288 mi: 2.20328116 zkl:   69.707 cd:   -1.907 pos_prob:   89.018 prob_neg:   90.925 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:58:50,244 - 12:49:33 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    54 rec:   14.379 mi: 2.28447104 zkl:   71.023 cd:   -5.023 pos_prob:   87.389 prob_neg:   92.412 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    54 rec:   14.379 mi: 2.28447104 zkl:   71.023 cd:   -5.023 pos_prob:   87.389 prob_neg:   92.412 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:58:59,660 - 12:49:42 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    54 rec:   15.150 mi: 2.17797923 zkl:   75.406 cd:    0.369 pos_prob:   92.834 prob_neg:   92.465 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    54 rec:   15.150 mi: 2.17797923 zkl:   75.406 cd:    0.369 pos_prob:   92.834 prob_neg:   92.465 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:59:08,988 - 12:49:52 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    54 rec:   15.809 mi: 1.99083734 zkl:   66.751 cd:  -10.250 pos_prob:   85.388 prob_neg:   95.638 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    54 rec:   15.809 mi: 1.99083734 zkl:   66.751 cd:  -10.250 pos_prob:   85.388 prob_neg:   95.638 kl_weight:    0.500 do_ae_train: False
2022-12-29 05:59:09,000 - 12:49:52 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-323.041
Langevin prior   1/ 40: energy=-323.041
2022-12-29 05:59:09,006 - 12:49:52 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1354.882
Langevin prior   6/ 40: energy=-1354.882
2022-12-29 05:59:09,011 - 12:49:52 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-1967.282
Langevin prior  11/ 40: energy=-1967.282
2022-12-29 05:59:09,016 - 12:49:52 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2297.180
Langevin prior  16/ 40: energy=-2297.180
2022-12-29 05:59:09,021 - 12:49:52 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2425.695
Langevin prior  21/ 40: energy=-2425.695
2022-12-29 05:59:09,028 - 12:49:52 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2490.240
Langevin prior  26/ 40: energy=-2490.240
2022-12-29 05:59:09,036 - 12:49:52 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2668.783
Langevin prior  31/ 40: energy=-2668.783
2022-12-29 05:59:09,044 - 12:49:52 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2647.298
Langevin prior  36/ 40: energy=-2647.298
2022-12-29 05:59:09,052 - 12:49:52 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2563.986
Langevin prior  40/ 40: energy=-2563.986
2022-12-29 06:08:54,091 - 12:59:37 - 585.0s - INFO - root - Negative Log-likehood -183.593018
200 done. -161.1012207316037
400 done. -155.54687309121812
600 done. -155.2379668707671
800 done. -155.9237590897883
1000 done. -154.86461601253592
1200 done. -155.69693728968772
1400 done. -156.81324766339475
1600 done. -154.88117843865322
1800 done. -154.2278971501016
2000 done. -171.8081731282751
2200 done. -182.99302312626665
2400 done. -195.13848576661272
2600 done. -206.26867375810937
2800 done. -213.39738437858765
3000 done. -223.3075091985485
3200 done. -228.84461157116328
3400 done. -236.13454832315912
3600 done. -240.6297910532963
3800 done. -246.4402292347942
4000 done. -250.58550577132013
4200 done. -240.68011133573364
4400 done. -230.44908955257876
4600 done. -221.3391884244832
4800 done. -212.8916245668956
5000 done. -205.30323468339958
5200 done. -198.15396799832783
5400 done. -191.64775457767539
5600 done. -185.6063192209829
Negative Log-likehood -183.593018
2022-12-29 06:08:54,091 - 12:59:37 - 0.0s - INFO - root - log-likelihood:   -183.593
log-likelihood:   -183.593
2022-12-29 06:09:40,608 - 13:00:23 - 46.5s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-054-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 06:09:40,610 - 13:00:23 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-054-test-greedy.txt
Generation: 188 batches
2022-12-29 06:09:46,260 - 13:00:29 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:09:47,781 - 13:00:30 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 06:09:47,781 - 13:00:30 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:09:50,222 - 13:00:33 - 2.4s - INFO - root - --- bleu: BLEU = 29.60, 51.6/31.4/25.7/22.9 (BP=0.947, ratio=0.949, hyp_len=161052, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.60, 51.6/31.4/25.7/22.9 (BP=0.947, ratio=0.949, hyp_len=161052, ref_len=169777)
--- bleu: BLEU = 29.60, 51.6/31.4/25.7/22.9 (BP=0.947, ratio=0.949, hyp_len=161052, ref_len=169777)

2022-12-29 06:09:50,223 - 13:00:33 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 06:09:50,224 - 13:00:33 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 06:09:59,564 - 13:00:42 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    55 rec:    5.996 mi: 2.14233708 zkl:  159.133 cd:  -17.707 pos_prob:   71.142 prob_neg:   88.849 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    55 rec:    5.996 mi: 2.14233708 zkl:  159.133 cd:  -17.707 pos_prob:   71.142 prob_neg:   88.849 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:10:08,740 - 13:00:51 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    55 rec:   10.237 mi: 2.20322204 zkl:  159.091 cd:  -16.672 pos_prob:   66.886 prob_neg:   83.558 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    55 rec:   10.237 mi: 2.20322204 zkl:  159.091 cd:  -16.672 pos_prob:   66.886 prob_neg:   83.558 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:10:18,087 - 13:01:01 - 9.3s - INFO - root - batch/max_batch/ep:   300/   529/    55 rec:   11.455 mi: 2.11534834 zkl:  161.604 cd:  -22.287 pos_prob:   62.872 prob_neg:   85.159 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    55 rec:   11.455 mi: 2.11534834 zkl:  161.604 cd:  -22.287 pos_prob:   62.872 prob_neg:   85.159 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:10:27,172 - 13:01:10 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    55 rec:   11.245 mi: 2.17469811 zkl:  165.203 cd:  -25.771 pos_prob:   63.262 prob_neg:   89.033 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    55 rec:   11.245 mi: 2.17469811 zkl:  165.203 cd:  -25.771 pos_prob:   63.262 prob_neg:   89.033 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:10:36,308 - 13:01:19 - 9.1s - INFO - root - batch/max_batch/ep:   500/   529/    55 rec:   13.150 mi: 2.09331012 zkl:  159.818 cd:  -33.959 pos_prob:   51.849 prob_neg:   85.808 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    55 rec:   13.150 mi: 2.09331012 zkl:  159.818 cd:  -33.959 pos_prob:   51.849 prob_neg:   85.808 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:10:36,321 - 13:01:19 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-303.172
Langevin prior   1/ 40: energy=-303.172
2022-12-29 06:10:36,326 - 13:01:19 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1430.018
Langevin prior   6/ 40: energy=-1430.018
2022-12-29 06:10:36,332 - 13:01:19 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2180.277
Langevin prior  11/ 40: energy=-2180.277
2022-12-29 06:10:36,337 - 13:01:19 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2464.888
Langevin prior  16/ 40: energy=-2464.888
2022-12-29 06:10:36,342 - 13:01:19 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2641.668
Langevin prior  21/ 40: energy=-2641.668
2022-12-29 06:10:36,348 - 13:01:19 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2728.507
Langevin prior  26/ 40: energy=-2728.507
2022-12-29 06:10:36,355 - 13:01:19 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2823.648
Langevin prior  31/ 40: energy=-2823.648
2022-12-29 06:10:36,363 - 13:01:19 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2790.905
Langevin prior  36/ 40: energy=-2790.905
2022-12-29 06:10:36,370 - 13:01:19 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2800.587
Langevin prior  40/ 40: energy=-2800.587
2022-12-29 06:10:38,938 - 13:01:22 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-055-test-greedy.txt
Generation: 188 batches
2022-12-29 06:10:44,595 - 13:01:27 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:10:46,100 - 13:01:29 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 06:10:46,101 - 13:01:29 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:10:48,528 - 13:01:31 - 2.4s - INFO - root - --- bleu: BLEU = 30.85, 54.1/32.8/26.9/24.0 (BP=0.943, ratio=0.944, hyp_len=160287, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.85, 54.1/32.8/26.9/24.0 (BP=0.943, ratio=0.944, hyp_len=160287, ref_len=169777)
--- bleu: BLEU = 30.85, 54.1/32.8/26.9/24.0 (BP=0.943, ratio=0.944, hyp_len=160287, ref_len=169777)

2022-12-29 06:10:48,529 - 13:01:31 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 06:10:48,530 - 13:01:31 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 06:10:57,707 - 13:01:40 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    56 rec:   11.037 mi: 2.07189393 zkl:  161.064 cd:  -48.154 pos_prob:   49.665 prob_neg:   97.819 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    56 rec:   11.037 mi: 2.07189393 zkl:  161.064 cd:  -48.154 pos_prob:   49.665 prob_neg:   97.819 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:11:06,887 - 13:01:50 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    56 rec:    6.383 mi: 2.09037042 zkl:  166.370 cd:  -41.587 pos_prob:   58.674 prob_neg:  100.261 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    56 rec:    6.383 mi: 2.09037042 zkl:  166.370 cd:  -41.587 pos_prob:   58.674 prob_neg:  100.261 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:11:15,810 - 13:01:59 - 8.9s - INFO - root - batch/max_batch/ep:   300/   529/    56 rec:    9.109 mi: 2.03444862 zkl:  160.005 cd:  -33.994 pos_prob:   53.703 prob_neg:   87.697 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    56 rec:    9.109 mi: 2.03444862 zkl:  160.005 cd:  -33.994 pos_prob:   53.703 prob_neg:   87.697 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:11:25,042 - 13:02:08 - 9.2s - INFO - root - batch/max_batch/ep:   400/   529/    56 rec:   11.681 mi: 2.35674238 zkl:  162.512 cd:  -43.974 pos_prob:   48.917 prob_neg:   92.892 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    56 rec:   11.681 mi: 2.35674238 zkl:  162.512 cd:  -43.974 pos_prob:   48.917 prob_neg:   92.892 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:11:34,368 - 13:02:17 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    56 rec:   14.102 mi: 2.10758972 zkl:  161.903 cd:  -42.999 pos_prob:   45.739 prob_neg:   88.738 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    56 rec:   14.102 mi: 2.10758972 zkl:  161.903 cd:  -42.999 pos_prob:   45.739 prob_neg:   88.738 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:11:34,380 - 13:02:17 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-351.968
Langevin prior   1/ 40: energy=-351.968
2022-12-29 06:11:34,385 - 13:02:17 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1432.743
Langevin prior   6/ 40: energy=-1432.743
2022-12-29 06:11:34,390 - 13:02:17 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2110.171
Langevin prior  11/ 40: energy=-2110.171
2022-12-29 06:11:34,396 - 13:02:17 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2433.727
Langevin prior  16/ 40: energy=-2433.727
2022-12-29 06:11:34,401 - 13:02:17 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2559.064
Langevin prior  21/ 40: energy=-2559.064
2022-12-29 06:11:34,407 - 13:02:17 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2660.351
Langevin prior  26/ 40: energy=-2660.351
2022-12-29 06:11:34,413 - 13:02:17 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2705.241
Langevin prior  31/ 40: energy=-2705.241
2022-12-29 06:11:34,422 - 13:02:17 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2788.063
Langevin prior  36/ 40: energy=-2788.063
2022-12-29 06:11:34,429 - 13:02:17 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2782.821
Langevin prior  40/ 40: energy=-2782.821
2022-12-29 06:11:37,055 - 13:02:20 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-056-test-greedy.txt
Generation: 188 batches
2022-12-29 06:11:42,710 - 13:02:25 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:11:44,228 - 13:02:27 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 06:11:44,228 - 13:02:27 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:11:46,674 - 13:02:29 - 2.4s - INFO - root - --- bleu: BLEU = 31.32, 53.6/32.8/27.0/23.9 (BP=0.959, ratio=0.960, hyp_len=163014, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.32, 53.6/32.8/27.0/23.9 (BP=0.959, ratio=0.960, hyp_len=163014, ref_len=169777)
--- bleu: BLEU = 31.32, 53.6/32.8/27.0/23.9 (BP=0.959, ratio=0.960, hyp_len=163014, ref_len=169777)

2022-12-29 06:11:46,674 - 13:02:29 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 06:11:46,676 - 13:02:29 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 06:11:55,864 - 13:02:39 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    57 rec:    7.297 mi: 2.17942953 zkl:  160.960 cd:  -39.149 pos_prob:   53.748 prob_neg:   92.897 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   100/   529/    57 rec:    7.297 mi: 2.17942953 zkl:  160.960 cd:  -39.149 pos_prob:   53.748 prob_neg:   92.897 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:12:05,109 - 13:02:48 - 9.2s - INFO - root - batch/max_batch/ep:   200/   529/    57 rec:    8.988 mi: 2.10046434 zkl:  161.807 cd:  -33.741 pos_prob:   48.049 prob_neg:   81.790 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   200/   529/    57 rec:    8.988 mi: 2.10046434 zkl:  161.807 cd:  -33.741 pos_prob:   48.049 prob_neg:   81.790 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:12:14,227 - 13:02:57 - 9.1s - INFO - root - batch/max_batch/ep:   300/   529/    57 rec:    6.357 mi: 2.03475571 zkl:  168.556 cd:  -44.294 pos_prob:   52.578 prob_neg:   96.872 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   300/   529/    57 rec:    6.357 mi: 2.03475571 zkl:  168.556 cd:  -44.294 pos_prob:   52.578 prob_neg:   96.872 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:12:23,303 - 13:03:06 - 9.1s - INFO - root - batch/max_batch/ep:   400/   529/    57 rec:    9.581 mi: 2.02606440 zkl:  165.925 cd:  -35.327 pos_prob:   50.731 prob_neg:   86.058 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   400/   529/    57 rec:    9.581 mi: 2.02606440 zkl:  165.925 cd:  -35.327 pos_prob:   50.731 prob_neg:   86.058 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:12:32,600 - 13:03:15 - 9.3s - INFO - root - batch/max_batch/ep:   500/   529/    57 rec:    9.376 mi: 2.07073045 zkl:  164.752 cd:  -43.883 pos_prob:   43.749 prob_neg:   87.632 kl_weight:    0.000 do_ae_train: True
batch/max_batch/ep:   500/   529/    57 rec:    9.376 mi: 2.07073045 zkl:  164.752 cd:  -43.883 pos_prob:   43.749 prob_neg:   87.632 kl_weight:    0.000 do_ae_train: True
2022-12-29 06:12:32,612 - 13:03:15 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-294.461
Langevin prior   1/ 40: energy=-294.461
2022-12-29 06:12:32,618 - 13:03:15 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1380.925
Langevin prior   6/ 40: energy=-1380.925
2022-12-29 06:12:32,623 - 13:03:15 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2002.380
Langevin prior  11/ 40: energy=-2002.380
2022-12-29 06:12:32,628 - 13:03:15 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2279.380
Langevin prior  16/ 40: energy=-2279.380
2022-12-29 06:12:32,633 - 13:03:15 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2411.404
Langevin prior  21/ 40: energy=-2411.404
2022-12-29 06:12:32,639 - 13:03:15 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2445.437
Langevin prior  26/ 40: energy=-2445.437
2022-12-29 06:12:32,645 - 13:03:15 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2395.597
Langevin prior  31/ 40: energy=-2395.597
2022-12-29 06:12:32,654 - 13:03:15 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2395.938
Langevin prior  36/ 40: energy=-2395.938
2022-12-29 06:12:32,661 - 13:03:15 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2401.062
Langevin prior  40/ 40: energy=-2401.062
2022-12-29 06:12:35,220 - 13:03:18 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-057-test-greedy.txt
Generation: 188 batches
2022-12-29 06:12:40,878 - 13:03:24 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:12:42,386 - 13:03:25 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 06:12:42,387 - 13:03:25 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:12:44,848 - 13:03:28 - 2.5s - INFO - root - --- bleu: BLEU = 31.62, 54.3/33.4/27.6/24.6 (BP=0.949, ratio=0.950, hyp_len=161264, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 31.62, 54.3/33.4/27.6/24.6 (BP=0.949, ratio=0.950, hyp_len=161264, ref_len=169777)
--- bleu: BLEU = 31.62, 54.3/33.4/27.6/24.6 (BP=0.949, ratio=0.950, hyp_len=161264, ref_len=169777)

2022-12-29 06:12:44,849 - 13:03:28 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 06:12:44,850 - 13:03:28 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 06:12:54,230 - 13:03:37 - 9.4s - INFO - root - batch/max_batch/ep:   100/   529/    58 rec:    9.662 mi: 2.11735749 zkl:   97.622 cd:  -27.533 pos_prob:   67.344 prob_neg:   94.878 kl_weight:    0.062 do_ae_train: False
batch/max_batch/ep:   100/   529/    58 rec:    9.662 mi: 2.11735749 zkl:   97.622 cd:  -27.533 pos_prob:   67.344 prob_neg:   94.878 kl_weight:    0.062 do_ae_train: False
2022-12-29 06:13:03,633 - 13:03:46 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    58 rec:   14.267 mi: 2.28554964 zkl:   86.276 cd:  -17.951 pos_prob:   69.966 prob_neg:   87.917 kl_weight:    0.125 do_ae_train: False
batch/max_batch/ep:   200/   529/    58 rec:   14.267 mi: 2.28554964 zkl:   86.276 cd:  -17.951 pos_prob:   69.966 prob_neg:   87.917 kl_weight:    0.125 do_ae_train: False
2022-12-29 06:13:13,051 - 13:03:56 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    58 rec:   11.777 mi: 2.22284675 zkl:   84.575 cd:   -2.451 pos_prob:   79.306 prob_neg:   81.757 kl_weight:    0.188 do_ae_train: False
batch/max_batch/ep:   300/   529/    58 rec:   11.777 mi: 2.22284675 zkl:   84.575 cd:   -2.451 pos_prob:   79.306 prob_neg:   81.757 kl_weight:    0.188 do_ae_train: False
2022-12-29 06:13:22,383 - 13:04:05 - 9.3s - INFO - root - batch/max_batch/ep:   400/   529/    58 rec:   13.241 mi: 2.23363280 zkl:   79.691 cd:  -11.960 pos_prob:   77.209 prob_neg:   89.169 kl_weight:    0.251 do_ae_train: False
batch/max_batch/ep:   400/   529/    58 rec:   13.241 mi: 2.23363280 zkl:   79.691 cd:  -11.960 pos_prob:   77.209 prob_neg:   89.169 kl_weight:    0.251 do_ae_train: False
2022-12-29 06:13:31,618 - 13:04:14 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    58 rec:   10.110 mi: 2.27189684 zkl:   79.266 cd:   -1.313 pos_prob:   77.213 prob_neg:   78.526 kl_weight:    0.314 do_ae_train: False
batch/max_batch/ep:   500/   529/    58 rec:   10.110 mi: 2.27189684 zkl:   79.266 cd:   -1.313 pos_prob:   77.213 prob_neg:   78.526 kl_weight:    0.314 do_ae_train: False
2022-12-29 06:13:31,630 - 13:04:14 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-311.509
Langevin prior   1/ 40: energy=-311.509
2022-12-29 06:13:31,636 - 13:04:14 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1442.749
Langevin prior   6/ 40: energy=-1442.749
2022-12-29 06:13:31,641 - 13:04:14 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2061.191
Langevin prior  11/ 40: energy=-2061.191
2022-12-29 06:13:31,646 - 13:04:14 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2327.129
Langevin prior  16/ 40: energy=-2327.129
2022-12-29 06:13:31,652 - 13:04:14 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2498.468
Langevin prior  21/ 40: energy=-2498.468
2022-12-29 06:13:31,658 - 13:04:14 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2552.729
Langevin prior  26/ 40: energy=-2552.729
2022-12-29 06:13:31,666 - 13:04:14 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2581.660
Langevin prior  31/ 40: energy=-2581.660
2022-12-29 06:13:31,674 - 13:04:14 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2616.881
Langevin prior  36/ 40: energy=-2616.881
2022-12-29 06:13:31,682 - 13:04:14 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2593.064
Langevin prior  40/ 40: energy=-2593.064
2022-12-29 06:13:34,329 - 13:04:17 - 2.6s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-058-test-greedy.txt
Generation: 188 batches
2022-12-29 06:13:39,977 - 13:04:23 - 5.6s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:13:41,750 - 13:04:24 - 1.8s - INFO - root - Generation Done
Generation Done
2022-12-29 06:13:41,750 - 13:04:24 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:13:44,253 - 13:04:27 - 2.5s - INFO - root - --- bleu: BLEU = 30.49, 53.3/32.4/26.6/23.7 (BP=0.944, ratio=0.945, hyp_len=160495, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 30.49, 53.3/32.4/26.6/23.7 (BP=0.944, ratio=0.945, hyp_len=160495, ref_len=169777)
--- bleu: BLEU = 30.49, 53.3/32.4/26.6/23.7 (BP=0.944, ratio=0.945, hyp_len=160495, ref_len=169777)

2022-12-29 06:13:44,254 - 13:04:27 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 06:13:44,255 - 13:04:27 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 06:13:53,541 - 13:04:36 - 9.3s - INFO - root - batch/max_batch/ep:   100/   529/    59 rec:   14.576 mi: 2.30542016 zkl:   67.555 cd:   -7.687 pos_prob:   74.262 prob_neg:   81.948 kl_weight:    0.396 do_ae_train: False
batch/max_batch/ep:   100/   529/    59 rec:   14.576 mi: 2.30542016 zkl:   67.555 cd:   -7.687 pos_prob:   74.262 prob_neg:   81.948 kl_weight:    0.396 do_ae_train: False
2022-12-29 06:14:02,996 - 13:04:46 - 9.5s - INFO - root - batch/max_batch/ep:   200/   529/    59 rec:   19.001 mi: 2.07500577 zkl:   70.846 cd:    1.428 pos_prob:   80.543 prob_neg:   79.114 kl_weight:    0.459 do_ae_train: False
batch/max_batch/ep:   200/   529/    59 rec:   19.001 mi: 2.07500577 zkl:   70.846 cd:    1.428 pos_prob:   80.543 prob_neg:   79.114 kl_weight:    0.459 do_ae_train: False
2022-12-29 06:14:12,388 - 13:04:55 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    59 rec:   14.637 mi: 2.22141671 zkl:   75.466 cd:   10.572 pos_prob:   87.597 prob_neg:   77.025 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    59 rec:   14.637 mi: 2.22141671 zkl:   75.466 cd:   10.572 pos_prob:   87.597 prob_neg:   77.025 kl_weight:    0.500 do_ae_train: False
2022-12-29 06:14:21,776 - 13:05:04 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    59 rec:   14.206 mi: 2.04043007 zkl:   67.520 cd:    5.003 pos_prob:   85.959 prob_neg:   80.957 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    59 rec:   14.206 mi: 2.04043007 zkl:   67.520 cd:    5.003 pos_prob:   85.959 prob_neg:   80.957 kl_weight:    0.500 do_ae_train: False
2022-12-29 06:14:31,175 - 13:05:14 - 9.4s - INFO - root - batch/max_batch/ep:   500/   529/    59 rec:   20.211 mi: 1.91452432 zkl:   67.750 cd:   -9.697 pos_prob:   81.335 prob_neg:   91.033 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    59 rec:   20.211 mi: 1.91452432 zkl:   67.750 cd:   -9.697 pos_prob:   81.335 prob_neg:   91.033 kl_weight:    0.500 do_ae_train: False
2022-12-29 06:14:31,187 - 13:05:14 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-345.775
Langevin prior   1/ 40: energy=-345.775
2022-12-29 06:14:31,193 - 13:05:14 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1502.719
Langevin prior   6/ 40: energy=-1502.719
2022-12-29 06:14:31,199 - 13:05:14 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2104.995
Langevin prior  11/ 40: energy=-2104.995
2022-12-29 06:14:31,204 - 13:05:14 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2397.579
Langevin prior  16/ 40: energy=-2397.579
2022-12-29 06:14:31,211 - 13:05:14 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2544.050
Langevin prior  21/ 40: energy=-2544.050
2022-12-29 06:14:31,219 - 13:05:14 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2629.699
Langevin prior  26/ 40: energy=-2629.699
2022-12-29 06:14:31,227 - 13:05:14 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2619.685
Langevin prior  31/ 40: energy=-2619.685
2022-12-29 06:14:31,237 - 13:05:14 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2705.615
Langevin prior  36/ 40: energy=-2705.615
2022-12-29 06:14:31,244 - 13:05:14 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2647.569
Langevin prior  40/ 40: energy=-2647.569
2022-12-29 06:24:15,997 - 13:14:59 - 584.8s - INFO - root - Negative Log-likehood -187.169920
200 done. -164.63157615960634
400 done. -158.62277570005233
600 done. -158.24331928026444
800 done. -159.01111923177285
1000 done. -158.01188030254048
1200 done. -158.73707461583138
1400 done. -159.99134381402635
1600 done. -158.15211095672362
1800 done. -157.5224031587299
2000 done. -175.1774620334929
2200 done. -186.7114159953675
2400 done. -198.7092691040455
2600 done. -210.13349213919003
2800 done. -217.3876928415847
3000 done. -227.2917595968551
3200 done. -233.045745112985
3400 done. -240.3912264652994
3600 done. -245.0519899901034
3800 done. -250.8613396956224
4000 done. -255.0515415649986
4200 done. -244.9497765327185
4400 done. -234.5993318676974
4600 done. -225.3446332675427
4800 done. -216.77704739675974
5000 done. -209.08724644604143
5200 done. -201.87760581006154
5400 done. -195.29648855780485
5600 done. -189.20353816013247
Negative Log-likehood -187.169920
2022-12-29 06:24:15,997 - 13:14:59 - 0.0s - INFO - root - log-likelihood:   -187.170
log-likelihood:   -187.170
2022-12-29 06:25:02,920 - 13:15:46 - 46.9s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-059-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 06:25:02,923 - 13:15:46 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-059-test-greedy.txt
Generation: 188 batches
2022-12-29 06:25:08,592 - 13:15:51 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:25:10,106 - 13:15:53 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 06:25:10,107 - 13:15:53 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:25:12,563 - 13:15:55 - 2.5s - INFO - root - --- bleu: BLEU = 29.90, 52.2/31.6/26.0/23.1 (BP=0.948, ratio=0.949, hyp_len=161124, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.90, 52.2/31.6/26.0/23.1 (BP=0.948, ratio=0.949, hyp_len=161124, ref_len=169777)
--- bleu: BLEU = 29.90, 52.2/31.6/26.0/23.1 (BP=0.948, ratio=0.949, hyp_len=161124, ref_len=169777)

2022-12-29 06:25:12,564 - 13:15:55 - 0.0s - INFO - root - Number of left over sample 0
Number of left over sample 0
2022-12-29 06:25:12,565 - 13:15:55 - 0.0s - INFO - root - Train begins with 529 batches
Train begins with 529 batches
2022-12-29 06:25:21,798 - 13:16:04 - 9.2s - INFO - root - batch/max_batch/ep:   100/   529/    60 rec:   14.517 mi: 1.97730911 zkl:   78.391 cd:    3.464 pos_prob:  104.691 prob_neg:  101.227 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   100/   529/    60 rec:   14.517 mi: 1.97730911 zkl:   78.391 cd:    3.464 pos_prob:  104.691 prob_neg:  101.227 kl_weight:    0.500 do_ae_train: False
2022-12-29 06:25:31,198 - 13:16:14 - 9.4s - INFO - root - batch/max_batch/ep:   200/   529/    60 rec:   15.363 mi: 1.76812375 zkl:   82.758 cd:    0.062 pos_prob:  115.695 prob_neg:  115.633 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   200/   529/    60 rec:   15.363 mi: 1.76812375 zkl:   82.758 cd:    0.062 pos_prob:  115.695 prob_neg:  115.633 kl_weight:    0.500 do_ae_train: False
2022-12-29 06:25:40,599 - 13:16:23 - 9.4s - INFO - root - batch/max_batch/ep:   300/   529/    60 rec:   19.304 mi: 2.33037043 zkl:   76.891 cd:   -9.493 pos_prob:   92.507 prob_neg:  102.000 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   300/   529/    60 rec:   19.304 mi: 2.33037043 zkl:   76.891 cd:   -9.493 pos_prob:   92.507 prob_neg:  102.000 kl_weight:    0.500 do_ae_train: False
2022-12-29 06:25:49,981 - 13:16:33 - 9.4s - INFO - root - batch/max_batch/ep:   400/   529/    60 rec:   18.128 mi: 2.00588965 zkl:   75.235 cd:   -3.805 pos_prob:  102.262 prob_neg:  106.067 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   400/   529/    60 rec:   18.128 mi: 2.00588965 zkl:   75.235 cd:   -3.805 pos_prob:  102.262 prob_neg:  106.067 kl_weight:    0.500 do_ae_train: False
2022-12-29 06:25:59,214 - 13:16:42 - 9.2s - INFO - root - batch/max_batch/ep:   500/   529/    60 rec:   13.773 mi: 1.85717750 zkl:   78.763 cd:   13.900 pos_prob:  111.051 prob_neg:   97.151 kl_weight:    0.500 do_ae_train: False
batch/max_batch/ep:   500/   529/    60 rec:   13.773 mi: 1.85717750 zkl:   78.763 cd:   13.900 pos_prob:  111.051 prob_neg:   97.151 kl_weight:    0.500 do_ae_train: False
2022-12-29 06:25:59,227 - 13:16:42 - 0.0s - INFO - root - Langevin prior   1/ 40: energy=-294.187
Langevin prior   1/ 40: energy=-294.187
2022-12-29 06:25:59,233 - 13:16:42 - 0.0s - INFO - root - Langevin prior   6/ 40: energy=-1480.437
Langevin prior   6/ 40: energy=-1480.437
2022-12-29 06:25:59,238 - 13:16:42 - 0.0s - INFO - root - Langevin prior  11/ 40: energy=-2227.997
Langevin prior  11/ 40: energy=-2227.997
2022-12-29 06:25:59,243 - 13:16:42 - 0.0s - INFO - root - Langevin prior  16/ 40: energy=-2681.979
Langevin prior  16/ 40: energy=-2681.979
2022-12-29 06:25:59,248 - 13:16:42 - 0.0s - INFO - root - Langevin prior  21/ 40: energy=-2799.216
Langevin prior  21/ 40: energy=-2799.216
2022-12-29 06:25:59,254 - 13:16:42 - 0.0s - INFO - root - Langevin prior  26/ 40: energy=-2868.095
Langevin prior  26/ 40: energy=-2868.095
2022-12-29 06:25:59,261 - 13:16:42 - 0.0s - INFO - root - Langevin prior  31/ 40: energy=-2931.740
Langevin prior  31/ 40: energy=-2931.740
2022-12-29 06:25:59,272 - 13:16:42 - 0.0s - INFO - root - Langevin prior  36/ 40: energy=-2952.635
Langevin prior  36/ 40: energy=-2952.635
2022-12-29 06:25:59,280 - 13:16:42 - 0.0s - INFO - root - Langevin prior  40/ 40: energy=-2946.695
Langevin prior  40/ 40: energy=-2946.695
2022-12-29 06:35:44,125 - 13:26:27 - 584.8s - INFO - root - Negative Log-likehood -187.323279
200 done. -164.25131871786678
400 done. -158.23412710293113
600 done. -157.95298606346
800 done. -158.9402858481517
1000 done. -157.9668518674516
1200 done. -158.64178543515285
1400 done. -159.7952623892843
1600 done. -157.96325387312157
1800 done. -157.2556916457002
2000 done. -174.87757887446415
2200 done. -186.54934159522827
2400 done. -198.64279110300726
2600 done. -210.04619424918556
2800 done. -217.40095442082713
3000 done. -227.43521150549893
3200 done. -233.27988925189524
3400 done. -240.78804602307252
3600 done. -245.5399835792746
3800 done. -251.2646075061459
4000 done. -255.42546786511082
4200 done. -245.3062359480403
4400 done. -234.90530923546225
4600 done. -225.6461494061467
4800 done. -217.06215137909368
5000 done. -209.34934701384924
5200 done. -202.10480795620327
5400 done. -195.49183675211515
5600 done. -189.36376121603146
Negative Log-likehood -187.323279
2022-12-29 06:35:44,125 - 13:26:27 - 0.0s - INFO - root - log-likelihood:   -187.323
log-likelihood:   -187.323
2022-12-29 06:36:30,715 - 13:27:13 - 46.6s - INFO - root - Generation Done
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/ep-060-sampling.txt
10/1534 epochs done.
20/1534 epochs done.
30/1534 epochs done.
40/1534 epochs done.
50/1534 epochs done.
60/1534 epochs done.
70/1534 epochs done.
80/1534 epochs done.
90/1534 epochs done.
100/1534 epochs done.
110/1534 epochs done.
120/1534 epochs done.
130/1534 epochs done.
140/1534 epochs done.
150/1534 epochs done.
160/1534 epochs done.
170/1534 epochs done.
180/1534 epochs done.
190/1534 epochs done.
200/1534 epochs done.
210/1534 epochs done.
220/1534 epochs done.
230/1534 epochs done.
240/1534 epochs done.
250/1534 epochs done.
260/1534 epochs done.
270/1534 epochs done.
280/1534 epochs done.
290/1534 epochs done.
300/1534 epochs done.
310/1534 epochs done.
320/1534 epochs done.
330/1534 epochs done.
340/1534 epochs done.
350/1534 epochs done.
360/1534 epochs done.
370/1534 epochs done.
380/1534 epochs done.
390/1534 epochs done.
400/1534 epochs done.
410/1534 epochs done.
420/1534 epochs done.
430/1534 epochs done.
440/1534 epochs done.
450/1534 epochs done.
460/1534 epochs done.
Generation Done
2022-12-29 06:36:30,718 - 13:27:13 - 0.0s - INFO - root - Generation: 188 batches
Saving test to /home/xiaodi/NLP/Lifelonglearning-main/Lifelonglearning_For_NLG_v3/ibebm/logs/ptb/dgmvae/2022-12-28T17-09-23-text_generation_from_pretrained.py/epoch-060-test-greedy.txt
Generation: 188 batches
2022-12-29 06:36:36,377 - 13:27:19 - 5.7s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:36:37,898 - 13:27:21 - 1.5s - INFO - root - Generation Done
Generation Done
2022-12-29 06:36:37,899 - 13:27:21 - 0.0s - INFO - model_basedon_huggingface.EBM_NR_LAMOL.ebm_models.dgmvae.evaluators - Generate report for default for 5640 samples
Generate report for default for 5640 samples
2022-12-29 06:36:40,351 - 13:27:23 - 2.5s - INFO - root - --- bleu: BLEU = 29.67, 51.0/31.0/25.5/22.7 (BP=0.959, ratio=0.960, hyp_len=162928, ref_len=169777)

Runing multi-bleu.perl
multi-bleu.perl return code:  0
BLEU = 29.67, 51.0/31.0/25.5/22.7 (BP=0.959, ratio=0.960, hyp_len=162928, ref_len=169777)
--- bleu: BLEU = 29.67, 51.0/31.0/25.5/22.7 (BP=0.959, ratio=0.960, hyp_len=162928, ref_len=169777)

